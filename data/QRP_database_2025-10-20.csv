source,file name,QRP_description,qrp_coded,description,model_phase,model_subphase,include,practice_target,QRP_reason,practice_description,practice_evidence,practice_notes,practice_reason,practice_solution
[[@Bodner2020]],Prosperi-2019-BF-hacking,Bayes Factor hacking: optimising the model to obtain a Bayes Factor above the required threshold.,S-hacking,S-hacking,"Model Evaluation, Model Construction","Data Processing, Model Performance Metric, Model Specification",FALSE,"model, output",Artificially inflates model performance and may result in spuriously selected model.," ""[[@Simonsohn2015]] showed through simulations (data peeking, convenient outlier removal, choosing favorable dependent variable or one variable combination out of many) how Bayes factors, not only p-values, can be hacked through selective reporting."""," ""[[@Simonsohn2015]] showed through simulations (data peeking, convenient outlier removal, choosing favorable dependent variable or one variable combination out of many) how Bayes factors, not only p-values, can be hacked through selective reporting.""","Bayes Factor Hacking by data peeking, removing outliers, or changing model specification (dependent variable and/or predictor variables) in order to find model with greatest Bayes Factor.",,
[[@Konijn2015]],Konijn-2015-Bayes-factor-hacking,Bayes Factor hacking: optimising the model to obtain a Bayes Factor above the required threshold.,S-hacking,S-hacking,Model Evaluation,Model Performance Metric,FALSE,"output, model",,"""However, a Bayesian approach is not without fallacies. That is, the results of Bayesian statistics can lead to instrumental optimization of Bayes factors (i.e., “BF-hacking”) similar to the phenomenon of p-hacking in traditional NHST, especially when dichotomous cut-off values for evidentiary power (and thus “publishability”) are set. In line with the famous quote of Rosnow and Rosenthal (1989, p. 1277), we argue that “God would love a Bayes Factor of 3.01 nearly as much as a BF of 2.99” (see section “BF-hacking”). Furthermore, not any BF-value can be taken as conclusive support without careful consideration of its interpretation, especially when BF values are small""",,,,
[[@feng2019]],Feng-2019-model-output-format-transformation,Changing model output format or transformation after observing results.,S-hacking,S-hacking,Model Application,Model Results,TRUE,output,,,,,"""The raw model predictions are sometimes transformed (for example, logistic transformation) via different methods under different assumptions.""",
[[@feng2019]],Feng-2019-performance-metric,Changing model output or evaluation metric thresholds after observing outcome.,S-hacking,S-hacking,Model Evaluation,Model Performance Metric,TRUE,output,,,,,"""Calculation of some evaluation indices requires a threshold. The threshold will vary by study because there is no single, default method for choosing a threshold.""",
[[@feng2019]],Feng-2019-model-output-threshold-hacking,Changing model output or evaluation metric thresholds after observing outcome.,S-hacking,S-hacking,Model Application,Model Output,TRUE,output,,,,,"""Often, the model predictions are in continuous format, which is subsequently transformed into a binary prediction under a particular threshold. Researchers have proposed different ways of thresholding for different purposes and under varied assumptions.""",
[[@Liu2020]],Liu-2020-changing-random-seed-after-seeing-results,Changing random seed and refitting model after seeing results to improve model performance.,S-hacking,S-hacking,"Model Construction, Model Evaluation","Model Specification, Model Tuning",TRUE,input,Model performance overrated.,"""Machine learning researchers note similar issues, for example tuning random seeds can drastically alter results"" (Cites: Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. 2018. Deep reinforcement learning that matters. In Proceedings of the AAAI Conference on Artificial Intelligence)",,,,
[[@hofman2023]],Hofman-2023-reaccessing-test-data-1,Changing the model specification or otherwise continuing to optimise the model after already validating or evaluating it on the test or holdout data.,Model Fishing,S-hacking,Model Construction,Model Specification,TRUE,model,,"""The other three participants described strategies that involved re-accessing test data"" (without reporting).","""P1 described how they might try reducing the model size, in terms of the number of neurons, in addition to checking the data distribution. If the model continued to fail after such checks would they attempt to acquire new data. P2 described how they would try varying the hyper-parameter settings in cross-validation, and if this failed to improve test performance, systematically test the effects of feature combinations on performance, followed by rethinking their pre-processing steps.""",,"""if not reported, they would jeopardize the reproducibility of the reported test metrics.""",
[[@hofman2023]],Hofman-2023-overhyping,Changing the model specification or otherwise continuing to optimise the model after already validating or evaluating it on the test or holdout data.,Model Fishing,S-hacking,"Model Evaluation, Model Construction, Model Application","Model Performance Metric, Model Tuning, Outcome Variable",TRUE,model,,"""One potentially common form \[of data leakage\] is due to continuing to optimize a model after having evaluated it on the test set (e.g., by adjusting hyperparameters or changing evaluation metrics after accessing the test set), known as “over-hyping” (Hosseini et al., 2020). While multiple accesses to the test set clearly violate the underlying principle of out-of-sample testing, in practice it may be common for practitioners to accidentally make this mistake in the same way that “HARK-ing”, or hypothesizing after results are known, is common in explanatory modeling (Kerr, 1998). \[...\] Less obvious sources of leakage can occur due to pre-processing or feature selection applied to both training and test data, use of illegitimate features that leak information about the outcome variable, lack of independence between training and test data, and use of future data in time series modeling, among others (Kapoor and Narayanan, 2022).""","""even with the dataset and model specification held fixed, exercising just a few degrees of freedom (specifically, treating the problem as classification versus regression, filtering or thresholding data, and choosing different performance metrics) is shown to change the qualitative nature of the results. For instance, if one operationalizes this exercise as a classification problem by building a model to predict whether a post will receive at least 10 re-posts, one can obtain impressive-sounding accuracies close to 100%. If, however, one treats this as a regression exercise to predict how many re-posts a given post gets without any data filtering, the R2 hovers around a relatively modest 35%. The reason for the difference comes from the fact that only a small fraction of posts exceed the threshold of 10 re-posts, and predicting which posts do—and how far they spread—is quite challenging. Given the stark difference in these numbers and pressure to report positive, strong results, a researcher who tries both approaches, even if only during the training and validation phase, might, upon seeing the results, publish the classification model as a success and discard the regression model as a failure. \[...\] The other three participants described strategies that involved re-accessing test data. Such steps would need to be explicitly reported as data exploration in communicating the results of research; if not reported, they would jeopardize the reproducibility of the reported test metrics. P1 described how they might try reducing the model size, in terms of the number of neurons, in addition to checking the data distribution. If the model continued to fail after such checks would they attempt to acquire new data. P2 described how they would try varying the hyperparameter settings in cross-validation, and if this failed to improve test performance, systematically test the effects of feature combinations on performance, followed by rethinking their pre-processing steps.""",,"""One of the simplest reasons for unreliable results is the (often unintentional) failure to adequately separate the training and validation sets from the test set, known as “leakage”. Sometimes this is as simple as mistakenly including the test examples in the training process (Wang, 2019; Oner et al., 2020). Other times there are more subtle forms of 'leakage'.""","""After the researcher has trained the model and proceeds to calculate test set performance, it is critical that they do not return to the training and validation phase or otherwise continue to manipulate the model."""
[[@lewis2023]],Lewis-2023-modifying-model-post-validation,Changing the model specification or otherwise continuing to optimise the model after already validating or evaluating it on the test or holdout data.,Model Fishing,S-hacking,Model Construction,Model Specification,TRUE,model,Model will be overfit to held-out data.,"""model evaluation using in-sample data will share any systematic errors, and a researcher using cross-validation may overfit the held-out data by modifying models post hoc after cross-validation""",,,,
[[@Bennett2013]],Bennett-2013-selective-reporting-weighting-evaluation-2,"Changing the relative weighting of model performance metrics after observing model results, when there are multiple model evaluation analyses.",S-hacking,S-hacking,Model Evaluation,Model Performance Metric,TRUE,output,,"""In every study, the researchers may tweak the analyses to make sure that their model is better. There are many statistical validation methodologies which may be combined at will to favor one particular result. The plague of selective reporting based on p- values, and the misuse of p-values in general (Wasserstein et al., 2019) can still affect internal and external validation [...] There may be even more subjectivity in considering the results of quantitative testing. Suppose one model is superior to another according to one metric, while it is the reverse according to another metric. What is the weight to be assigned to individual quantitative criteria in the overall assessment? How are these weights decided and by whom?""",,,,
[[@McDermott2021]],McDermott-2021-over-claiming-generalisability,Claiming the model has greater generalisability or credibility than it does.,Overhyping,Overhyping,Model Application,Inference,TRUE,output,,"""Just as statistical reproducibility is like internal validity, replicability is closely related to external validity (8), as it describes the notion of how well the desired results can be reproduced under conditions that match the conceptual description of the purported effect. Replicability is task-definition dependent; claiming a task has a greater conceptual horizon of generalizability makes it harder to satisfy this requirement.""",,,,
[[@Auspurg2021]],Nagy-2025-data-peeking-optional-stopping,Collecting new data and refitting model after observing model evaluation / model checking results (optional stopping rules).,Data Curation,Sample Curation,"Model Construction, Model Evaluation",Data Collection,TRUE,input,Overrates model performance and may result in spuriously selected model.,"""Monitoring hypothesis tests during data collection, and stopping when statistical inference is favorable, without controlling for sequential testing"" e.g. ""Researcher is collecting responses and tests the hypothesis after every participant - when significance is reached, the researcher stops collecting data.""",,Is this S-hacking or Sample Curation? outcome variable or performance measure is indirectly manipulated by directly manipulating input data.,"""inflated type 1 error, reduced replicability"" from the following sources: - de Heide, R., & Grünwald, P. D. (2021). Why optional stopping can be a problem for Bayesians. Psychonomic Bulletin & Review, 28(3), 795–812. [https://doi.org/10.3758/s13423-020-01803-x](https://doi.org/10.3758/s13423-020-01803-x) - Lakens, D. (2022). Sample size justification. Collabra. Psychology, 8(1). [https://doi.org/10.1525/collabra.33267](https://doi.org/10.1525/collabra.33267) - Schönbrodt, F. D., & Perugini, M. (2013). At what sample size do correlations stabilize? Journal of Research in Personality, 47(5), 609–612. [https://doi.org/10.1016/j.jrp.2013.05.009](https://doi.org/10.1016/j.jrp.2013.05.009) - Schönbrodt, F. D., Wagenmakers, E.-J., Zehetleitner, M., & Perugini, M. (2017). Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences. Psychological Methods, 22, 322–339. doi:10.1037/met0000061 [OSF project with reproducible code, workshop slides, presentation slides] - Wicherts, J. M., Veldkamp, C. L. S., Augusteijn, H. E. M., Bakker, M., van Aert, R. C. M., & van Assen, M. A. L. M. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. Frontiers in Psychology, 7, 1832. [https://doi.org/10.3389/fpsyg.2016.01832](https://doi.org/10.3389/fpsyg.2016.01832)""",
[[@hofman2023]],Hofman-2023-reaccessing-test-data-2,Collecting new data and refitting model after observing model evaluation / model checking results (optional stopping rules).,Data Curation,Sample Curation,"Model Construction, Model Evaluation",Data Processing,TRUE,input,,"""The other three participants described strategies that involved re-accessing test data"" (without reporting).","""P1 described how they might try reducing the model size, in terms of the number of neurons, in addition to checking the data distribution. If the model continued to fail after such checks would they attempt to acquire new data. P2 described how they would try varying the hyper-parameter settings in cross-validation, and if this failed to improve test performance, systematically test the effects of feature combinations on performance, followed by rethinking their pre-processing steps.""",,"""if not reported, they would jeopardize the reproducibility of the reported test metrics.""",
[[@Liu2020]],Liu-2020-executing-alternative-analyses,"Conducting multiple different analyses or model variations after observing model checking / model performance results, selectively reporting only those analyses that yield favourable results without disclosing the full range of analyses performed.",Model Fishing,Executing Alternative Analyses,Model Evaluation,,,"input, model, output",,,,,,
[[@Babel2019]],Babel-2019-constructing-new-model,Constructing new model / using new modelling approach rather than applying pre-existing one that might be superior.,Poor Practice,Poor Practice,Model Construction,,FALSE,model,Novel model / approach may be inferior to pre-existing model / approach.,,,,"""Constructing a new model rather than using a pre-existing one could prove profitable in the long term. Interviewee B reported that while no direct application was determined, the model would contribute to establish an innovative orientation of the research group in hydrology: its existence would open up collaborations with other groups and funding opportunities, as well as enable to attract researchers whose expertise would be durably beneficial for the institute.""",
[[@Nagy2025]],Nagy-2025-Discretizing-continuous-variables,Discretising continuous variables after observing model checking / model performance results.,S-hacking,Sample Curation,"Model Construction, Model Evaluation, Model Application",Data Processing,TRUE,input,Overrates model performance.,"""Taking a continuous variable and making it categorical without proper justification and transparent reporting. (1) Researcher doesn’t find an association between depression and continuous age variables, and recodes age into young and old categories. After that, age groups show a significant association with depression. An independent samples t-test is reported instead of a correlation.""",,,"""Inflated or deflated effect size estimates, Inflated type I or type II error, Reduced replicability, Compromised generalizability.""",
[[@Nakagawa2017]],Nakagawa-2017-data-dredging,Dredging fitted models for statistical significance or other outcome variable.,S-hacking,S-hacking,"Model Construction, Model Evaluation","Model Selection, Model Performance Metric",TRUE,"model, output",Spuriously selected model.,"""This means that, as a reader, you may want to check for the inclusion of too many [[predictors]] / [[moderators]] in a single model, or ‘[[over-fitting]]’ (the rule of thumb is that the authors may need at least ten effect sizes per estimated moderator) \[64\], and for ‘fishing expeditions’ (also known as ‘[[data dredging]]’ or ‘[[p hacking]]’; that is, non-hypothesis-based exploration for statistical significance \[28, 65, 66\])"" p.7",,"Similar to Model Dredging, but model space is constrained by some performance metric or other outcome variable.",,
[[@MacNally2000]],MacNally2000-dredging,"Dredging for models in unconstrained model space, where model space is not informed by theory or consists of models that are biologically implausible.",Model Fishing,Model Dredging,Model Construction,"Model Specification, Model Selection",TRUE,model,'Best' or 'final' models are only spuriously selected. ,"“Statistical tinkering, which really what the entire domain of model selection is about, can never be a substitute for intelligent prior selection of independent variables that may influence the dependent variable (e.g. Flack and Chang 1987. Ecologists and conservation biologists too often measure almost everything they can (especially in these days of automated probes and data loggers, remote sensing and GIS) and hope that something important will by ‘fished out’ of the resultant murky data. The variable-selection process will be substantially improved – and, therefore, the inferences too – if that process involves building upon existing knowledge and theory. In a sense, ‘forcing’ variables to be included in MR settings (e.g. Panzer and Schwartz 1998) is consistent with this philosophy because workers adjudge, ab initio, that these variables are crucial to the understanding of the system. Thus, all of the methods described in this paper are adjuncts rather than definitive expressions of the reasons for variation in the critical dependent variables and should be couched in these terms.”",,"Unfortunately, pseudoreplication stemming from inappropriate error terms in mixed models is very common in ecology and evolution [3–9]. For example, an inspection of all papers utilizing ‘experimental evolution’ and published in the Journal of Evolutionary Biology during 2015–2019 (N = 40) revealed 11 cases of appropriate modeling, eight cases where the analyses were at least partly inappropriate, and 21 cases where inferential models and methods were not given in sufficient detail to assess whether the analyses were appropriate (many of which were likely inappropriate).",,
[[@Shmueli2010]],shmuelli-model-selection-on-F-statistic,"Dredging for models in unconstrained model space, where model space is not informed by theory or consists of models that are biologically implausible.",Model Fishing,Model Dredging,Model Construction,Model Selection,TRUE,model,,"""Stepwise-type methods, which use overall F statistics to include and/or exclude variables, might appear suitable for achieving high explanatory power. However, optimizing explanatory power in this fash- ion conceptually contradicts the validation step, where variable inclusion/exclusion and the structure of the statistical model are carefully designed to represent the theoretical model. Hence, proper explanatory model selection is performed in a constrained manner. In the words of Jaccard (2001): Trimming potentially theoretically mean- ingful variables is not advisable unless one is quite certain that the coefficient for the variable is near zero, that the variable is in- consequential, and that trimming will not introduce misspecification error.""",,,,
[[@Risbey2005]],Risbey-SelectiveDebugging,Error checking only when unexpected or anomalous results are produced.,Selective Debugging,Selective Debugging,"Model Construction, Model Evaluation, Model Application",Model Verification,FALSE,"input, model, output",,"""When model code reaches a critical size (exceeded by all but the simplest models), error checking tends to occur only (or mostly) when results depart from expectations, and thus has more of the character of systematic bias than systematic checking.""",,,"""If the model is scrutinized mostly in those situations when strange results occur, then strange results will tend to be corrected (when a model error is found) so that the model reconfirms expectations. On the flip side, when model results confirm expectations, error checking tends to be only cursory and the model will go uncorrected, even if wrong. Thus, there is a bias towards confirmation of expectations.""",
[[@Bodner2020]],Bodner-2020-failing-to-define-prediction-properties,Failing to define model prediction properties.,Poor Practice,Poor Practice: pre-QRP,Model Evaluation,Model Performance Metric,FALSE,output,"Aids in choosing most appropriate model type. Increases transparency, and therefore improves accuracy and reliability.","Inverse of ""Establishing expected prediction properties refers to establishing a priori the characteristics and expected accuracy and/or precision levels of predictions (Refsgaard et al. 2007). For example, we may specify if predictions will be qualitative or quantitative, and if quantitative, whether predictions will be regarded as first approximations or precise values. We may also state whether they will be deterministic or probabilistic and what level of accuracy is acceptable given the model purpose. Additionally, if the predictions are probabilistic, we may state the amount of uncertainty expected given our purpose and knowledge about the model, data, and system.""",,"This is the next step on from the problem formulation phase. It's the failure to precisely operationalise the model evaluation procedure based on the problem articulated during the problem formulation phase, perhaps because the problem was imprecisely formulated. It can provide the opportunity for QRPs.",,
[[@Bodner2020]]],Bodner-2020-failing-to-define-model-purpose,"Failing to define or inadequately defining model purpose, framing and or scope.",Poor Practice,Poor Practice: pre-QRP,Model Construction,,FALSE,"model, output, input","Stating the model purpose, scope and framing clearly helps to clarify expectations about what a model can do, and how we should use it, therefore avoiding, controversy, mis-application, and misinterpretation. Unclear model purposes / inappropriate scope can introduce context uncertainty into the model.","""The first step to building an appropriate predictive model is to establish the context and the frame of the question. \[...\] Explicitly defining the purpose refers to specifying the intentions of a predictive model, which could be, for example, to inform policy management (Taylor and Hastings 2004, Lowe et al. 2014) or to test a theory (e.g., Bailly et al. 2014, Borregaard et al. 2017 ) in either a general or specific system.""",,,,
[[@Bodner2020]],Auspurg-2021-vague-parameter-definition,Failure to clearly define research question or give precise definition of parameter of interest.,Poor Practice,Poor Practice: pre-QRP,Model Construction,"Model Purpose, Outcome Variable",FALSE,"model, output",,Inverse of: '*clearly defined research question and give a precise definition of the parameter of interest.*',,,,
[[@Bennett2013]],Bennett-2013-swapping-out-multiple-weights,Failure to establish relative weighting of performance measures prior to beginning modelling.,Poor Practice,Poor Practice: pre-QRP,Model Evaluation,Performance Measure Weighting,FALSE,output,,"""There may be even more subjectivity in considering the results of quantitative testing. Suppose one model is superior to another according to one metric, while it is the reverse according to another metric. What is the weight to be assigned to individual quantitative criteria in the overall assessment? How are these weights decided and by whom?""",,,,
[[@Bodner2020]],Bodner-2020-a-priori-performance-metrics,"Failure to explicitly state the model purpose, and / or failure to establish a priori performance metrics and measures after establishing the model purpose before beginning modelling.",Poor Practice,Poor Practice: pre-QRP,"Model Evaluation, Problem Formulation","Model Performance Metric, Model Purpose",FALSE,"output, model",,"Once the purpose has been specified, we should establish performance metrics for this purpose -- i.e. what properties or characteristics of prediction do we wish to achieve with the model? This should be set *a priori* (examples given): ""Establishing expected prediction properties refers to establishing a priori the characteristics and expected accuracy and/or precision levels of predictions (Refsgaard et al. 2007). For example, we may specify if predictions will be qualitative or quantitative, and if quantitative, whether predictions will be regarded as first approximations or precise values. We may also state whether they will be deterministic or probabilistic and what level of accuracy is acceptable given the model purpose. Additionally, if the predictions are probabilistic, we may state the amount of uncertainty expected given our purpose and knowledge about the model, data, and system.""",,,"""a clear purpose helps avoid unnecessary controversies that may just stem from misunderstandings regarding what a model is and is not supposed to do (Grimm et al. 2010). \[...\] Defining the explicit model purpose prevents misinterpretation of the model’s goals and limitations (Grimm et al. 2010)  and can lead to more informed model choices \[...\]  A priori establishment of expected prediction properties establishes a baseline for model evaluations, thus increasing transparency, and allows for mismatches between desired and observed properties to be addressed, thus improving accuracy and reliability.""",
[[@Fourcade2018]],Fourcade-2018-not-using-biologically-informed-predictors,Failure to use biologically informed / justified predictor variables.,Poor Practice,Poor Practice,Model Construction,Model Specification,FALSE,"model, input",,,,See also [[QRP_modelling_database/Auspurg-2021-improper-model-specification]],,"""We recommend that researchers carefully select variables according to the species’ ecology and evaluate models only according to their capacity to be transfered \[sic\] in distant areas."""
[[@Auspurg2021]],Auspurg-2021-improper-model-specification,Failure to use theory in guiding model specification / using default controls in regression model that are uninformed by theory.,Poor Practice,Poor Practice: pre-QRP,Model Construction,"Model Specification, Model Tuning",FALSE,model,,Inverse of '*theory-guided causal reasoning to justify an appropriate model specification for identifying the parameter of interest. It is insufficient to simply throw the “usual suspects” as controls in a regression model.*',"""Unfortunately, this is common practice \['throwing the ""usual suspects"" as controls in regression model'\]""",,"""*non-rigorous social research that does not start with a clear research question provides divergent results. However, rigorous social science research is able to provide a more consistent answer.*"" See also: [@Bodner2020]: ""model structure uncertainty may arise from unwittingly excluding variables of influence, utilizing surrogate variables, and/or approximating functional forms [@AscoughII:2008br].""",
[[@Pu2018]],Pu2018-persistence-of-impressions,"Focusing only on data / models that seemingly supports expectation or hypotheses, and disregarding evidence that does not corroborate hypotheses or expectation (even if present).",Model Fishing,,Model Construction,,,"model, input",Spuriously selected model that is overfit to the data and may poorly generalise.,"""Analysts may also be subject to problems of persistence of impressions based on discredited evidence if during some portion of exploratory analysis they come up with an interest—but incorrect—model or hypothesis, they may be slow to abandon it in the face of additional evidence later in exploration. Or worse, they may resist abandoning a model that was discovered during exploratory analysis but refuted during confirmatory analysis.""",,,,
[[@paulengelschalt2023]],Engelscheldt-2022-oversensitivity-to-consistency-bias,"Focusing only on data that seemingly supports expectation or hypotheses, and disregarding data that does not corroborate hypotheses or expectation (even if present).",Data Curation,,Model Construction,,,"model, input",Spuriously selected model that is overfit to the data and may poorly generalise.,"""As one way to explain why analysts might be susceptible to the forking paths problem in visual analytics, we look to Wall et al.’s \[30\] metrics for measuring cognitive biases in visual analytics. These biases provide one reason that might cause analysts to be susceptible to the forking paths problem. For instance, the oversensitivity to consistency bias manifests as an analyst focusing only on data that seem to support their all-encompassing hypothesis and ignoring data that does not corroborate their hypothesis (even if present). With such biased exploration, the analyst is likely to overfit to their sliced data and generate exploratory conclusions that are unlikely to generalize.""",,,"From this perspective, an analyst engaged in data exploration is building an explanation of the data (possibly hypotheses, or more generally some kind of mental model of the data), and cognitive biases may lead them to explore only subsets of the data, leading to biased inferences. In the end, the analyst considers only their final mental model of the data and the path that got them there, not other possible paths or models they might have considered along the way. \[...\] With such biased exploration, the analyst is likely to overfit to their sliced data and generate exploratory conclusions that are unlikely to generalize.",
[[@Pu2018]],Pu2018-vividness-criterion-bias,"Focusing only on data that seemingly supports expectation or hypotheses, and disregarding data that does not corroborate hypotheses or expectation (even if present).",Data Curation,Overfitting,Model Construction,Conceptual Model Specification,,"model, input",Spuriously selected model that is overfit to the data and may poorly generalise.,"""Many of these biases dovetail with the forking paths problem: the vividness criterion, for example, might lead an analyst to pay undue attention to certain data, building a mental model that overfits to the particulars of a sample but generalizes poorly.""",,,"""building a mental model that overfits to the particulars of a sample but generalizes poorly""",
[[@McDermott2021]],McDermott-2021-optimising-method,"For studies developing new methods or approaches, optimising method / approach to improve the performance against baseline methods.",Poor Practice,Overhyping,Model Construction,Method Selection / Model Selection,FALSE,model,,"""Issues with statistical reproducibility may also arise due to an effect whereby researchers routinely spend more effort optimizing their method than is spent on the baseline methods against which their method is compared (**11**).""",,,,
[[@Fourcade2018]],Fourcade-2018-improper-use-of-evaluation-measures,Improper use of model evaluation metrics (e.g. using an evaluation metric ill-suited to the stated model purpose).,Poor Practice,Poor Practice,Model Evaluation,Model Performance Metric,FALSE,output,"Improper use of evaluation measures, such as AUC, may overrate the performance of SDMs with biologically meaningless predictors.","""Post-hoc evaluation of distribution models is commonly performed to assess their predictive performance and statistical significance  (Peterson et al., 2011). The most common diagnostic metrics in the area  of SDMs is the area under the receiver operating curve (ROC) (AUC;  Porfirio et al., 2014), obtained by plotting the model sensitivity against  its false positive rate at all possible thresholds (Hanley & McNeil, 1982)."" \[...\] ""it has been widely adopted by the SDM  community to measure the performance of models in discriminating  between presences and absences of species (Lobo, Jimenez-Valverde, &  Real, 2008). AUC has been adapted to presence-only (or presencebackground) modelling approaches by comparing the predicted suitability at presence points versus background points taken from the training  area. In this context, the implementation of AUC in a SDM framework  is usually carried out by partitioning species occurrences into two sets:  a training dataset, which is used to compute the model, and a test dataset that is used thereafter to evaluate the model’s discrimination ability  (Fielding & Bell, 1997). This process can be repeated several times, each  partition being used alternately to train and to test the model. This  approach assumes that training and testing data are spatially independent, an assumption rarely fully met in practice, especially when occurrences are randomly partitioned (Veloz, 2009)."" \[...\]. ""We have demonstrated that SDMs computed using meaningless variables as input environmental predictors are often classified as good or even excellent according to the most widely used evaluation measures.""","""The flaws of AUC that our results suggest have been known for a while (Lobo et al., 2008). Yet, it is still used in more than 80% of distribution modelling papers published in recent years in leading biogeography journals. More than half of SDM studies even relied on this single measure alone to assess the performance of their models."" \[...\] ""As it appears that standard evaluation metrics can hardly discriminate biologically relevant SDMs from meaningless models"" \[...\] ""our results showed that, as for AUC, c. 30% of models based on pseudopredictors were better evaluated (lower AICc) than those based on real environment variables."" \[...\] ""We showed, in line with other authors (Jimenez-Valverde, 2012; Lobo et al., 2008), that two widely used evaluation metrics overrate the performance of biologically meaningless SDMs.""",,"""We showed, in line with other authors (Jimenez-Valverde, 2012; Lobo et al., 2008), that two widely used evaluation metrics overrate the performance of biologically meaningless SDMs."" \[...\]""We have demonstrated that SDMs computed using meaningless variables as input environmental predictors are often classified as good or even excellent according to the most widely used evaluation measures."" \[...\] ""Moreover, AUC has  been recognized as a highly questionable measure for several years  (Lobo et al., 2008), especially when used with background data instead  of true absences (Jimenez-Valverde, 2012). Many alternative metrics  have been proposed to evaluate SDMs (see for example Allouche,  Tsoar, & Kadmon, 2006; Hijmans, 2012; Phillips & Elith, 2010). However, despite these criticisms, so far none of these alternatives seems to  have taken over from AUC in most SDM studies.""","""inferences based on  SDMs could be improved by using an effective method of model evaluation."""
[[@Josefsson2020]],Joseffson-2020-false-causal-claim,Misreporting correlative claims using causal language.,Overhyping,Overhyping,Model Application,Inference,TRUE,output,"For researchers, the problem (and solution) to misusing causal language to report correlational findings from observational studies is likely institutional. In addition, there may be unique extrinsic constraints on researchers working at the [[practitioner-research-policy nexus]], where by policy makers are funding the research, and expect clear messages about the research findings such that researchers do not report on study design limitations [[study limitation reporting]]. Another possible explanation is that applied journals pressure authors into providing clear directives to practitioners and policy-makers, However, given that this problem occurs in other disciplines, I would say that this is a science-wide problem not unique to discipline, but caused by some other facet of the science-publication pipeline.","Data derived from observational studies may be biased due to selection bias, and because the driver of observed effects can't be properly identified because of confounding - thus causal inference is not possible with observational studies. Because observations are merely correlative, and the true driver of an effect can be hidden when attempting to make causal inferences from observational studies. This results in false confidence, and possibly false negatives, in drivers of observed patterns. ""observational study designs are generally restricted in terms of their capacity for causal inference (Underwood, 1997). The main problem of implying causation from correlative observations is that it may divert attention from the real reasons for any observed effect, promoting false confidence in the drivers of the observed pattern."" p.2","""Most intervention evaluations used observational study-designs observational studies, and of those studies, the majority used causal language (inappropriately), with few acknowledging base-line bias at all. Many studies used hedged causal statements to soften causal wording. \[...\] despite the correlative nature of the data in the Obs-CI studies, definitive (i.e., without hedging) causal word ing was common (66%). Hedged causal statements, using words such as “may,” “appears to,” and “indicates” to soften causal terminology, was used in another 16% of the studies (Figure 2a; Table S4 in the Supporting Information)."" p.5",,"""It has been suggested that competition among researchers and journals for high impact publications may foster a culture to neglect inherent and fundamental flaws related to study design, or to falsely make causal claims, in order to increase the seeming significance of research findings (Cofield, Corona, & Allison, 2010; Lipton & Ødegaard, 2005; Puhan et al., 2012; Robinson et al., 2007). \[...\] A culture to let study design limitations go by unremarked may also be fostered at the interface between applied sciences and policy when policymakers provide funding and expect clear answers to research questions. \[...\] Similarly, editors of applied journals may suggest authors to provide clear directives to practitioners (Robinson et al., 2007; but see Cofield et al., 2010, who found no link between funding source and causal language).""",
[[@Nagy2025]],Nagy-2025-missing-data-hacking,Missing data hacking: changing the strategy to handle missing data after fitting the model and observing model checking / model performance evaluation results.,Data Curation,Sample Curation,"Model Construction, Model Evaluation",Data Processing,TRUE,input,Overrates model performance and may result in spuriously selected model.,"""Choosing the strategy to handle missing data based on the impact on the results. (1) A researcher tries three ways of handling missing data, for example, listwise deletion, multiple imputation, and inverse probability weighting. The expected results only appear with inverse probability weighting. The researcher reports only this strategy in the paper and leaves out results with listwise deletion and multiple imputation. (2) Can also be within a single method, specifically multiple imputation, since it uses one or more variables to replace missing data, and the choice of these variables is up to the researcher, but can also be statistically based.""",,,"""Inflated or deflated effect size estimates, Inflated type I or type II error, Reduced reproducibility, Reduced replicability""",
[[@Arnqvist2020]],Arnqvist-2020-model-misspecification-pseudoreplication,Misspecification of random effects structure by premature pruning of random effects / choosing random effect structure based on data rather than study design.,Poor Practice,,Model Construction,"Model Specification, Model Selection",FALSE,model,"Model misspecification caused by Incorrect error terms and random effects structures is common in ecology and can lead to pseudoreplication, which inflates Type I error rates and results in overconfidence.","""Do not perform model simplification or reduction of random terms prior to arriving at your inferential model, where nonsignificant effects (including random interactions) that reflect levels of replication or design in your data are removed. This is because dropping random terms can greatly affect tests of fixed effects in a manner resulting in pseudoreplication [6]. Trust the full model – the random effects structure should be determined by your design rather than by your data [15].""","""Unfortunately, pseudoreplication stemming from inappropriate error terms in mixed models is very common in ecology and evolution [3–9]. For example, an inspection of all papers utilizing ‘experimental evolution’ and published in the Journal of Evolutionary Biology during 2015–2019 (N = 40) revealed 11 cases of appropriate modeling, eight cases where the analyses were at least partly inappropriate, and 21 cases where inferential models and methods were not given in sufficient detail to assess whether the analyses were appropriate (many of which were likely inappropriate).""",,"""pseudoreplication is sometimes believed to be a problem that emerges only when we engage in explicit null hypothesis testing, and that inferences based on parameter estimates and their confidence intervals (or the Bayesian equivalent, credible intervals) are somehow immune to these issues. They are not. Here, a misspecification of the random effects structure can generate inferential overconfidence by inappropriate shrinkage of the inferential intervals. [...] Yet, an appropriate random effect structure is as essential here as it is when engaging in more traditional model fitting and hypothesis testing strategies [15]. [...] Modeling hierarchical data by complex linear mixed models (LMMs) can be a remedy for inferential errors due to pseudoreplication (e.g., [10–13]). However, importantly, this will only ever be true if appropriate models are fitted to data (e.g., [6,13–15]). If random effects structures are not modeled correctly, then LMMs offer no antidote. In fact, the now widespread use of such models seems to instead have aggravated the general problem of pseudoreplication in biology. [...] Second, misspecification of the random effect structure leads to pseudoreplication and inappropriate deflation of P-values (and higher type I error rates). This overconfidence can stem from inflated test statistics, for example, by F-ratios that are formed by incorrect denominator mean squares, and also from improper degrees of freedom when test statistics are converted to P-values.""",See @Arnqvist2020 for list of practices to avoid model misspecification / pseudoreplication caused by these practices.
[[@Campbell2021]],Campbell-2021-model-selection-bias,"Model Selection Bias: deciding post hoc which distributional assumptions should be accepted, i.e. performing preliminary tests for distributional assumptions on the same data used for model selection. For example, checking for zero-inflation or overdispersion on the same data used for model selection.",Poor Practice,Data Peeking,Model Construction,Model Selection,FALSE,data,"Results in model selection bias, i.e. inflation of type 1 errors when sample sizes are small.","The model selection bias at issue here is not the better known model selection bias associated with deciding post hoc which variables to include in the model, for example, the model selection bias associated with stepwise regression (Hurvich & Tsai, 1990; Whittingham et al., 2005). Instead, here we are concerned with the potential bias introduced when deciding post hoc which distributional assumptions should be accepted [...] However, recently, some researchers have warned against preliminary testing for distributional assumptions; for example, Shuster (2005) and Wells and Hintze (2007). Their warnings are based on the following concern: since the preliminary tests are applied to the same data as the main hypothesis tests, this multi-stage procedure amounts to ‘using the data twice’ or ‘double dipping’; see Devezer et al. (2020) and Kriegeskorte et al. (2009).","Our simulation study suggests that, if sample sizes are sufficiently large, there is little need to worry about model selection bias following a series of sequential score tests. However, when sample sizes are small, our simulation study demonstrated that model selection bias can lead to potentially substantial type 1 error inflation",,"Regardless of the method used for model selection, the process of selecting a model for inference based on the data ignores a crucial source of uncertainty. And, if the model selection procedure and the hypothesis testing which follows are not independent, type 1 error inflation may occur.",
[[@Hildebrandt2018]],Hildebrandt-2018-removal-of-noise,"Modifying exclusion criteria or excluding data points, such as outliers or other values, without justification and prior planning, i.e. after fitting model and observing model evaluation / model checking results.",Data Curation,Sample Curation,"Model Construction, Model Evaluation",Data Processing,TRUE,input,Overfitting.," ""Before training their learning algorithm (‘the learner’) on the data, developers will attempt to remove irrelevant or incorrect ‘noise’, depending on the goal of the operation. """,,,"""They always run the risk of removing highly relevant data, even though the risk can be reduced by testing on differently curated data sets.""",
[[@Nagy2025]],Nagy-2025-ad-hoc-exclusion-criteria,"Modifying exclusion criteria or excluding data points, such as outliers or other values, without justification and prior planning, i.e. after fitting model and observing model evaluation / model checking results.",Data Curation,Sample Curation,Model Evaluation,"Data Processing, Model Performance Metric",TRUE,input,Overrates model performance and may result in spuriously selected model.,"""Exclusion of participants without proper justification transparent reporting. (1) Researcher finds that a correlation between two variables is not significant. After removing two participants - who should be included - the association becomes significant. Then the researcher comes up with post hoc exclusion criteria for those participants. (2) A researcher doesn’t find an expected association between perceived stress and personality. When looking only at the top 25% of perceived stress scores, the association is there. They go on to report the top 25% scores as their population of interest and do not disclose that they looked at the rest of the sample population.""",,,"""Inflated or deflated effect size estimates, Inflated type I or type II error, Compromised generalizability, Reduced replicability, Reduced reproducibility""",
[[@Nagy2025]],Nagy-2025-excluding-data-points,"Modifying exclusion criteria or excluding data points, such as outliers or other values, without justification and prior planning, i.e. after fitting model and observing model evaluation / model checking results.",Data Curation,Sample Curation,"Model Construction, Model Evaluation",Data Processing,TRUE,input,Overrates model performance and may result in spuriously selected model.,"""Exclusion of data points or outliers without proper justification and transparent reporting. e.g. (1) Removing individual reaction time trials based on post hoc criteria. (2) Trying different outlier cut-off criteria until an effect is statistically significant""",,,"""Inflated or deflated effect size estimates, Inflated type I or type II error, Compromised generalizability, Reduced replicability, Reduced reproducibility""",
[[@Babel2019]],Babel-2019-ideologically-driven-simplicity,Over-simplifying models due to ideological stance rather than based on modelling objectives or performance measures linked to those objectives.,Model Fishing,Poor Practice,Model Construction,Model Specification,FALSE,model,Model is not fit for purpose / may not be the most appropriate or adequate model.,,"""Ecologist E explained how his local model of vegetation dynamics was at first “quite complicated” until a colleague of his department “forced \[him\] to make it as simple as possible”, leading him to discard terms in the time transfer function describing plant density, selecting linear functions and removing any differentiation between plant components. Ecologist E consequently parameterized or suppressed the terms of the transfer function the colleague had addressed, while they had been physically represented before.  Interestingly, while ecologist E seemingly easily accepted the input of his colleague, he rejected comments of the reviewers of the first journal he submitted the manuscript on his model to, which deemed it too simplistic. Conversely, the interviewee modified the (nowadays widely used) model towards even greater simplicity with one of his doctoral students in later stages of model development.  \[...\] Simplicity was repeatedly advocated in the course of the interview as a form of epistemic virtue to which the interviewee was increasingly tending over his career. While he had already built “very simple models” during his PhD, he had started to complexify them after it; the encounter with his colleague appeared to have resonated with his epistemic stance and brought him back to an approach of modelling which he nowadays strongly identified himself with. His colleague’s questions, we could interpret, had steered the representation towards their shared epistemic stance. The reviewers’ and hydrologists’ comments, distant from it, had been disregarded. \[...\] The consistent and repeated use of the term “simplicity” in ecologist E’s discourse could be linked to the simplicity/complexity dichotomy being often discussed within the discipline (e.g. Peck, 2004, and Merow et al., 2014), simplicity having historically been a well-established epistemic view on modeling in ecology (see the analysis of Evans et al., 2013).""",,,
[[@lewis2023]],Lewis-2023-overfitting-models-to-calibration-data,Overfitting a model to calibration dataset by including too many moderators or predictors relative to the size and complexity of the dataset.,Poor Practice,Overfitting,Model Construction,Model Specification,TRUE,model,Model will be overfit to data and generate spurious results and predictions that do not generalise to new / out-of-sample data.,"""calibrating a model to closely match one dataset, at the expense of predictive power for new data), as can happen when all available data are used to fit a model.""",,,"""spurious results that result from overfitting models to data (i.e. calibrating a model to closely match one dataset, at the expense of predictive power for new data), as can happen when all available data are used to fit a model.""",
[[@McDermott2021]],McDermott-2021-overfitting-model,Overfitting a model to calibration dataset by including too many moderators or predictors relative to the size and complexity of the dataset.,Poor Practice,Overfitting,Model Construction,"Model Tuning, Model Specification",TRUE,model,Especially at risk when datasets are small.,,,,"""It is likely that these problems with statistical reproducibility also plague MLH papers as well, especially given that health datasets tend to be relatively small, have high dimensionality, are noisy, and often suffer from sparse/irregular sampling.""",
[[@Nakagawa2017]],Nakagawa-2017-fishing-expedition,Overfitting a model to calibration dataset by including too many moderators or predictors relative to the size and complexity of the dataset.,Poor Practice,Overfitting,Model Construction,Model Specification,TRUE,model,Spuriously selected model that is overfit to the data and may poorly generalise. ,"""This means that, as a reader, you may want to check for the inclusion of too many [[predictors]] / [[moderators]] in a single model, or ‘[[over-fitting]]’ (the rule of thumb is that the authors may need at least ten effect sizes per estimated moderator) \[64\], and for ‘fishing expeditions’ (also known as ‘[[data dredging]]’ or ‘[[p hacking]]’; that is, non-hypothesis-based exploration for statistical significance \[28, 65, 66\])"" p.7",,,,
[[@lewis2023]],Lewis-2023-overfitting-by-adding-parameters,Overfitting model to calibration data by adding additional parameters after observing fitted model.,Poor Practice,Overfitting,Model Construction,"Model Specification, Model Tuning",TRUE,model,"Model is overfitted to one dataset, at the expense of predictive power for new data. Model is unable to successfully predict out-of-sample data.","""Ecological forecasts that successfully predict out-of-sample  data often come from very simple models (Chevalier & Knape, 2020; Ward et al., 2014), in part because they cannot increase model fit simply by adding additional parameters and overfitting to data. \[...\] overfitting models to data (i.e. calibrating a model closely to match one dataset, at the expense of predictive power for new data).""","""For example,  Averill et al. (2021) found that out-of-sample validations for some  forecasts of soil microbial taxonomic and functional groups were  systematically biased due to small differences in measurement techniques.""",,,
[[@Harder2020]],Harder-2020-inferior-methods,"Overuse of inferior, familiar methods / failing to adopt new, superior or best-practice methods.",Poor Practice,Poor Practice,Model Construction,Modelling Approach,FALSE,"model, output, input",Inferior methods may yield inaccurate or wrong results.,"""A multiverse-of-methods analysis can be used to confront either of these sources of methodological variation. When one method is clearly superior to another, that superiority can usually be established through statistical reasoning or experimentation, without the use of a multiverse analysis. However, the task of persuading researchers that this difference is important enough to merit changing their methods can sometimes be difficult. As controversy over reproducible methods demonstrates, there is variation in the extent to which researchers adopt best-practice methods (John, Loewenstein, & Prelec, 2012).""","""If the inferior methods are more familiar, or if implementing the superior methods would require obtaining new knowledge or equipment, adopting the superior methods could lead researchers to incur some short-term costs in effort, time, or money. In these situations, motivated reasoning can lead researchers to dis- count arguments that they should make such methodological changes.""",,,
[[@feng2019]],Feng-2019-model-evaluation-dataset,Re-partitioning data after observing model evaluation or model checking results.,Model Fishing,,Model Evaluation,"Data Selection, Data Partitioning",,"input, model, output",Choice of dataset can influence evaluation results and contingent interpretations.,,,,"""Evaluation of a model is usually based on another independent dataset, or part of the dataset not used in model training. The choice of dataset can influence the evaluation results and the subsequent interpretations.""",
[[@Bennett2013]],Bennett-2013-using-calibration-validation-results-as-performance-validation,"Resubstitution: assessing model performance on training set only or failing to evaluate model on independent data, or on partitioned holdout data.",Poor Practice,Poor Practice,Model Evaluation,"Data Partitioning, Data Partitioning",FALSE,input,Model performance is overestimated / overrated or model is overfitted.,"""The same data are used for development and testing. Therefore, the performance evaluation will be the same as the calibration evaluation and model performance is likely to be overestimated because the model has been ‘tuned’ to the results. This approach is the least rigorous, and should be avoided.""",,,,
[[@Bodner2020]],Bodner-2020-failing-to-perform-out-of-sample-assessment,"Resubstitution: assessing model performance on training set only or failing to evaluate model on independent data, or on partitioned holdout data.",Poor Practice,Poor Practice,Model Evaluation,"Data Partitioning, Model Tuning",FALSE,"intput, output",Overrates model performance or results in spurious model specification.,"""Relying only on in-sample assessments is suboptimal (Mosteller and Tukey 1977)""",,,"""top-performing models from in-sample assessment may have worse transferability than lower ranked models (Wenger and Olden 2012) \[...\] both approaches \[(cross-validation and information-theoretic approaches on in-sample model assessment )\] may select for more complex models when uncertainties are not accounted for (Dietze 2017a), reinforcing the need for out-of-sample assessment and uncertainty quantification when selecting final models. \[...\] While out-of-sample assessment may be optimized with fully independent data, often this type of data is not available in ecology (Urban et al. 2016, Dietze et al. 2018""",
[[@Pu2018]],Pu2018-assessing-model-performance-on-training-set-only,"Resubstitution: assessing model performance on training set only or failing to evaluate model on independent data, or on partitioned holdout data.",Poor Practice,Poor Practice,"Model Evaluation, Model Construction","Data Partitioning, NA, Model Tuning",FALSE,input,Model performance will be overrated due to overfitting to training data.,"""Simply assessing model performance on a training set will lead to biased estimates of performance, because the model will have fit to some regular and irregular features of the sample at hand, but cannot distinguish between them.""",,,"""will lead to biased estimates of performance, because the model will have fit to some regular and irregular features of the sample at hand, but cannot distinguish between them.""",
[[@Auspurg2021]],Auspurg-2021-selective-reporting-robustness-checks,Selective reporting of robustness checks in support of main results.,Selective Reporting,Selective Reporting,Model Evaluation,Robustness Checks,TRUE,output,,"""However, there is evidence that robustness analyses are selectively reported that support the main results at 100 per- cent (Young and Holsteen 2017). Therefore, we need more serious robustness checks. In this vein, it should become standard to present the full distribution of estimates that can be obtained based on all reasonable specifications. Multiverse analysis, as used in this article, seems particularly promising in this regard. In addition, one can complement summary tables (e.g., descriptive statistics and regression tables) with visualizations that disclose the full variance in raw data and results (Cumming 2014; Healy and Moody 2014). If the robustness analysis shows that results vary widely over the model space, then researchers need to make explicit why they chose their specific model specification. This will increase transparency and credibility of social research.""","""However, there is evidence that [[robustness analyses]] are selectively reported that support the main results at 100 percent (Young and Holsteen 2017).""",,,"""it should become standard to present the full distribution of estimates that can be obtained based on all reasonable specifications. Multiverse analysis, as used in this article, seems particularly promising in this regard. In addition, one can complement summary tables (e.g., descriptive statistics and regression tables) with visualizations that disclose the full variance in raw data and results (Cumming 2014; Healy and Moody 2014). If the robustness analysis shows that results vary widely over the model space, then researchers need to make explicit why they chose their specific model specification. This will increase transparency and credibility of social research."""
[[@Nagy2025]],Nagy-2025-selective-sampling,"Selective sampling / biased sampling, e.g. convenience or opportunistic sampling.",Poor Practice,Sample Curation,Model Construction,Data collection,FALSE,input,,"""Collecting a sample in a way that biases the findings. e.g. (1) Researcher tests the likeability of chocolate on a group of children only in order to find that everyone loves it. (2) Using uncomparable groups: The researcher tests if men are more aggressive than women. For comparison, women from a university are compared with men from a prison. (3) Picking a subsample of a panel dataset to find the desired results."" \[...\] ""convenience sampling""",,"Sometimes opportunistic sampling is the best we can do when collecting ecological datasets. Only clearly problematic when not communicated or intentional, which in the latter case is a clear case of misconduct.","""Inflated or deflated effect size estimates, Inflated type I or type II error, Compromised generalizability, Reduced replicability if sampling bias is not disclosed""",
[[@hofman2023]],Hofman-2023-cherry-picking-comparisons,Selectively reporting comparisons that support a foregone conclusion.,Overhyping,Selective Reporting,Model Application,Model Results,TRUE,output,,"""cherry-picking comparisons to support a foregone conclusion, for example, are different from issues of leakage.""",,The purpose of this practice is to 'overhype' the model results.,,
[[@Bennett2013]],Bennett-2013-selective-reporting-weighting-evaluation-1,Selectively reporting performance metrics that increase perception of performance after fitting model and/or observing model evaluation / model checking results.,Selective Reporting,Selective Reporting,"Model Construction, Model Evaluation",Model Performance Metric,TRUE,output,,"""In every study, the researchers may tweak the analyses to make sure that their model is better. There are many statistical validation methodologies which may be combined at will to favor one particular result. The plague of selective reporting based on p- values, and the misuse of p-values in general (Wasserstein et al., 2019) can still affect internal and external validation [...] There may be even more subjectivity in considering the results of quantitative testing. Suppose one model is superior to another according to one metric, while it is the reverse according to another metric. What is the weight to be assigned to individual quantitative criteria in the overall assessment? How are these weights decided and by whom?""",,,,
[[@Hildebrandt2018]],Hildebrandt-2018-cherry-picking-performance-metrics,Selectively reporting performance metrics that increase perception of performance after fitting model and/or observing model evaluation / model checking results.,Selective Reporting,Selective Reporting,"Model Construction, Model Evaluation",Model Performance Metric,TRUE,output,,"""Next to bias in the data and the hypotheses space, the outcome of an ML application may be biased due to cherry picking with regard to the performance metric (P). This metric determines the accuracy of the system, based on the task (T) the system aims to perform and the data (E) it trains on. As one can imagine, if some metric P1 achieves 67% accuracy, whereas another metric P2 achieves 98% accuracy, the temptation to use only P2 and boast high accuracy is formidable. will call this P-hacking, as it seems to be the twin sister of p-hacking (Berman et al. 2018).""",,,"""Especially in systems that are difficult to interpret high accuracy does not mean much, as the system may be getting things wrong despite the accuracy. The opacity of the underlying causality (e.g. in the case of medical diagnosis) or reasoning (e.g. in the case of quantified legal prediction) easily hides potential misfits.""",
[[@Nagy2025]],Nagy-2025-modifying-measurements-2,Selectively reporting performance metrics that increase perception of performance after fitting model and/or observing model evaluation / model checking results.,Selective Reporting,Selective Reporting,"Model Construction, Model Evaluation",Model Performance Metric,TRUE,output,Overrates model performance and may result in spuriously selected model.,"""Changing the properties of a measure/measurement to produce favorable results without proper justification and/or transparent reporting. (1) Researcher uses only a portion of the items from a longer scale. (2) Researcher combines items from different scales into a single measure. (3) Researcher chooses which EEG electrodes to aggregate based on the results.""",,Have split this practice into two.,"""Reduced replicability, Reduced reproducibility, Reduced validity of the measure, Inflated or deflated reliability of the measure, Inflated type I or type II error, Inflated or deflated effect size estimates.""",
[[@Harder2020]],Harder-2020-p-hacking-selective-reporting-methods,Trying out different outcome variables or model evaluation metrics unrelated to model objectives and selecting based on performance after fitting the model and/or observing model checking / model evaluation results.,Model Fishing,Executing Alternative Analyses,"Model Construction, Model Evaluation","Modelling Approach, Model Evaluation Approach",TRUE,output,,"'The second reason applies to decisions for which the question of which option is the “best” is more complicated, as reasonable arguments could be made for multiple choices. In these cases, it may not be possible to answer the question of which option is best. Instead, the important question is whether the different options are likely to lead to different study conclusions. If they are, then there is a risk that researchers may try out multiple methods or analyses, selectively report results from the ones that produced significant effects, and ignore those that “did not work,” contributing to an overrepresentation of Type I errors in the published literature. Analytic options that enable such a process are often referred to as researcher degrees of freedom (Simmons, Nelson, & Simonsohn, 2011).'",,"See also: Prosperi [-@Prosperi2019] who also describe all the ways researchers may try different modelling methods, and they distinguish between different models and different behaviours to change those models.

Could potentially look like S-hacking, but also could be legitimate under methodological / analytic uncertainty.",,
[[@Hildebrandt2018]],Hildebrandt-2018-choosing-performance-metrics-on-value-not-objective,Trying out different outcome variables or model evaluation metrics unrelated to model objectives and selecting based on performance after fitting the model and/or observing model checking / model evaluation results.,Model Fishing,Executing Alternative Analyses,Model Evaluation,Model Performance metric,TRUE,output,,"""Next to bias in the data and the hypotheses space, the outcome of an ML application may be biased due to cherry picking with regard to the performance metric (P). This metric determines the accuracy of the system, based on the task (T) the system aims to perform and the data (E) it trains on. As one can imagine, if some metric P1 achieves 67% accuracy, whereas another metric P2 achieves 98% accuracy, the temptation to use only P2 and boast high accuracy is formidable. will call this P-hacking, as it seems to be the twin sister of p-hacking (Berman et al. 2018).""",,,"""Especially in systems that are difficult to interpret high accuracy does not mean much, as the system may be getting things wrong despite the accuracy. The opacity of the underlying causality (e.g. in the case of medical diagnosis) or reasoning (e.g. in the case of quantified legal prediction) easily hides potential misfits.""",
[[@hofman2023]],Hofman-2023-ML-p-hacking,Trying out different outcome variables or model evaluation metrics unrelated to model objectives and selecting based on performance after fitting the model and/or observing model checking / model evaluation results.,Model Fishing,Executing Alternative Analyses,"Model Evaluation, Model Application","Model Performance Metric, Outcome Variable",TRUE,output,,"""Including making decisions about outcome variables and metrics after seeing results""","""Four participants specifically mentioned the value of reducing the equivalent of “p-hacking” in machine learning, including making decisions about outcome variables and metrics after seeing results (P2, P4, P5, P6 ). \[...\] pre-registering was perceived as at odds with typical practice. P6 described how it can be hard to identify the best metric to represent model performance a priori, suggesting that one could “develop the model with well-accepted metrics for the problem but then later find a better metric that justifies how the model performs,” but did not provide any specific examples of what they imagined.""",,,
[[@Nagy2025]],Nagy-2025-modifying-measurements-1,Trying out different outcome variables or model evaluation metrics unrelated to model objectives and selecting based on performance after fitting the model and/or observing model checking / model evaluation results.,Model Fishing,Executing Alternative Analyses,Model Evaluation,Model Performance Metric,TRUE,output,Overrates model performance and may result in spuriously selected model.,"""Changing the properties of a measure/measurement to produce favorable results without proper justification and/or transparent reporting. (1) Researcher uses only a portion of the items from a longer scale. (2) Researcher combines items from different scales into a single measure. (3) Researcher chooses which EEG electrodes to aggregate based on the results.""",,,"""Reduced replicability, Reduced reproducibility, Reduced validity of the measure, Inflated or deflated reliability of the measure, Inflated type I or type II error, Inflated or deflated effect size estimates.""",
[[@Hildebrandt2018]],Hildebrandt-2018-using-easily-available-data,"Using bad or easily obtainable, or inappropriate data to develop model.",Poor Practice,Sample Curation,Model Construction,Data Processing,FALSE,input,,"""The choice of data, the kind of translation it implies, the type of error it may contain and the way it has been curated all impact the accomplishments of ML. For instance, developers may use ‘low hanging fruit’, i.e. data that is easily available but not necessarily relevant or complete.""",,"Similar to ""Missing data hacking: changing the strategy to handle missing data after fitting the model and observing model checking / model performance evaluation results."" Is definitely a case of poor practice, and not so clear cut.","""This may result in bad ML applications (garbage in, garbage out or GIGO), and can be remedied either by obtaining other and/or more data, or by accepting that the data needed for the task cannot be harvested.""",
[[@Bodner2020]],Bodner-2020-using-information-theoretic-approachs-for-in-sample-assessment,Using information-theoretic approaches to perform in-sample model assessment.,Poor Practice,Poor Practice,Model Evaluation,Model Performance Metric,FALSE,"output, model",Information theoretic approaches do not provide a direct measure of predictive accuracy and therefor the selected model may be the 'best' of a poor selection. This links to the need for theory informing model-specification.,"""In ecological research, information-theoretic approaches are commonly adopted to perform in-sample assessment [@Hooten:2015vw].""","""commonly adopted"" in ecological research [@Hooten:2015vw].",,"""However, as they do not provide a direct measure of predictive accuracy, the best-performing model or models may only be the best of a poor selection (Taper et al. 2008).""",
[[@Babel2019]],Babel-2019-posteriorii-rationalising,,,HARKing,,,,,,,,,,
[[@Liu2020]],Liu-2020-selective-reporting,,,Selective Reporting,,,,,,,,,,
[[@Prosperi2019]],Prosperi-2019-HARKing,,,HARKing,"Model Evaluation, Model Application",,,,,"""HARK (Hypothesizing after results are known) can lead to data-dredging, data fishing, or p-hacking that is unduly manipulating data collection or statistical analysis in order to produce a statistically significant result. \[...\] HARK has been deconstructed in various ways (Rubin, 2017), including constructing, retrieving, and secretly suppressing hypotheses after results are known (CHARK, RHARK, SHARK). Not all types of HARK are considered detrimental: for instance, transparent HARK (THARK), i.e., reporting and discussing post hoc exploratory data analysis on empirical studies, has been deemed “beneficial to scientific progress and, in many cases, ethically required” (Hollenbeck and Wright, 2017). Nonetheless, it is often not possible to distinguish what kind of HARK has happened in a study.""",,,"""HARK and data dredging can take place due to costs and rewards: the costs being associated with collecting more data and running more experiments to test multiple hypotheses or to redesign and rerun studies; and the rewards for publishing negative studies being relatively low. The recognition that a large proportion of published research is likely to be false through conscious or unconscious HARK and data dredging (Macleod et al., 2014; Begley and [[@Ioannidis:2014cm]] Ioannidis, 2015) has ignited a reprodu- cibility crisis, especially in social, health, and biomedical sciences (Baker, 2016).""",
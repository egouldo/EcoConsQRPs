model_phase,model_subphase,qrp_coded,target,practice_coded,include,notes,reason_coded,practice_source,evidence_source,reason_source,solution_source,source
Model Construction,Model Selection,Model Dredging,model,"Dredging for models in unconstrained model space, where model space is not informed by theory or consists of models that are biologically implausible.",TRUE,NA,NA,"""Stepwise-type methods, which use overall F statistics to include and/or exclude variables, might appear suitable for achieving high explanatory power. However, optimizing explanatory power in this fash- ion conceptually contradicts the validation step, where variable inclusion/exclusion and the structure of the statistical model are carefully designed to represent the theoretical model. Hence, proper explanatory model selection is performed in a constrained manner. In the words of Jaccard (2001): Trimming potentially theoretically mean- ingful variables is not advisable unless one is quite certain that the coefficient for the variable is near zero, that the variable is in- consequential, and that trimming will not introduce misspecification error.""",NA,NA,NA,[@Shmueli2010]
Model Evaluation,Model Verification,Selective Debugging,model,Error checking only when unexpected or anomalous results are produced.,TRUE,NA,NA,"""When model code reaches a critical size (exceeded by all but the simplest models), error checking tends to occur only (or mostly) when results depart from expectations, and thus has more of the character of systematic bias than systematic checking.""",NA,"""If the model is scrutinized mostly in those situations when strange results occur, then strange results will tend to be corrected (when a model error is found) so that the model reconfirms expectations. On the flip side, when model results confirm expectations, error checking tends to be only cursory and the model will go uncorrected, even if wrong. Thus, there is a bias towards confirmation of expectations.""",NA,[@Risbey2005]
"Data, Model Evaluation, Model Construction","Data Partitioning, NA, Model Tuning",Poor Practice,input,"Resubstitution: assessing model performance on training set only or failing to evaluate model on independent data, or on partitioned holdout data.",FALSE,NA,Model performance will be overrated due to overfitting to training data.,"""Simply assessing model performance on a training set will lead to biased estimates of performance, because the model will have fit to some regular and irregular features of the sample at hand, but cannot distinguish between them.""",NA,"""will lead to biased estimates of performance, because the model will have fit to some regular and irregular features of the sample at hand, but cannot distinguish between them.""",NA,[@Pu2018]
"Data, Model Evaluation, Model Construction","Data Processing, Model Performance Metric, Model Specification",S-hacking,"model, output",Bayes Factor hacking: optimising the model to obtain a Bayes Factor above the required threshold.,FALSE,"Bayes Factor Hacking by data peeking, removing outliers, or changing model specification (dependent variable and/or predictor variables) in order to find model with greatest Bayes Factor.",Artificially inflates model performance and may result in spuriously selected model.,"""[@Simonsohn2015] showed through simulations (data peeking, convenient outlier removal, choosing favorable dependent variable or one variable combination out of many) how Bayes factors, not only p-values, can be hacked through selective reporting.""","""[@Simonsohn2015] showed through simulations (data peeking, convenient outlier removal, choosing favorable dependent variable or one variable combination out of many) how Bayes factors, not only p-values, can be hacked through selective reporting.""",NA,NA,[@Bodner2020]
Model Construction,Model Specification,Overfitting,model,Overfitting a model to calibration dataset by including too many moderators or predictors relative to the size and complexity of the dataset.,TRUE,NA,Spuriously selected model that is overfit to the data and may poorly generalise.,"""This means that, as a reader, you may want to check for the inclusion of too many [predictors] / [[moderators]] in a single model, or ‘[[over-fitting]]’ (the rule of thumb is that the authors may need at least ten effect sizes per estimated moderator) \[64\], and for ‘fishing expeditions’ (also known as ‘[[data dredging]]’ or ‘[[p hacking]]’; that is, non-hypothesis-based exploration for statistical significance \[28, 65, 66\])"" p.7",NA,NA,NA,[@Nakagawa2017]
"Model Construction, Model Evaluation","Model Selection, Model Performance Metric",S-hacking,"model, output",Dredging fitted models for statistical significance or other outcome variable.,TRUE,"Similar to Model Dredging, but model space is constrained by some performance metric or other outcome variable.",Spuriously selected model.,"""This means that, as a reader, you may want to check for the inclusion of too many [predictors] / [[moderators]] in a single model, or ‘[[over-fitting]]’ (the rule of thumb is that the authors may need at least ten effect sizes per estimated moderator) \[64\], and for ‘fishing expeditions’ (also known as ‘[[data dredging]]’ or ‘[[p hacking]]’; that is, non-hypothesis-based exploration for statistical significance \[28, 65, 66\])"" p.7",NA,NA,NA,[@Nakagawa2017]
Data,Data collection,Sample Curation,input,"Selective sampling / biased sampling, e.g. convenience or opportunistic sampling.",FALSE,"Sometimes opportunistic sampling is the best we can do when collecting ecological datasets. Only clearly problematic when not communicated or intentional, which in the latter case is a clear case of misconduct.",NA,"""Collecting a sample in a way that biases the findings. e.g. (1) Researcher tests the likeability of chocolate on a group of children only in order to find that everyone loves it. (2) Using uncomparable groups: The researcher tests if men are more aggressive than women. For comparison, women from a university are compared with men from a prison. (3) Picking a subsample of a panel dataset to find the desired results."" \...\ ""convenience sampling""",NA,"""Inflated or deflated effect size estimates, Inflated type I or type II error, Compromised generalizability, Reduced replicability if sampling bias is not disclosed""",NA,[@Nagy2025]
Model Evaluation,Model Performance Metric,Selective Reporting,output,Selectively reporting performance metrics that increase perception of performance after fitting model and observing model evaluation / model checking results.,TRUE,Have split this practice into two.,Overrates model performance and may result in spuriously selected model.,"""Changing the properties of a measure/measurement to produce favorable results without proper justification and/or transparent reporting. (1) Researcher uses only a portion of the items from a longer scale. (2) Researcher combines items from different scales into a single measure. (3) Researcher chooses which EEG electrodes to aggregate based on the results.""",NA,"""Reduced replicability, Reduced reproducibility, Reduced validity of the measure, Inflated or deflated reliability of the measure, Inflated type I or type II error, Inflated or deflated effect size estimates.""",NA,[@Nagy2025]
Model Evaluation,Model Performance Metric,Executing Alternative Analyses,output,Trying out different outcome variables or model evaluation metrics unrelated to model objectives and selecting based on performance after fitting the model and/or observing model checking / model evaluation results.,TRUE,NA,Overrates model performance and may result in spuriously selected model.,"""Changing the properties of a measure/measurement to produce favorable results without proper justification and/or transparent reporting. (1) Researcher uses only a portion of the items from a longer scale. (2) Researcher combines items from different scales into a single measure. (3) Researcher chooses which EEG electrodes to aggregate based on the results.""",NA,"""Reduced replicability, Reduced reproducibility, Reduced validity of the measure, Inflated or deflated reliability of the measure, Inflated type I or type II error, Inflated or deflated effect size estimates.""",NA,[@Nagy2025]
Data,Data Processing,Sample Curation,input,Missing data hacking: changing the strategy to handle missing data after fitting the model and observing model checking / model performance evaluation results.,TRUE,NA,Overrates model performance and may result in spuriously selected model.,"""Choosing the strategy to handle missing data based on the impact on the results. (1) A researcher tries three ways of handling missing data, for example, listwise deletion, multiple imputation, and inverse probability weighting. The expected results only appear with inverse probability weighting. The researcher reports only this strategy in the paper and leaves out results with listwise deletion and multiple imputation. (2) Can also be within a single method, specifically multiple imputation, since it uses one or more variables to replace missing data, and the choice of these variables is up to the researcher, but can also be statistically based.""",NA,"""Inflated or deflated effect size estimates, Inflated type I or type II error, Reduced reproducibility, Reduced replicability""",NA,[@Nagy2025]
Data,Data Processing,Sample Curation,input,"Modifying exclusion criteria or excluding data points, such as outliers or other values, without justification and prior planning, i.e. after fitting model and observing model evaluation / model checking results.",TRUE,NA,Overrates model performance and may result in spuriously selected model.,"""Exclusion of data points or outliers without proper justification and transparent reporting. e.g. (1) Removing individual reaction time trials based on post hoc criteria. (2) Trying different outlier cut-off criteria until an effect is statistically significant""",NA,"""Inflated or deflated effect size estimates, Inflated type I or type II error, Compromised generalizability, Reduced replicability, Reduced reproducibility""",NA,[@Nagy2025]
Data,Data Collection,Sample Curation,input,Collecting new data and refitting model after observing model evaluation / model checking results (optional stopping rules).,TRUE,Is this S-hacking or Sample Curation? outcome variable or performance measure is indirectly manipulated by directly manipulating input data.,Overrates model performance and may result in spuriously selected model.,"""Monitoring hypothesis tests during data collection, and stopping when statistical inference is favorable, without controlling for sequential testing"" e.g. ""Researcher is collecting responses and tests the hypothesis after every participant - when significance is reached, the researcher stops collecting data.""",NA,"""inflated type 1 error, reduced replicability"" from the following sources: - de Heide, R., & Grünwald, P. D. (2021). Why optional stopping can be a problem for Bayesians. Psychonomic Bulletin & Review, 28(3), 795–812. https://doi.org/10.3758/s13423-020-01803-x(https://doi.org/10.3758/s13423-020-01803-x) - Lakens, D. (2022). Sample size justification. Collabra. Psychology, 8(1). [https://doi.org/10.1525/collabra.33267](https://doi.org/10.1525/collabra.33267) - Schönbrodt, F. D., & Perugini, M. (2013). At what sample size do correlations stabilize? Journal of Research in Personality, 47(5), 609–612. [https://doi.org/10.1016/j.jrp.2013.05.009](https://doi.org/10.1016/j.jrp.2013.05.009) - Schönbrodt, F. D., Wagenmakers, E.-J., Zehetleitner, M., & Perugini, M. (2017). Sequential hypothesis testing with Bayes factors: Efficiently testing mean differences. Psychological Methods, 22, 322–339. doi:10.1037/met0000061 [OSF project with reproducible code, workshop slides, presentation slides] - Wicherts, J. M., Veldkamp, C. L. S., Augusteijn, H. E. M., Bakker, M., van Aert, R. C. M., & van Assen, M. A. L. M. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. Frontiers in Psychology, 7, 1832. [https://doi.org/10.3389/fpsyg.2016.01832](https://doi.org/10.3389/fpsyg.2016.01832)""",NA,[@Auspurg2021]
"Data, Model Evaluation","Data Processing, Model Performance Metric",Sample Curation,input,"Modifying exclusion criteria or excluding data points, such as outliers or other values, without justification and prior planning, i.e. after fitting model and observing model evaluation / model checking results.",TRUE,NA,Overrates model performance and may result in spuriously selected model.,"""Exclusion of participants without proper justification transparent reporting. (1) Researcher finds that a correlation between two variables is not significant. After removing two participants - who should be included - the association becomes significant. Then the researcher comes up with post hoc exclusion criteria for those participants. (2) A researcher doesn’t find an expected association between perceived stress and personality. When looking only at the top 25% of perceived stress scores, the association is there. They go on to report the top 25% scores as their population of interest and do not disclose that they looked at the rest of the sample population.""",NA,"""Inflated or deflated effect size estimates, Inflated type I or type II error, Compromised generalizability, Reduced replicability, Reduced reproducibility""",NA,[@Nagy2025]
Data,Data Processing,Sample Curation,input,Discretising continuous variables after observing model checking / model performance results.,TRUE,NA,Overrates model performance.,"""Taking a continuous variable and making it categorical without proper justification and transparent reporting. (1) Researcher doesn’t find an association between depression and continuous age variables, and recodes age into young and old categories. After that, age groups show a significant association with depression. An independent samples t-test is reported instead of a correlation.""",NA,"""Inflated or deflated effect size estimates, Inflated type I or type II error, Reduced replicability, Compromised generalizability.""",NA,[@Nagy2025]
Model Construction,"Model Tuning, Model Specification",Overfitting,model,Overfitting a model to calibration dataset by including too many moderators or predictors relative to the size and complexity of the dataset.,TRUE,NA,Especially at risk when datasets are small.,NA,NA,"""It is likely that these problems with statistical reproducibility also plague MLH papers as well, especially given that health datasets tend to be relatively small, have high dimensionality, are noisy, and often suffer from sparse/irregular sampling.""",NA,[@McDermott2021]
Model Application,Inference,Overhyping,output,Claiming the model has greater generalisability or credibility than it does.,TRUE,NA,NA,"""Just as statistical reproducibility is like internal validity, replicability is closely related to external validity (8), as it describes the notion of how well the desired results can be reproduced under conditions that match the conceptual description of the purported effect. Replicability is task-definition dependent; claiming a task has a greater conceptual horizon of generalizability makes it harder to satisfy this requirement.""",NA,NA,NA,[@McDermott2021]
Model Construction,Method Selection / Model Selection,Overhyping,model,"For studies developing new methods or approaches, optimising method / approach to improve the performance against baseline methods.",FALSE,NA,NA,"""Issues with statistical reproducibility may also arise due to an effect whereby researchers routinely spend more effort optimizing their method than is spent on the baseline methods against which their method is compared (**11**).""",NA,NA,NA,[@McDermott2021]
Model Construction,"Model Specification, Model Selection",Model Dredging,model,"Dredging for models in unconstrained model space, where model space is not informed by theory or consists of models that are biologically implausible.",TRUE,NA,'Best' or 'final' models are only spuriously selected.,"“Statistical tinkering, which really what the entire domain of model selection is about, can never be a substitute for intelligent prior selection of independent variables that may influence the dependent variable (e.g. Flack and Chang 1987. Ecologists and conservation biologists too often measure almost everything they can (especially in these days of automated probes and data loggers, remote sensing and GIS) and hope that something important will by ‘fished out’ of the resultant murky data. The variable-selection process will be substantially improved – and, therefore, the inferences too – if that process involves building upon existing knowledge and theory. In a sense, ‘forcing’ variables to be included in MR settings (e.g. Panzer and Schwartz 1998) is consistent with this philosophy because workers adjudge, ab initio, that these variables are crucial to the understanding of the system. Thus, all of the methods described in this paper are adjuncts rather than definitive expressions of the reasons for variation in the critical dependent variables and should be couched in these terms.”",NA,NA,NA,[@MacNally2000]
Model Construction,Model Specification,S-hacking,input,Changing random seed and refitting model after seeing results to improve model performance.,TRUE,NA,Model performance overrated.,"""Machine learning researchers note similar issues, for example tuning random seeds can drastically alter results"" (Cites: Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. 2018. Deep reinforcement learning that matters. In Proceedings of the AAAI Conference on Artificial Intelligence)",NA,NA,NA,[@Liu2020]
Model Construction,Model Specification,Overfitting,model,Overfitting a model to calibration dataset by including too many moderators or predictors relative to the size and complexity of the dataset.,TRUE,NA,Model will be overfit to data and generate spurious results and predictions that do not generalise to new / out-of-sample data.,"""calibrating a model to closely match one dataset, at the expense of predictive power for new data), as can happen when all available data are used to fit a model.""",NA,"""spurious results that result from overfitting models to data (i.e. calibrating a model to closely match one dataset, at the expense of predictive power for new data), as can happen when all available data are used to fit a model.""",NA,[@lewis2023]
Model Construction,"Model Specification, Model Tuning",Overfitting,model,Overfitting model to calibration data by adding additional parameters after observing fitted model.,TRUE,NA,"Model is overfitted to one dataset, at the expense of predictive power for new data. Model is unable to successfully predict out-of-sample data.","""Ecological forecasts that successfully predict out-of-sample  data often come from very simple models (Chevalier & Knape, 2020; Ward et al., 2014), in part because they cannot increase model fit simply by adding additional parameters and overfitting to data. \...\ overfitting models to data (i.e. calibrating a model closely to match one dataset, at the expense of predictive power for new data).""","""For example,  Averill et al. (2021) found that out-of-sample validations for some  forecasts of soil microbial taxonomic and functional groups were  systematically biased due to small differences in measurement techniques.""",NA,NA,[@lewis2023]
Model Construction,Model Specification,S-hacking,model,Changing the model specification or otherwise continuing to optimise the model after already validating or evaluating it on the test or holdout data.,TRUE,NA,Model will be overfit to held-out data.,"""model evaluation using in-sample data will share any systematic errors, and a researcher using cross-validation may overfit the held-out data by modifying models post hoc after cross-validation""",NA,NA,NA,[@lewis2023]
Model Evaluation,Model Performance Metric,S-hacking,"output, model",Bayes Factor hacking: optimising the model to obtain a Bayes Factor above the required threshold.,FALSE,NA,NA,"""However, a Bayesian approach is not without fallacies. That is, the results of Bayesian statistics can lead to instrumental optimization of Bayes factors (i.e., “BF-hacking”) similar to the phenomenon of p-hacking in traditional NHST, especially when dichotomous cut-off values for evidentiary power (and thus “publishability”) are set. In line with the famous quote of Rosnow and Rosenthal (1989, p. 1277), we argue that “God would love a Bayes Factor of 3.01 nearly as much as a BF of 2.99” (see section “BF-hacking”). Furthermore, not any BF-value can be taken as conclusive support without careful consideration of its interpretation, especially when BF values are small""",NA,NA,NA,[@Konijn2015]
Model Application,Inference,Overhyping,output,Misreporting correlative claims using causal language.,TRUE,NA,"For researchers, the problem (and solution) to misusing causal language to report correlational findings from observational studies is likely institutional. In addition, there may be unique extrinsic constraints on researchers working at the [[practitioner-research-policy nexus]], where by policy makers are funding the research, and expect clear messages about the research findings such that researchers do not report on study design limitations [[study limitation reporting]]. Another possible explanation is that applied journals pressure authors into providing clear directives to practitioners and policy-makers, However, given that this problem occurs in other disciplines, I would say that this is a science-wide problem not unique to discipline, but caused by some other facet of the science-publication pipeline.","Data derived from observational studies may be biased due to selection bias, and because the driver of observed effects can't be properly identified because of confounding - thus causal inference is not possible with observational studies. Because observations are merely correlative, and the true driver of an effect can be hidden when attempting to make causal inferences from observational studies. This results in false confidence, and possibly false negatives, in drivers of observed patterns. ""observational study designs are generally restricted in terms of their capacity for causal inference (Underwood, 1997). The main problem of implying causation from correlative observations is that it may divert attention from the real reasons for any observed effect, promoting false confidence in the drivers of the observed pattern."" p.2","""Most intervention evaluations used observational study-designs observational studies, and of those studies, the majority used causal language (inappropriately), with few acknowledging base-line bias at all. Many studies used hedged causal statements to soften causal wording. \...\ despite the correlative nature of the data in the Obs-CI studies, definitive (i.e., without hedging) causal word ing was common (66%). Hedged causal statements, using words such as “may,” “appears to,” and “indicates” to soften causal terminology, was used in another 16% of the studies (Figure 2a; Table S4 in the Supporting Information)."" p.5","""It has been suggested that competition among researchers and journals for high impact publications may foster a culture to neglect inherent and fundamental flaws related to study design, or to falsely make causal claims, in order to increase the seeming significance of research findings (Cofield, Corona, & Allison, 2010; Lipton & Ødegaard, 2005; Puhan et al., 2012; Robinson et al., 2007). \...\ A culture to let study design limitations go by unremarked may also be fostered at the interface between applied sciences and policy when policymakers provide funding and expect clear answers to research questions. \[...\] Similarly, editors of applied journals may suggest authors to provide clear directives to practitioners (Robinson et al., 2007; but see Cofield et al., 2010, who found no link between funding source and causal language).""",NA,[@Josefsson2020]
Data,Data Processing,Sample Curation,input,Collecting new data and refitting model after observing model evaluation / model checking results (optional stopping rules).,TRUE,NA,NA,"""The other three participants described strategies that involved re-accessing test data"" (without reporting).","""P1 described how they might try reducing the model size, in terms of the number of neurons, in addition to checking the data distribution. If the model continued to fail after such checks would they attempt to acquire new data. P2 described how they would try varying the hyper-parameter settings in cross-validation, and if this failed to improve test performance, systematically test the effects of feature combinations on performance, followed by rethinking their pre-processing steps.""","""if not reported, they would jeopardize the reproducibility of the reported test metrics.""",NA,[@hofman2023]
Model Construction,Model Specification,S-hacking,model,Changing the model specification or otherwise continuing to optimise the model after already validating or evaluating it on the test or holdout data.,TRUE,NA,NA,"""The other three participants described strategies that involved re-accessing test data"" (without reporting).","""P1 described how they might try reducing the model size, in terms of the number of neurons, in addition to checking the data distribution. If the model continued to fail after such checks would they attempt to acquire new data. P2 described how they would try varying the hyper-parameter settings in cross-validation, and if this failed to improve test performance, systematically test the effects of feature combinations on performance, followed by rethinking their pre-processing steps.""","""if not reported, they would jeopardize the reproducibility of the reported test metrics.""",NA,[@hofman2023]
"Model Evaluation, Model Construction, Model Application","Model Performance Metric, Model Tuning, Outcome Variable",S-hacking,model,Changing the model specification or otherwise continuing to optimise the model after already validating or evaluating it on the test or holdout data.,TRUE,NA,NA,"""One potentially common form \of data leakage\ is due to continuing to optimize a model after having evaluated it on the test set (e.g., by adjusting hyperparameters or changing evaluation metrics after accessing the test set), known as “over-hyping” (Hosseini et al., 2020). While multiple accesses to the test set clearly violate the underlying principle of out-of-sample testing, in practice it may be common for practitioners to accidentally make this mistake in the same way that “HARK-ing”, or hypothesizing after results are known, is common in explanatory modeling (Kerr, 1998). \[...\] Less obvious sources of leakage can occur due to pre-processing or feature selection applied to both training and test data, use of illegitimate features that leak information about the outcome variable, lack of independence between training and test data, and use of future data in time series modeling, among others (Kapoor and Narayanan, 2022).""","""even with the dataset and model specification held fixed, exercising just a few degrees of freedom (specifically, treating the problem as classification versus regression, filtering or thresholding data, and choosing different performance metrics) is shown to change the qualitative nature of the results. For instance, if one operationalizes this exercise as a classification problem by building a model to predict whether a post will receive at least 10 re-posts, one can obtain impressive-sounding accuracies close to 100%. If, however, one treats this as a regression exercise to predict how many re-posts a given post gets without any data filtering, the R2 hovers around a relatively modest 35%. The reason for the difference comes from the fact that only a small fraction of posts exceed the threshold of 10 re-posts, and predicting which posts do—and how far they spread—is quite challenging. Given the stark difference in these numbers and pressure to report positive, strong results, a researcher who tries both approaches, even if only during the training and validation phase, might, upon seeing the results, publish the classification model as a success and discard the regression model as a failure. \...\ The other three participants described strategies that involved re-accessing test data. Such steps would need to be explicitly reported as data exploration in communicating the results of research; if not reported, they would jeopardize the reproducibility of the reported test metrics. P1 described how they might try reducing the model size, in terms of the number of neurons, in addition to checking the data distribution. If the model continued to fail after such checks would they attempt to acquire new data. P2 described how they would try varying the hyperparameter settings in cross-validation, and if this failed to improve test performance, systematically test the effects of feature combinations on performance, followed by rethinking their pre-processing steps.""","""One of the simplest reasons for unreliable results is the (often unintentional) failure to adequately separate the training and validation sets from the test set, known as “leakage”. Sometimes this is as simple as mistakenly including the test examples in the training process (Wang, 2019; Oner et al., 2020). Other times there are more subtle forms of 'leakage'.""","""After the researcher has trained the model and proceeds to calculate test set performance, it is critical that they do not return to the training and validation phase or otherwise continue to manipulate the model.""",[@hofman2023]
Model Application,Model Results,Selective Reporting,output,Selectively reporting comparisons that support a foregone conclusion.,TRUE,The purpose of this practice is to 'overhype' the model results.,NA,"""cherry-picking comparisons to support a foregone conclusion, for example, are different from issues of leakage.""",NA,NA,NA,[@hofman2023]
"Model Evaluation, Model Application","Model Performance Metric, Outcome Variable",Executing Alternative Analyses,output,Trying out different outcome variables or model evaluation metrics unrelated to model objectives and selecting based on performance after fitting the model and/or observing model checking / model evaluation results.,TRUE,NA,NA,"""Including making decisions about outcome variables and metrics after seeing results""","""Four participants specifically mentioned the value of reducing the equivalent of “p-hacking” in machine learning, including making decisions about outcome variables and metrics after seeing results (P2, P4, P5, P6 ). \...\ pre-registering was perceived as at odds with typical practice. P6 described how it can be hard to identify the best metric to represent model performance a priori, suggesting that one could “develop the model with well-accepted metrics for the problem but then later find a better metric that justifies how the model performs,” but did not provide any specific examples of what they imagined.""",NA,NA,[@hofman2023]
Data,Data Processing,Sample Curation,input,"Using bad or easily obtainable, or inappropriate data to develop model.",FALSE,"Similar to ""Missing data hacking: changing the strategy to handle missing data after fitting the model and observing model checking / model performance evaluation results."" Is definitely a case of poor practice, and not so clear cut.",NA,"""The choice of data, the kind of translation it implies, the type of error it may contain and the way it has been curated all impact the accomplishments of ML. For instance, developers may use ‘low hanging fruit’, i.e. data that is easily available but not necessarily relevant or complete.""",NA,"""This may result in bad ML applications (garbage in, garbage out or GIGO), and can be remedied either by obtaining other and/or more data, or by accepting that the data needed for the task cannot be harvested.""",NA,[@Hildebrandt2018]
Data,Data Processing,Sample Curation,input,"Modifying exclusion criteria or excluding data points, such as outliers or other values, without justification and prior planning, i.e. after fitting model and observing model evaluation / model checking results.",TRUE,NA,Overfitting.,"""Before training their learning algorithm (‘the learner’) on the data, developers will attempt to remove irrelevant or incorrect ‘noise’, depending on the goal of the operation. """,NA,"""They always run the risk of removing highly relevant data, even though the risk can be reduced by testing on differently curated data sets.""",NA,[@Hildebrandt2018]
Model Evaluation,Model Performance metric,Executing Alternative Analyses,output,Trying out different outcome variables or model evaluation metrics unrelated to model objectives and selecting based on performance after fitting the model and/or observing model checking / model evaluation results.,TRUE,NA,NA,"""Next to bias in the data and the hypotheses space, the outcome of an ML application may be biased due to cherry picking with regard to the performance metric (P). This metric determines the accuracy of the system, based on the task (T) the system aims to perform and the data (E) it trains on. As one can imagine, if some metric P1 achieves 67% accuracy, whereas another metric P2 achieves 98% accuracy, the temptation to use only P2 and boast high accuracy is formidable. will call this P-hacking, as it seems to be the twin sister of p-hacking (Berman et al. 2018).""",NA,"""Especially in systems that are difficult to interpret high accuracy does not mean much, as the system may be getting things wrong despite the accuracy. The opacity of the underlying causality (e.g. in the case of medical diagnosis) or reasoning (e.g. in the case of quantified legal prediction) easily hides potential misfits.""",NA,[@Hildebrandt2018]
Model Evaluation,Model Performance Metric,Selective Reporting,output,Selectively reporting performance metrics that increase perception of performance after fitting model and/or observing model evaluation / model checking results.,TRUE,NA,NA,"""Next to bias in the data and the hypotheses space, the outcome of an ML application may be biased due to cherry picking with regard to the performance metric (P). This metric determines the accuracy of the system, based on the task (T) the system aims to perform and the data (E) it trains on. As one can imagine, if some metric P1 achieves 67% accuracy, whereas another metric P2 achieves 98% accuracy, the temptation to use only P2 and boast high accuracy is formidable. will call this P-hacking, as it seems to be the twin sister of p-hacking (Berman et al. 2018).""",NA,"""Especially in systems that are difficult to interpret high accuracy does not mean much, as the system may be getting things wrong despite the accuracy. The opacity of the underlying causality (e.g. in the case of medical diagnosis) or reasoning (e.g. in the case of quantified legal prediction) easily hides potential misfits.""",NA,[@Hildebrandt2018]
"Model Construction, Model Evaluation","Modelling Approach, Model Evaluation Approach",Executing Alternative Analyses,output,Trying out different outcome variables or model evaluation metrics unrelated to model objectives and selecting based on performance after fitting the model and/or observing model checking / model evaluation results.,TRUE,"See also: Prosperi [-@Prosperi2019] who also describe all the ways researchers may try different modelling methods, and they distinguish between different models and different behaviours to change those models.

Could potentially look like S-hacking, but also could be legitimate under methodological / analytic uncertainty.",NA,"'The second reason applies to decisions for which the question of which option is the “best” is more complicated, as reasonable arguments could be made for multiple choices. In these cases, it may not be possible to answer the question of which option is best. Instead, the important question is whether the different options are likely to lead to different study conclusions. If they are, then there is a risk that researchers may try out multiple methods or analyses, selectively report results from the ones that produced significant effects, and ignore those that “did not work,” contributing to an overrepresentation of Type I errors in the published literature. Analytic options that enable such a process are often referred to as researcher degrees of freedom (Simmons, Nelson, & Simonsohn, 2011).'",NA,NA,NA,[@Harder2020]
Model Construction,Modelling Approach,Poor Practice,"model, output, input","Overuse of inferior, familiar methods / failing to adopt new, superior or best-practice methods.",FALSE,NA,Inferior methods may yield inaccurate or wrong results.,"""A multiverse-of-methods analysis can be used to confront either of these sources of methodological variation. When one method is clearly superior to another, that superiority can usually be established through statistical reasoning or experimentation, without the use of a multiverse analysis. However, the task of persuading researchers that this difference is important enough to merit changing their methods can sometimes be difficult. As controversy over reproducible methods demonstrates, there is variation in the extent to which researchers adopt best-practice methods (John, Loewenstein, & Prelec, 2012).""","""If the inferior methods are more familiar, or if implementing the superior methods would require obtaining new knowledge or equipment, adopting the superior methods could lead researchers to incur some short-term costs in effort, time, or money. In these situations, motivated reasoning can lead researchers to dis- count arguments that they should make such methodological changes.""",NA,NA,[@Harder2020]
Model Construction,Model Specification,Poor Practice,"model, input",Failure to use biologically informed / justified predictor variables.,FALSE,See also [[QRP_modelling_database/Auspurg-2021-improper-model-specification]],NA,NA,NA,NA,"""We recommend that researchers carefully select variables according to the species’ ecology and evaluate models only according to their capacity to be transfered \sic\ in distant areas.""",[@Fourcade2018]
Model Evaluation,Model Performance Metric,Poor Practice,output,Improper use of model evaluation metrics (e.g. using an evaluation metric ill-suited to the stated model purpose).,FALSE,NA,"Improper use of evaluation measures, such as AUC, may overrate the performance of SDMs with biologically meaningless predictors.","""Post-hoc evaluation of distribution models is commonly performed to assess their predictive performance and statistical significance  (Peterson et al., 2011). The most common diagnostic metrics in the area  of SDMs is the area under the receiver operating curve (ROC) (AUC;  Porfirio et al., 2014), obtained by plotting the model sensitivity against  its false positive rate at all possible thresholds (Hanley & McNeil, 1982)."" \...\ ""it has been widely adopted by the SDM  community to measure the performance of models in discriminating  between presences and absences of species (Lobo, Jimenez-Valverde, &  Real, 2008). AUC has been adapted to presence-only (or presencebackground) modelling approaches by comparing the predicted suitability at presence points versus background points taken from the training  area. In this context, the implementation of AUC in a SDM framework  is usually carried out by partitioning species occurrences into two sets:  a training dataset, which is used to compute the model, and a test dataset that is used thereafter to evaluate the model’s discrimination ability  (Fielding & Bell, 1997). This process can be repeated several times, each  partition being used alternately to train and to test the model. This  approach assumes that training and testing data are spatially independent, an assumption rarely fully met in practice, especially when occurrences are randomly partitioned (Veloz, 2009)."" \[...\]. ""We have demonstrated that SDMs computed using meaningless variables as input environmental predictors are often classified as good or even excellent according to the most widely used evaluation measures.""","""The flaws of AUC that our results suggest have been known for a while (Lobo et al., 2008). Yet, it is still used in more than 80% of distribution modelling papers published in recent years in leading biogeography journals. More than half of SDM studies even relied on this single measure alone to assess the performance of their models."" \...\ ""As it appears that standard evaluation metrics can hardly discriminate biologically relevant SDMs from meaningless models"" \[...\] ""our results showed that, as for AUC, c. 30% of models based on pseudopredictors were better evaluated (lower AICc) than those based on real environment variables."" \[...\] ""We showed, in line with other authors (Jimenez-Valverde, 2012; Lobo et al., 2008), that two widely used evaluation metrics overrate the performance of biologically meaningless SDMs.""","""We showed, in line with other authors (Jimenez-Valverde, 2012; Lobo et al., 2008), that two widely used evaluation metrics overrate the performance of biologically meaningless SDMs."" \...\""We have demonstrated that SDMs computed using meaningless variables as input environmental predictors are often classified as good or even excellent according to the most widely used evaluation measures."" \[...\] ""Moreover, AUC has  been recognized as a highly questionable measure for several years  (Lobo et al., 2008), especially when used with background data instead  of true absences (Jimenez-Valverde, 2012). Many alternative metrics  have been proposed to evaluate SDMs (see for example Allouche,  Tsoar, & Kadmon, 2006; Hijmans, 2012; Phillips & Elith, 2010). However, despite these criticisms, so far none of these alternatives seems to  have taken over from AUC in most SDM studies.""","""inferences based on  SDMs could be improved by using an effective method of model evaluation.""",[@Fourcade2018]
Model Evaluation,Model Performance Metric,S-hacking,output,Changing model output or evaluation metric thresholds after observing outcome.,TRUE,NA,NA,NA,NA,"""Calculation of some evaluation indices requires a threshold. The threshold will vary by study because there is no single, default method for choosing a threshold.""",NA,[@feng2019]
Model Application,Model Output,S-hacking,output,Changing model output or evaluation metric thresholds after observing outcome.,TRUE,NA,NA,NA,NA,"""Often, the model predictions are in continuous format, which is subsequently transformed into a binary prediction under a particular threshold. Researchers have proposed different ways of thresholding for different purposes and under varied assumptions.""",NA,[@feng2019]
Model Application,Model Results,S-hacking,output,Changing model output format or transformation after observing results.,TRUE,NA,NA,NA,NA,"""The raw model predictions are sometimes transformed (for example, logistic transformation) via different methods under different assumptions.""",NA,[@feng2019]
Model Evaluation,Model Performance Metric,Poor Practice,"output, model",Using information-theoretic approaches to perform in-sample model assessment.,FALSE,NA,Information theoretic approaches do not provide a direct measure of predictive accuracy and therefor the selected model may be the 'best' of a poor selection. This links to the need for theory informing model-specification.,"""In ecological research, information-theoretic approaches are commonly adopted to perform in-sample assessment @Hooten:2015vw.""","""commonly adopted"" in ecological research @Hooten:2015vw.","""However, as they do not provide a direct measure of predictive accuracy, the best-performing model or models may only be the best of a poor selection (Taper et al. 2008).""",NA,[@Bodner2020]
"Model Evaluation, Data","Data Partitioning, Model Tuning",Poor Practice,"intput, output","Resubstitution: assessing model performance on training set only or failing to evaluate model on independent data, or on partitioned holdout data.",FALSE,NA,Overrates model performance or results in spurious model specification.,"""Relying only on in-sample assessments is suboptimal (Mosteller and Tukey 1977)""",NA,"""top-performing models from in-sample assessment may have worse transferability than lower ranked models (Wenger and Olden 2012) \...\ both approaches \[(cross-validation and information-theoretic approaches on in-sample model assessment )\] may select for more complex models when uncertainties are not accounted for (Dietze 2017a), reinforcing the need for out-of-sample assessment and uncertainty quantification when selecting final models. \[...\] While out-of-sample assessment may be optimized with fully independent data, often this type of data is not available in ecology (Urban et al. 2016, Dietze et al. 2018""",NA,[@Bodner2020]
Model Evaluation,Model Performance Metric,Poor Practice: pre-QRP,output,Failing to define model prediction properties.,FALSE,"This is the next step on from the problem formulation phase. It's the failure to precisely operationalise the model evaluation procedure based on the problem articulated during the problem formulation phase, perhaps because the problem was imprecisely formulated. It can provide the opportunity for QRPs.","Aids in choosing most appropriate model type. Increases transparency, and therefore improves accuracy and reliability.","Inverse of ""Establishing expected prediction properties refers to establishing a priori the characteristics and expected accuracy and/or precision levels of predictions (Refsgaard et al. 2007). For example, we may specify if predictions will be qualitative or quantitative, and if quantitative, whether predictions will be regarded as first approximations or precise values. We may also state whether they will be deterministic or probabilistic and what level of accuracy is acceptable given the model purpose. Additionally, if the predictions are probabilistic, we may state the amount of uncertainty expected given our purpose and knowledge about the model, data, and system.""",NA,NA,NA,[@Bodner2020]
Problem Formulation,NA,Poor Practice: pre-QRP,"model, output, input","Failing to define or inadequately defining model purpose, framing and or scope.",FALSE,NA,"Stating the model purpose, scope and framing clearly helps to clarify expectations about what a model can do, and how we should use it, therefore avoiding, controversy, mis-application, and misinterpretation. Unclear model purposes / inappropriate scope can introduce context uncertainty into the model.","""The first step to building an appropriate predictive model is to establish the context and the frame of the question. \...\ Explicitly defining the purpose refers to specifying the intentions of a predictive model, which could be, for example, to inform policy management (Taylor and Hastings 2004, Lowe et al. 2014) or to test a theory (e.g., Bailly et al. 2014, Borregaard et al. 2017 ) in either a general or specific system.""",NA,NA,NA,[@Bodner2020]]
"Model Evaluation, Problem Formulation","Model Performance Metric, Model Purpose",Poor Practice: pre-QRP,"output, model","Failure to explicitly state the model purpose, and / or failure to establish a priori performance metrics and measures after establishing the model purpose before beginning modelling.",FALSE,NA,NA,"Once the purpose has been specified, we should establish performance metrics for this purpose -- i.e. what properties or characteristics of prediction do we wish to achieve with the model? This should be set *a priori* (examples given): ""Establishing expected prediction properties refers to establishing a priori the characteristics and expected accuracy and/or precision levels of predictions (Refsgaard et al. 2007). For example, we may specify if predictions will be qualitative or quantitative, and if quantitative, whether predictions will be regarded as first approximations or precise values. We may also state whether they will be deterministic or probabilistic and what level of accuracy is acceptable given the model purpose. Additionally, if the predictions are probabilistic, we may state the amount of uncertainty expected given our purpose and knowledge about the model, data, and system.""",NA,"""a clear purpose helps avoid unnecessary controversies that may just stem from misunderstandings regarding what a model is and is not supposed to do (Grimm et al. 2010). \...\ Defining the explicit model purpose prevents misinterpretation of the model’s goals and limitations (Grimm et al. 2010)  and can lead to more informed model choices \[...\]  A priori establishment of expected prediction properties establishes a baseline for model evaluations, thus increasing transparency, and allows for mismatches between desired and observed properties to be addressed, thus improving accuracy and reliability.""",NA,[@Bodner2020]
"Model Evaluation, Data","Data Partitioning, Data Partitioning",Poor Practice,input,"Resubstitution: assessing model performance on training set only or failing to evaluate model on independent data, or on partitioned holdout data.",FALSE,NA,Model performance is overestimated / overrated or model is overfitted.,"""The same data are used for development and testing. Therefore, the performance evaluation will be the same as the calibration evaluation and model performance is likely to be overestimated because the model has been ‘tuned’ to the results. This approach is the least rigorous, and should be avoided.""",NA,NA,NA,[@Bennett2013]
Model Evaluation,Performance Measure Weighting,Poor Practice: pre-QRP,output,Failure to establish relative weighting of performance measures prior to beginning modelling.,FALSE,NA,NA,"""There may be even more subjectivity in considering the results of quantitative testing. Suppose one model is superior to another according to one metric, while it is the reverse according to another metric. What is the weight to be assigned to individual quantitative criteria in the overall assessment? How are these weights decided and by whom?""",NA,NA,NA,[@Bennett2013]
Model Evaluation,Model Performance Metric,S-hacking,output,"Changing the relative weighting of model performance metrics after observing model results, when there are multiple model evaluation analyses.",TRUE,NA,NA,"""In every study, the researchers may tweak the analyses to make sure that their model is better. There are many statistical validation methodologies which may be combined at will to favor one particular result. The plague of selective reporting based on p- values, and the misuse of p-values in general (Wasserstein et al., 2019) can still affect internal and external validation ... There may be even more subjectivity in considering the results of quantitative testing. Suppose one model is superior to another according to one metric, while it is the reverse according to another metric. What is the weight to be assigned to individual quantitative criteria in the overall assessment? How are these weights decided and by whom?""",NA,NA,NA,[@Bennett2013]
Model Evaluation,Model Performance Metric,Selective Reporting,output,Selectively reporting performance metrics that increase perception of performance after fitting model and/or observing model evaluation / model checking results.,TRUE,NA,NA,"""In every study, the researchers may tweak the analyses to make sure that their model is better. There are many statistical validation methodologies which may be combined at will to favor one particular result. The plague of selective reporting based on p- values, and the misuse of p-values in general (Wasserstein et al., 2019) can still affect internal and external validation ... There may be even more subjectivity in considering the results of quantitative testing. Suppose one model is superior to another according to one metric, while it is the reverse according to another metric. What is the weight to be assigned to individual quantitative criteria in the overall assessment? How are these weights decided and by whom?""",NA,NA,NA,[@Bennett2013]
Model Construction,Model Specification,Poor Practice,model,Over-simplifying models due to ideological stance rather than based on modelling objectives or performance measures linked to those objectives.,FALSE,NA,Model is not fit for purpose / may not be the most appropriate or adequate model.,NA,"""Ecologist E explained how his local model of vegetation dynamics was at first “quite complicated” until a colleague of his department “forced \him\ to make it as simple as possible”, leading him to discard terms in the time transfer function describing plant density, selecting linear functions and removing any differentiation between plant components. Ecologist E consequently parameterized or suppressed the terms of the transfer function the colleague had addressed, while they had been physically represented before.  Interestingly, while ecologist E seemingly easily accepted the input of his colleague, he rejected comments of the reviewers of the first journal he submitted the manuscript on his model to, which deemed it too simplistic. Conversely, the interviewee modified the (nowadays widely used) model towards even greater simplicity with one of his doctoral students in later stages of model development.  \[...\] Simplicity was repeatedly advocated in the course of the interview as a form of epistemic virtue to which the interviewee was increasingly tending over his career. While he had already built “very simple models” during his PhD, he had started to complexify them after it; the encounter with his colleague appeared to have resonated with his epistemic stance and brought him back to an approach of modelling which he nowadays strongly identified himself with. His colleague’s questions, we could interpret, had steered the representation towards their shared epistemic stance. The reviewers’ and hydrologists’ comments, distant from it, had been disregarded. \[...\] The consistent and repeated use of the term “simplicity” in ecologist E’s discourse could be linked to the simplicity/complexity dichotomy being often discussed within the discipline (e.g. Peck, 2004, and Merow et al., 2014), simplicity having historically been a well-established epistemic view on modeling in ecology (see the analysis of Evans et al., 2013).""",NA,NA,[@Babel2019]
Model Construction,NA,Poor Practice,model,Constructing new model / using new modelling approach rather than applying pre-existing one that might be superior.,FALSE,NA,Novel model / approach may be inferior to pre-existing model / approach.,NA,NA,"""Constructing a new model rather than using a pre-existing one could prove profitable in the long term. Interviewee B reported that while no direct application was determined, the model would contribute to establish an innovative orientation of the research group in hydrology: its existence would open up collaborations with other groups and funding opportunities, as well as enable to attract researchers whose expertise would be durably beneficial for the institute.""",NA,[@Babel2019]
Problem Formulation,"Model Purpose, Outcome Variable",Poor Practice: pre-QRP,"model, output",Failure to clearly define research question or give precise definition of parameter of interest.,FALSE,NA,NA,Inverse of: '*clearly defined research question and give a precise definition of the parameter of interest.*',NA,NA,NA,[@Bodner2020]
Model Evaluation,Robustness Checks,Selective Reporting,output,Selective reporting of robustness checks in support of main results.,TRUE,NA,NA,"""However, there is evidence that robustness analyses are selectively reported that support the main results at 100 per- cent (Young and Holsteen 2017). Therefore, we need more serious robustness checks. In this vein, it should become standard to present the full distribution of estimates that can be obtained based on all reasonable specifications. Multiverse analysis, as used in this article, seems particularly promising in this regard. In addition, one can complement summary tables (e.g., descriptive statistics and regression tables) with visualizations that disclose the full variance in raw data and results (Cumming 2014; Healy and Moody 2014). If the robustness analysis shows that results vary widely over the model space, then researchers need to make explicit why they chose their specific model specification. This will increase transparency and credibility of social research.""","""However, there is evidence that [robustness analyses] are selectively reported that support the main results at 100 percent (Young and Holsteen 2017).""",NA,"""it should become standard to present the full distribution of estimates that can be obtained based on all reasonable specifications. Multiverse analysis, as used in this article, seems particularly promising in this regard. In addition, one can complement summary tables (e.g., descriptive statistics and regression tables) with visualizations that disclose the full variance in raw data and results (Cumming 2014; Healy and Moody 2014). If the robustness analysis shows that results vary widely over the model space, then researchers need to make explicit why they chose their specific model specification. This will increase transparency and credibility of social research.""",[@Auspurg2021]
Model Construction,"Model Specification, Model Tuning",Poor Practice: pre-QRP,model,Failure to use theory in guiding model specification / using default controls in regression model that are uninformed by theory.,FALSE,NA,NA,Inverse of '*theory-guided causal reasoning to justify an appropriate model specification for identifying the parameter of interest. It is insufficient to simply throw the “usual suspects” as controls in a regression model.*',"""Unfortunately, this is common practice \'throwing the ""usual suspects"" as controls in regression model'\""","""*non-rigorous social research that does not start with a clear research question provides divergent results. However, rigorous social science research is able to provide a more consistent answer.*"" See also: @Bodner2020: ""model structure uncertainty may arise from unwittingly excluding variables of influence, utilizing surrogate variables, and/or approximating functional forms [@AscoughII:2008br].""",NA,[@Auspurg2021]

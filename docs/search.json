[
  {
    "objectID": "ms/QRPs in Ecological Modelling.html",
    "href": "ms/QRPs in Ecological Modelling.html",
    "title": "A Framework for Questionable Research Practices in Ecological Modelling",
    "section": "",
    "text": "Self-report surveys of researchers’ statistical practices suggest high rates of Questionable research practices (QRPs) in several different disciplines: psychology (John, Loewenstein, and Prelec 2012), education (Makel et al. 2023) and ecology (Fraser et al. 2018). QRPs are a set of methodological and statistical practices that can substantially influence research conclusions, and include practices like p-hacking, hypothesising after results are known (HARKing) and selective reporting of results (see Table 1). These practices fall into an “ethical grey zone” between ideal responsible research conduct and unacceptable research behaviours like fabrication and plagiarism (Butler, Delaney, and Spoelstra 2017, 94). Widespread QRPs, accompanied by a lack of transparency and openness in research reporting (Culina et al. 2020), can leave disciplines at risk of a ‘reproducibility crisis’ (Fidler et al. 2017).\nCurrent definitions and lists of QRPs are focused on hypothesis testing research (specifically, null hypothesis significance testing; NHST), characterising these practices as inflating the probability of false positive findings (see Table 3 in Nagy et al. 2025). This definition makes sense in this context, since this is the primary statistical estimand on which a finding is deemed ‘publishable’ in null-hypothesis significance testing. Indeed, Fraser et al.’s (2018) survey of ecology researchers asked how often they used QRPs documented in other disciplines, and many QRPs relate to p-values (Table 1). However, this NHST-centric focus creates particular challenges for ecology, because, although NHST is still popular in ecology (Fidler et al. 2017; Stephens, Buskirk, and Rio 2007), model-based methods in ecology are increasingly common, especially within applied research contexts (Connolly et al. 2017; García-Díaz et al. 2019; DeAngelis et al. 2021). The emphasis of existing QRP definitions on Type I errors is unhelpful for model-based research because multiple sources and types of error may arise in the modelling process; there is model structural uncertainty, uncertainty in parameter estimates and predictions, and uncertainty in scenarios (Rounsevell et al. 2021; Simmonds et al. 2024). What constitutes an ‘error’, the source of that error, as well as the relative weighting of different errors, depend on some combination of the model’s purpose (prediction, explanation, description), the type of model used (correlative or mechanistic) and the context for the model. Without a more encompassing definition of QRPs, ecological modellers may be inclined to think that concerns associated with QRPs and reproducibility are irrelevant, since many of the practices described as questionable within an NHST context do not directly relate to their work.\n\nCode\nQRP_tbl &lt;- readr::read_csv(\n  here::here(\"data\", \"tbl-QRP-frequency.csv\"),\n  show_col_types = FALSE\n) %&gt;%\n  mutate(\n    Category = here::here(\"data\", \"icons\", glue::glue(\"{Category}.png\"))\n  ) %&gt;%\n  gt::gt(groupname_col = \"Category\", row_group_as_column = TRUE) %&gt;%\n  gt::cols_align(align = \"center\", columns = \"Category\")\n\nimage_out &lt;- function(x, height = 50) {\n  out &lt;- map_chr(x, gt::local_image, height = height)\n  out &lt;- map(out, gt::html)\n  out\n}\n\nQRP_tbl_html &lt;- QRP_tbl %&gt;%\n  gt::text_transform(\n    locations = gt::cells_row_groups(everything()),\n    fn = image_out\n  ) %&gt;%\n  # gtExtras::gt_theme_538() %&gt;%\n  gtExtras::gt_plt_bar_pct(\n    label_cutoff = 0.1,\n    fill = \"green\",\n    column = c(\"Ecology\"),\n    scaled = TRUE,\n    width = 200,\n    labels = TRUE,\n    background = \"lightgrey\"\n  ) %&gt;%\n  gtExtras::gt_plt_bar_pct(\n    label_cutoff = 0.1,\n    fill = \"purple\",\n    column = c(\"Evolution\"),\n    scaled = TRUE,\n    width = 200,\n    labels = TRUE,\n    background = \"lightgrey\"\n  ) %&gt;%\n  tab_style(\n    style = cell_text(\n      color = \"black\",\n      font = google_font(\"Chivo\"),\n      size = \"medium\",\n      weight = 400\n    ),\n    locations = cells_column_labels()\n  ) %&gt;%\n  tab_style(\n    style = cell_text(\n      color = \"black\",\n      font = google_font(\"Cairo\"),\n      size = \"small\",\n      weight = 400\n    ),\n    locations = cells_body(columns = c(-Ecology, -Evolution))\n  )\n\nQRP_tbl_html\n\n\n\n\nTable 1: Examples and self-reported frequency of questionable research practices (QRPs) in hypothesis-testing research in ecology and evolutionary biology. QRPs are categorised as “cherry-picking,” “p-hacking,” and “methodologically flawed,” indicated by the cherry, saw, and cross icons respectively. Data compiled from Fraser et al. (2018) and abbreviations defined by Makel et al. (2023). Ecology: \\(n=494\\); Evolution: \\(n=313\\).\n\n\n\n\n\n\n\n\n\n\n\nQuestionable Research Practice\nAbbreviation\nEcology\nEvolution\n\n\n\n\n\nNot reporting response (outcome) variable that failed to reach statistical significance.\nOmitting non-significant studies or variables.\n\n\n64%\n\n\n\n\n64%\n\n\n\n\nNot reporting covariates that failed to reach statistical significance (e.g. p &lt;= 0.05) or some other desired statistical threshold.\nOmitting non-significant covariates.\n\n\n45%\n\n\n\n\n43%\n\n\n\n\nReporting an unexpected finding as having been predicted from the start.\nHARKing.\n\n\n49%\n\n\n\n\n54%\n\n\n\n\nReporting a set of results as the complete set of analyses when other analyses were also conducted.\nOmitting analyses.\n\n\n53%\n\n\n\n\n53%\n\n\n\n\n\nRounding off a p-value or other quantity to meet a pre-specified threshold.\nRounding p-values.\n\n\n27%\n\n\n\n\n18%\n\n\n\n\nDeciding to exclude data points after first checking the impact on statistical significance.\nData Exclusion (ARKing).\n\n\n24%\n\n\n\n\n24%\n\n\n\n\nCollecting more data after inspecting whether the results are statistically significant.\nData peeking.\n\n\n37%\n\n\n\n\n51%\n\n\n\n\nChanging to another type of statistical analysis after the analysis initially chosen failed to reach statistical significance (e.g. p&lt;= 0.05) or some other desired statistical threshold.\nAnalysis gaming.\n\n\n54%\n\n\n\n\n52%\n\n\n\n\n\nNot disclosing known problems in the method, analysis or data quality that potentially impact conclusions.\nHiding methodological problems.\n\n\n20%\n\n\n\n\n22%\n\n\n\n\nFilling in missing data points without identifying those data as simulated.\nFilling in missing data.\n\n\n5%\n\n\n\n\n2%\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding on these limitations, we argue that current QRP frameworks fail to address model-based research because the underlying research processes are fundamentally different. One principle underlying the classification of these practices as ‘questionable’ relates to data driven decision making. In contrast to hypothesis testers, modellers make a series of analytic decisions on the basis of both objectively identified model performance criteria and subjective judgements (Babel, Vinck, and Karssenberg 2019; Bennett et al. 2013), whereby they shift between subjecting the model to analysis, validation and testing, and refining the model in response to those results (Getz et al. 2017). This process is non-linear, iterative, and generates many interim versions of the model preceding publication (Augusiak, Van den Brink, and Grimm 2014).\nRather than dichotomous inferences relying almost exclusively on p-values, model performance metrics include both qualitative and quantitative measures that incrementally build a subjective picture of model credibility (Augusiak, Van den Brink, and Grimm 2014; Hamilton et al. 2019). Since the publishability of a model-based study hinges on the collection of these model outputs and their contribution to overall model credibility, each of these model outputs may be susceptible to QRPs that aim to strategically alter the perceived credibility of the model.\nAs such, we argue that a conceptual framework of QRPs in model-based research must account for certain kinds of data-dependent decisions, which are appropriate and justifiable aspects of the modelling process, while highlighting the primacy of guarding against data-dependent decision-making that might be questionable. Moreover, the conceptual framework should de-emphasise the risk of type I errors and false positive findings to account for other biases more relevant to how complex models are evaluated and used.\nThis begs the question of whether QRPs are relevant to model-based research? If specific QRPs related to p-values do not apply, such as p-hacking, are there parallel or counterpart practices that do? What form might they take? And where in the research process would we be likely to locate them? This paper addresses each of these questions. My primary aim is to highlight the range of specific practices that are problematic in different stages of the modelling process, so as to identify the QRPs and associated decision-points relevant to model-based research. We aim to raise awareness among ecologists (and modellers within other disciplines) about the potential for QRPs throughout the modelling process. We hope to facilitate future attempts to estimate the severity and extent of QRPs and provide solutions to help mitigate questionable practices in model-based research.\n\n\nBelow we outline the conditions under which QRPs may arise in ecological modelling and give an overview of the modelling process and the ‘objects’ it produces (inputs, outputs, the model itself). This sets the scene for exploring how QRPs could unfold in model-based research.\n\n\nUnderstanding what makes ecological models “publishable” is crucial for identifying where QRPs might emerge, since publication bias provides a primary motivation for engaging in questionable practices (Ware and Munafò 2015). Unlike hypothesis testing research where p-values serve as the main target for manipulation, model-based research involves multiple attributes that collectively determine publishability. These attributes become potential targets for the QRP classes we later identify in the typology (Table 2).\n\n\nNovelty is an important factor influencing the publishability of modelling research. Publication bias and funding stipulations reward and require advantage over existing approaches; either through development of new methodological approaches, enhanced performance of existing algorithms and modelling methods, or application of existing models to new contexts, such as new environmental conditions or ecological contexts (Alexandrov et al. 2011). Conversely, publication bias disincentivises the evaluation and testing of existing published models (Babel, Vinck, and Karssenberg 2019; Houlahan et al. 2016). This requirement for novelty incentivises model fishing and selective reporting, where researchers may opportunistically explore new modelling approaches until they achieve apparent superiority over baseline approaches.\n\n\n\nModel credibility is based on the subjective degree of confidence that both the model and model-derived inferences about the real system can be used as claimed (Risbey et al. 2005; Augusiak, Van den Brink, and Grimm 2014; Hamilton et al. 2019). That is, can the model adequately answer the research question (Christin, Hervet, and Lecomte 2020), and can it be used reliably to inform management decisions (Alexandrov et al. 2011)? Credibility emerges gradually throughout the modelling process, by demonstrated adequacy (conceptual validity and predictive accuracy, Rykiel Jr 1996) and reliability (consistent performance and transferability, Schmolke et al. 2010; Yates et al. 2018).\nUnlike the binary nature of statistical significance, model credibility builds incrementally through multiple performance metrics and evaluation approaches (Figure 1, model outputs). This multi-faceted assessment creates numerous opportunities for statistic hacking, or “S-hacking” (Table 2 and Table 3), where researchers can manipulate i) performance metric selection and thresholds, ii) validation approaches and data partitioning strategies, and iii) evaluation timeframes and spatial scales. The subjective nature of credibility assessment also fosters overhyping (Table 3), where model capabilities are overstated beyond what evaluation results justify.\n\n\n\nA suite of modelling objects is generated throughout the modelling process (described below), collectively building a subjective picture of the publishability and credibility of the model. These outputs may be manipulated to improve the chance of model acceptance or publication. Models that serve different purposes are vulnerable to different QRPs and depending on the combination of modelling approach, model type (e.g. agnostic, correlative, or mechanistic) and purpose of the model, QRPs will target different model objects (Hoffmann et al. 2021). For instance, when the modeller’s estimand of interest (“the target quantity to be estimated in an analysis,” Borger and Ramesh 2025, 2) are parameter estimates, like in many cases of explanatory modelling in ecology, then QRPs are likely to affect parameter estimates, parameter uncertainty, goodness-of-fit metrics, or variable importance measures. While for analyses concerned with model predictions, QRPs are more likely to affect model components, like forecast accuracy metrics and measures of model transferability. Different questionable practices are concentrated at different locations across modelling phases (Figure 2).\n\n\n\n\nTo help conceptualise where in the modelling process QRPs might emerge, and which ‘model objects’ QRPs may affect, we first give an overview of the modelling process, articulating various inputs and outputs, including the model itself, model fit statistics, summary measures and other evaluation results (Figure 1), to which we ascribe mathematical notation.1 This framework will also provide the foundations for extending Gelman and Loken’s (2013) mathematical formalism to modelling (which we do in Section 1.3).\nWe acknowledge the plurality and lack of consensus in how the modelling process is described (Lahtinen, Guillaume, and Hämäläinen 2017), including the terminology used for different modelling phases, steps and tasks (Schmolke et al. 2010; Augusiak, Van den Brink, and Grimm 2014). Rather than adopting a comprehensive taxonomy that captures all distinct processes and categories of modelling, we instead describe the modelling process at a high-level that can be generalised across different model purposes, contexts, types and methods. There will, of course, be exceptions. Some aspects may not apply in every modelling problem, and the specific collection of model objects, their relative weighting in informing study conclusions, and the relative weighting of publishable attributes, will differ depending on the model purpose, context and methodology applied to the problem at hand. We also recognise that analysis decisions are procedurally dependent (Liu, Althoff, and Heer 2020), for instance, the way models are specified and parameterised depends on the model type (i.e. whether using a correlative, mechanistic, or agnostic model, sensu Hoffmann et al. 2021) and modelling purpose (i.e. exploration, inference, prediction, see Tredennick et al. 2021).\nWe have divided the modelling process into three phases; 1) model construction, 2) model evaluation, and 3) model application. These distinctions align with the phases underpinning the preregistration template in Gould et al. (2025).\n\n\n\n\n\n\n\n\nFigure 1: Three phases of model development: model construction, where the conceptual model \\(M_c\\) is specified into the formal model (\\({M_c\\rightarrow M}_s\\)) then parameterised (\\(M_s{\\rightarrow M}_p\\)); model evaluation, where the calibration and validation fits are evaluated, possibly leading to re-specification and re-parameterisation (dashed arrow); and model application, where the model is analysed to answer the research question.  Modelling generates objects, including the conceptual, specified and parameterised model, (\\(\\symbf{M}=\\left\\{M_c,M_s,M_p\\right\\}\\), purple circles); model inputs (blue squares), including hyper parameters \\(\\lambda\\) and calibration settings, data \\(\\symbf{X}\\) for model parameterisation, evaluation and application. Model outputs (\\(M_j\\) green hexagons) include model predictions \\(\\hat{y}\\), which are used to characterise model performance during model evaluation or subject to further aggregation, transformation, analysis and visualisation during model application.  Note: Data analyses may also inform model specification during construction. New or alternative input data may be used during scenario analysis to make predictions or projections about how the system will respond to intervention \\(\\hat{y}\\).\n\n\n\n\n\n\n\nTo begin the model development process, a conceptual model, \\(M_c\\) or candidate set of models, \\(\\symbf{M_c}\\) is specified by the modeller, synthesising their understanding of the ecological system. Conceptual models may be represented by a set of qualitative statements, mathematical formulas, or else visually as plots or directed acyclic graphs (Shmueli 2010). A candidate set of multiple models at this stage may represent competing hypotheses, where differences in the structure and/or parameterisation of the models represents critical uncertainty about the ecological system.\n\n\n\nNext, the modeller formalises each conceptual model mathematically or statistically, \\(M_s\\) (Figure 1). The modeller chooses which variables should be included in the model, how to operationalise or represent them in the chosen framework, what the appropriate dependencies are between variables and the model type, and the functional form of the model (if relevant). Because the variables in the conceptual model are not directly observable, they are operationalised into measurable outcomes \\(Y\\) and input variables \\(X\\) in a data matrix \\(\\symbf{X} = {X_1, X_2,\\ldots,X_p}\\), where \\(p\\) is the number of input variables, and f represents a function relating \\(Y\\) to \\(X\\) such that \\(E\\left(Y\\right) = f\\left(\\symbf{X}\\right)\\) (Hoffmann et al. 2021; Shmueli 2010). Note that, for some predictive modelling contexts, such as data-driven modelling employing black-box algorithmic approaches, like machine-learning, \\(f\\) may not be specified and is instead represented by \\(\\mathscr{I}_{\\lambda}\\) where \\(\\mathscr{I}\\) represents some learning algorithm and \\(\\lambda\\) denotes its hyperparameters (following Bischl et al. 2023). Exploratory analyses are often conducted at this stage to inform variable selection, for example by analysing variable importance and examining collinearity among variables (Kass et al. 2025).\n\n\n\nNext, each specified model \\(M_s\\) is parameterised yielding \\(M_p\\) (Figure 1). Model parameters refer to any component of a model that can be quantified or estimated, such as slopes or intercepts in a linear regression or growth rate in a population model (García-Díaz et al. 2019, 2). Regardless of the overarching model purpose (e.g. explanation or prediction, Shmueli 2010), for correlative or agnostic (e.g. machine-learning) models, parameterisation typically occurs by estimation, or calibration, whereby the modeller applies techniques, like maximum-likelihood estimation or Bayesian inference, to the data matrix \\(\\symbf{X}\\) (Figure 1) to estimate the parameters specified by \\(f\\), with uncertainty (García-Díaz et al. 2019; Hoffmann et al. 2021), yielding \\(\\hat{y}=\\hat{f}\\left(\\symbf{X}\\right)\\). In the case of agnostic models, the algorithm \\(\\mathscr{I }_{\\lambda}\\) returns the fitted model and its parameters when applied to \\(\\symbf{X}\\), \\(\\hat{y}=\\hat{f}_{\\hat{\\lambda}}\\left(\\symbf{X}\\right)\\). Parameters of mechanistic models are typically provided as inputs to the specified model \\(f\\), gleaned from expert knowledge, published literature or via calibration (Hoffmann et al. 2021).\nWhen conducting inference or explanatory modelling, the estimand(s) of interest are the parameters \\(\\symbf{\\hat{\\theta}}\\), like standardised mean differences, correlation coefficients or response ratios (Williams et al. 2025), whereas for predictive modelling, predicted values \\(\\hat{y}\\) constitute the estimand(s) of interest (Tredennick et al. 2021; Hoffmann et al. 2021; Shmueli 2010). This is true regardless of whether the model is correlative Silk, Harrison, and Hodgson (2020), mechanistic (e.g. a population viability model), or agnostic (e.g. a machine learning or deep learning models, Pichler and Hartig 2023). However, different types of models are more likely to be used for inference or prediction in practice, for example, agnostic models are more likely to be used for prediction, but inferences about parameters are certainly possible (Lucas 2020). Note that agnostic modelling approaches require the modeller to supply hyperparameters \\(\\symbf{\\lambda}\\) (Figure 1), which may be decided by the modeller, or else estimated by some tuning or optimisation method \\(\\symbf{\\hat{\\lambda}}\\). Hyperparameters may influence the model learning process, such that with each set of hyperparameters the model will provide a different set of results (Ahmed et al. 2025).\n\n\n\nArriving at the optimal final model or collection of final models is typically iterative, determined by the outcomes of model validation and evaluation (Shmueli 2010) whereby the model is subjected to a series of analyses that generate performance measures that are used to establish its validity, reliability and credibility and ensuring that the model is fit for intended use (Bennett et al. 2013; Eker et al. 2018; García-Díaz et al. 2019; Rykiel Jr 1996).\nI distinguish between model validation and model evaluation. Model validation checks that the fitted model \\(M_p\\) suitably approximates the data \\(\\symbf{X}\\), and is evaluated using goodness-of-fit tests, and model diagnoses like residual analyses (Shmueli 2010). Model selection whittles down the candidate set of models into a single ‘best’ model or a smaller subset of ‘best models’ (e.g. AIC within \\(\\Delta2\\)), after which the modeller might choose to consider multiple models or conduct model averaging. Outside of formal model selection approaches, the procedure for determining the best model(s) may involve a degree of trial and error of different model structures that is not always preplanned (i.e. new \\(M_s\\) are specified after validation).\nFollowing validation, model evaluation of the best model(s) is undertaken, assessing the fitness of \\(M_p\\) for purpose by calculating additional performance measures to fully understand the model’s capabilities, like constructing confusion matrices or calculating omission and commission rates. Ultimately, model evaluation is case-dependent and context-specific insofar as the overall evaluation process, types of analyses, metrics, estimand of interest, and desirable properties of the model differing depending on the overarching modelling purpose and type of model and modelling approach (Tredennick et al. 2021; Bokulich 2013).\n\n\n\nOnce \\(M_p\\) is considered plausible and fit for purpose, the modeller shifts to model application (Engelschalt et al. 2023), querying the model and using the model to undertake analyses that inform the stated research questions (Figure 1). Prior to analysis, model output may be subject to further processing, for example, continuous predictions may be aggregated or transformed into binary predictions for visualisation and communication purposes (Feng et al. 2019). Explanatory model output may be visualised with coefficient plots, or effect size plots to inform the relevance of observed effects (Lüdecke et al. 2020). In applied settings, forecasts or anticipatory predictions into the future or across space are generated from the model based on plausible scenarios or to simulate outcomes under different management actions or policies (Paniw et al. 2023), which may be subject to a range of visualisations (e.g. Barros et al. 2023, fig. 2).\nTo summarise, a collection of model outputs are generated in modelling, which may variously be the target of QRPs, including: point-estimates; such as means, medians and effect-sizes; uncertainty measures, like confidence intervals, prediction intervals, standard errors; model performance metrics, like \\(R^2\\) / AIC / BIC; inference results, like p-values, credible intervals, and significance determinations; predictions and forecasts, like future values or classification outcomes; or, the model itself.\n\n\n\n\n\nHere, we present a taxonomy and map of QRPs derived from the modelling literature, which aims to illustrate the different types of QRPs that might occur throughout different points in the modelling process. We follow with a synthetic example that reveals how these different types of QRPs might look in practice (Box 1).\n\n\nWe first surveyed the modelling literature to identify potential QRPs in ecological modelling and their location in the modelling process. QRPs were categorised into broader classes corresponding to families of similar practices using well-known published classifications (e.g. Table 1), adopting new classes when there was no analogue in the existing QRP literature. We coded the phase and sub-phase of modelling in which the practice occurs, as well as the target of the practice (input, model, output). After initial coding of the QRPs we generalised the descriptions of individual practices and categorised them according to a QRP class schema. The literature review and coding are described in further detail in Appendix 1.6.\n\n\n\nWe identified six classes of QRPs: sample curation, model fishing, selective reporting, S-hacking, overhyping, HARKing (Table 1). All classes of QRPs have analogous practices under NHST, but the practices themselves are not directly comparable. The list of QRPs we identified is not exhaustive and instead illustrates a range of practices that can occur in model-based research (See Appendix 1.6, Table 4 for the complete list).\nParadigmatic examples of QRPs are presented for each class in a roadmap (Figure 2), illustrating that QRPs occur throughout all phases of the modelling process, and some may occur at multiple stages. Sample curation, S-hacking and fishing were the classes of QRPs most likely to occur during model construction. The iterative nature of model validation and evaluation creates multiple opportunities for opportunistic optimisation of apparent model performance during model evaluation, with selective reporting, S-hacking and model fishing primarily affecting this phase of modelling. Fewer, but distinct, QRP types were identified for the model application phase, primarily concerning the misrepresentation of model capabilities and findings.\nThe target objects affected by QRPs across modelling phases reflected the focus of modelling activities and tasks, with QRPs affecting the model itself occurring primarily during model construction and evaluation, and QRPs affecting the model outputs being concentrated in model evaluation and model application phases. Below, we briefly describe the different classes of QRPs, providing illustrative examples and explaining how they can bias results.\n\n\nCode\nwaffle_plot_data &lt;-\n  tidy_QRP_database(here::here(\"data/QRP_database_2025-10-20.csv\")) %&gt;%\n  filter(include) %&gt;%\n  select(\n    -practice_notes,\n    -model_subphase,\n    -source,\n    -qrp_reason,\n    -include,\n    -starts_with(\"practice_\"),\n    -file_name,\n    practice_target\n  ) %&gt;%\n  distinct() %&gt;% #rm duplicate qrp_coded #TODO next merge duplicates while keeping source\n  drop_na() %&gt;% #interim approach until datachecks in place\n  mutate(\n    model_phase = str_split(model_phase, \", \"),\n    practice_target = str_split(practice_target, \", \"),\n    values = 1\n  ) %&gt;%\n  unnest(model_phase) %&gt;%\n  unnest(practice_target) %&gt;%\n  complete(\n    qrp_description,\n    model_phase,\n    practice_target,\n    fill = list(values = 0.0001) # see https://github.com/hrbrmstr/waffle/issues/66\n  ) %&gt;%\n  group_by(qrp_description) %&gt;%\n  fill(qrp_coded, model_phase, .direction = \"downup\") %&gt;%\n  ungroup()\n\n\n\nCode\nwaffle_plot &lt;- waffle_plot_data %&gt;%\n  distinct() %&gt;% #TODO identify duplicated rows - suspect non-unique practice_target coding is cause\n  mutate(\n    model_phase = forcats::as_factor(model_phase) %&gt;%\n      forcats::fct_relevel(c(\n        \"Model Construction\",\n        \"Model Evaluation\",\n        \"Model Application\"\n      )),\n    qrp_coded = forcats::as_factor(qrp_coded) |&gt;\n      forcats::fct_relevel(c(\n        \"Sample Curation\",\n        \"Model Fishing\",\n        \"Selective Reporting\",\n        \"S-hacking\",\n        \"Overhyping\",\n        \"Poor Practice\"\n      ))\n  ) %&gt;%\n  arrange(model_phase, qrp_coded) %&gt;%\n  ggplot(aes(fill = practice_target, values = values)) +\n  geom_waffle(\n    size = 2,\n    color = \"white\",\n    make_proportional = FALSE,\n    flip = TRUE\n  ) +\n  scale_x_discrete(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  scale_fill_manual(\n    name = \"Target Model Object\",\n    values = c(\n      \"input\" = \"#0F9ED5\", # blue\n      \"model\" = \"#A02B93\", # purple\n      \"output\" = \"#4EA72E\"\n    ), # green\n    labels = c(\n      \"input\" = \"Model Input\",\n      \"model\" = \"Model\",\n      \"output\" = \"Model Output\"\n    )\n  ) +\n  # ggsci::scale_fill_futurama(name = \"QRP Target\") +\n  coord_equal() +\n  facet_nested(\n    qrp_coded + qrp_description ~ model_phase,\n    switch = \"y\",\n    labeller = labeller(\n      qrp_description = label_wrap_gen(width = 80, multi_line = TRUE),\n      qrp_coded = label_wrap_gen(width = 10, multi_line = TRUE),\n      model_phase = label_wrap_gen(width = 10, multi_line = TRUE)\n    ),\n    nest_line = element_line(linetype = 1),\n    solo_line = TRUE,\n    space = \"free\",\n    strip = strip_nested(size = \"variable\", clip = \"off\")\n  ) +\n  # facet_grid(qrp_description ~ model_phase,\n  #            switch = \"y\",\n  #            labeller = labeller(qrp_description =\n  #                                        label_wrap_gen(width = 100))) +\n  # theme_no_axes() +\n  hrbrthemes::theme_ipsum_rc() +\n  waffle::theme_enhance_waffle() +\n  theme(\n    strip.text.y.left = element_text(angle = 0, size = 16),\n    strip.text.x = element_text(size = 18, vjust = 0),\n    strip.background.x = element_part_rect(side = \"b\"),\n    strip.background.y = element_blank(),\n    legend.text = element_text(size = 12),\n    legend.title = element_text(size = 14)\n    # legend.position = \"bottom\"\n  ) +\n  ggh4x::force_panelsizes(rows = unit(1.1, \"cm\"), cols = unit(3, \"cm\")) +\n  guides(fill = guide_legend(\"QRP Target:\", position = \"bottom\"))\nwaffle_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Synthesis of questionable research practices (QRPs) in ecological modelling. QRPs may target model inputs (blue squares), the model itself (purple squares), and/or model outputs (green squares), and may occur at different phases in the modelling cycle. QRPs are grouped according to broader classes defined in Table 3. See Table 4 for the full list of QRPs identified.\n\n\n\n\n\nSelective reporting involves failure to disclose methods and/or results. Selective reporting can be distinguished from other practices, such as S-hacking and model fishing, in that it lends unwarranted credibility to the model, but the model and model outputs remain unaffected. Instead of analytic decisions being data-dependent, the communication of those results is data-dependent. The ‘garden of forking paths’ is not altered by selective reporting but rather is not fully transparent.\n\n\n\nWe expanded the concept of p-hacking and termed it ‘S-hacking’, or ‘statistic hacking’, which encompasses analogous practices in modelling that target metrics that contribute to the publishability of a model. S-hacking involves an element of selective reporting, but a critical point of difference is that S-hacking includes the execution of alternative analyses and manipulation of data, models, or outputs to obtain a favourable result. For example, a modeller may systematically trial multiple different evaluation metrics, selectively reporting only those that present the model in a favourable light (Hildebrandt 2018). In this instance the model remains unaffected by S-hacking. Alternatively, random seeds in model tuning can be changed after observing test set performance which can drastically alter model results (Liu, Althoff, and Heer 2020). If S-hacking is performed during model construction or validation, or alternative model specifications are trialled after observing model performance results, the model itself is altered, and overfitted to the training data. If S-hacking is performed during model construction or validation, the model is overfitted to the training data and poorly generalises to new data. S-hacking artificially inflates model performance, resulting in spuriously selected models that that may not reflect genuine ecological or predictive relationships. Any performance metric with a threshold dependent outcome (e.g. AUC, TSS, partial ROC, sensitivity, specificity, Feng et al. 2019) will be subject to the same types of practices as p-hacking.\n\n\n\nWe distinguish ‘model fishing’ from the methodological technique of ‘model dredging’ for the purposes of exploration or model selection. In the case of formal model selection procedures employing dredging, there is some a priori chosen objective criteria on which the model is selected, and the model space (usually, though not always) is constrained by a priori specification of candidate models that are theoretically or ecologically motivated. In contrast, when conducting model dredging for the purposes of exploration in pursuit of generating new hypotheses, the initial model space may not be as constrained, but the dredging procedure is transparently reported, and the exploratory nature of the modelling exercise is disclosed and not mispresented post-hoc as otherwise. Model fishing occurs when the dredging procedure is not disclosed, and/or there is no formal criterion for model selection, and the overarching purpose is not exploration. Alternatively, model fishing can occur without dredging through a large model space, but by conducting alternative analyses or new model variations and selectively reporting only those with favourable results. Model fishing is problematic because of the risk of cognitive biases, such as hindsight bias, where post-hoc rationalisation combined with haphazard model selection leads to spuriously selected models. Model fishing therefore involves an element of systematic exploration of researcher degrees of freedom that is not necessarily planned, nor transparent.\n\n\n\nSample curation (sensu Nagy et al. 2025) includes a range of data-dependent decisions about model inputs without justification or prior planning, i.e. after model fitting or observing model evaluation or application results. Sample curation may include removing observations in order to make a correlation of interest become significant and generating a data-dependent criterion for the exclusion of particular observations (Nagy et al. 2025). Opportunistic handling of missing data could occur in a number of ways, for instance when a researcher attempts list-wise deletion, multiple imputation or inverse probability weighting. The expected results may only appear with one of those options, which is problematic if the researcher only reports this strategy in the paper, and omits the results from the other data handling methods (Nagy et al. 2025). Similarly, opportunistic stopping occurs when new data is collected and is used to re-parameterise the model after previously observing model validation and model evaluation results, without reporting results of earlier iterations (Table 3, Table 4).\n\n\n\nAlthough the overarching purpose of ecological modelling in applied contexts is not hypothesis-testing, it is important to acknowledge that ecological models implicitly encapsulate hypotheses in the form of assumptions about which patterns, relationships, or predictors are most relevant to the system being modelled (Bodner, Fortin, and Molnár 2020; Prosperi et al. 2019; Schuwirth et al. 2019). For example, the choice of which variables to include or exclude from a model are based on implicit hypotheses about which processes are relevant to the system. In the case of modelling, HARKing can occur when a researcher presents a post-hoc explanation and justification for the variables or model structure that performed best, while failing to disclose the initial exploration of other variables or model structures. As such, HARKing in ecological modelling for purposes other than hypothesis testing is likely to occur as an effect of other related QRPs (Table 3) rather than as the motivating practice.\n\n\n\nOverhyping involves claims about the models’ performance that are not substantiated by model evaluation results, such as claiming the model has greater generalisability than it does (Corneille et al. 2023). A specific form of overhyping involves misreporting correlative claims using causal language, which is particularly common in studies evaluating conservation interventions using observational study designs (Josefsson et al. 2020). The practice of implying causation from correlation can cause false confidence in the intervention’s effectiveness while ignoring the real mechanisms for the observed effect.\n\nBox 1: Synthetic Example Of Questionable Research Practices In Applied Ecological Modelling\nA modeller seeks to predict species abundance on the basis of habitat quality to help inform conservation management (Figure 3). When the modeller evaluates how two different management actions affecting habitat quality influence species abundance, the initial a priori model does not provide sufficient certainty for choosing between one action and another (Panel A, Stage 1). The modeller revises the model without theoretical justification, instead opportunistically trialling different models and selecting the one with the best Performance Score (Panel B). On checking the predicted species abundance for the two actions on the overfitted model, the modeller finds that the actions are still not clearly distinguishable in terms of their predicted outcomes (Panel A, Stage 2), so the modeller adjusts the scenario input values for the two management actions, and plots the predicted outcomes (Panel B, Stage 3). They are sufficiently happy that the model now clearly supports their preferred management action B and proceed to publish the overfitted model, its predictions and management recommendations without ever disclosing their model fishing and scenario hacking, effectively a form of HARKing (‘hypothesising after results are known’). The impacts of the modeller’s actions are summarised in Panel C.\n\n\n\n\nCode\nmagick::image_read_pdf(here::here(\"figures/synthetic_example_QRPs.pdf\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: A synthetic illustration of model fishing & scenario hacking (HARKing). A. Violin boxplots of predicted species abundance for two management actions from two models constructed at different stages of the modelling process: a priori model (stage 1), a model generated from a model fishing exercise (stage 2), and the same model, but illustrating scenario hacking (stage 3). Dots are predicted values. Violin outlines illustrate kernel density probability distributions, where the width of the shaded area represents the corresponding proportion of data. The model-estimated median and quartiles are displayed for each action. Colours correspond to the scenario actions displayed in Panel C. B. Performance Scores calculated from multiple model performance measures for the Initial Model and a new, superior Overfitted Model derived from model fishing (greater overall Performance Score). See Lüdecke et al. (2020) for metric calculation details. C. Predicted species abundance as a function of habitat quality for the Initial Model (yellow line) and the Overfitted Model generated from the model fishing exercise (dashed orange line). The management action scenarios used in the first two stages of modelling are shown as solid light blue and light green lines. Scenario hacking occurs when the modeller selects two new management scenarios with a greater difference in mean predicted species abundance under the Overfitted Model. See Appendix 1.7 for code.\n\n\n\n\n\n\n\n\n\nIn this section, I extend Gelman and Loken’s (2013) mathematical formalism explaining the emergence of QRPs, or the “garden of forking paths” to model-based research. Outlining the mathematical formulation of QRPs for ecological modelling helps to formally differentiate defensible and questionable data-dependent decisions.\nAs emphasised above, I hesitate to designate all data-dependent analytic decisions as questionable, as is implied in the prevailing literature on preregistration and QRPs. There are situations in modelling where decisions are necessarily dependent on the outcome of previous analytic decisions within the modelling workflow (Liu, Althoff, and Heer 2020), and so not all data-dependent analytic decisions are automatically questionable within the context of modelling. For example, many modelling decisions are data-driven, like the choice to remove correlated variables or checking for distributional assumptions to aid in deciding the most appropriate model functional form (See Gould et al. (2025), Figure 4for an example from the case study). Liu, Althoff, and Heer (2020) distinguish defensible from questionable motivations for engaging in data-dependent analytic decision-making, by classifying them as either systematic or opportunistic, respectively.\nBox 2 helps us to formally distinguish between defensible and opportunistic data-dependent decisions, for both analytic and reporting decisions. This, in turn, helps to identify and distinguish between different types of QRPs in model-based research.\n\nBox 2: Formal Description of Questionable Research Practices\nThe modeller makes a series of analytic decisions to derive \\(M_p\\) from \\(M_s\\), from \\(M_c\\), referred to hereafter as \\(M\\) for simplicity (see Section 1.1.2 for notation definitions). We term the sequence of modelling choices throughout the modelling process the realised “modelling path.” Analytic uncertainty, or analogously ‘researcher degrees of freedom,’ propagates combinatorially along each decision-point to inform a multiplicity of plausible analysis strategies (Hoffmann et al. 2021) constituting the “garden of forking paths” (Gelman and Loken 2013). Consider that a modeller faces some decision \\(C\\) along that path about a modelling task concerning model \\(M\\) and some observed data \\(\\symbf{X}\\), with a predetermined choice or decision rule \\(\\phi\\). Decisions made before observing data or model outputs reflect idealised practice where choices are predetermined and independent of results \\(C(\\mathbf{X}, M; \\phi)\\). When the modelling choice is “data-contingent” \\(\\phi(\\symbf{X},M, M_j)\\) insofar as it hinges on the observed state of the model(s) \\(M\\) and/or any associated outputs \\(M_j\\) and data \\(\\symbf{X}\\) at that point along the modelling path, it satisfies a broad definition of ‘questionable.’\nI define defensible data-dependent decisions \\(\\phi_D\\) as following a systematic process \\(\\Psi_{\\text{systematic}}\\):\n\\[\n\\begin{aligned}\n\\phi_D(\\mathbf{X}, M; \\Omega, \\Psi_{\\text{systematic}}) = g(\\mathbf{X}, M, \\Omega)\n\\end{aligned}\n\\tag{1}\\]\nWhere \\(g(x)\\) is a deterministic function of the data, model state, and predefined decision-rule \\(\\Omega\\), based on systematic objectives such as, model adequacy, predictive accuracy or theoretical consistency.\nIn contrast, we define questionable practices \\(\\phi_Q\\) as:\n\\[\n\\begin{aligned}\n\\phi_Q(\\mathbf{X}, M; \\Psi_{\\text{opportunistic}}) = \\phi^* \\\\\n& \\text{ such that } h(\\phi^* | \\mathbf{X}, M, R^*) \\geq h(\\phi | \\mathbf{X}, M, R^*) \\\\\n&\\text{ for all } \\phi \\in \\Phi\n\\end{aligned}\n\\tag{2}\\]\nWhere the decision-making is opportunistic and result-seeking \\(\\Psi_{\\text{opportunistic}}\\), and \\(h(\\phi | \\mathbf{X}, M, R^*)\\) represents how well decision \\(\\phi\\) serves the desired outcome \\(R^*\\). Data-contingent decisions are therefore questionable when a researcher’s drive to make their research publishable influences the direction that the realised modelling path takes.\nA defining aspect of QRPs is that they remain undisclosed. Just as decisions about the modelling process can be questionable, so too can reporting practices. We apply the same logic describing questionable modelling practices to reporting practices:\n\\[\n\\begin{aligned}C_{\\text{Reported}} = S({C(\\mathbf{X}, M, \\phi)})\\end{aligned}\n\\tag{3}\\]\nWhere \\(S\\) is a selecting function that determines what to report from a set of conducted analyses.\nJournal method or article length conventions restrict complete transparency, and not all results can be reported. The decision about what to report from a set of conducted analyses \\(S\\) is made following predetermined plans \\(\\Omega\\), not contingent on observed results \\(S_{\\text{pre}}\\):\n\\[\n\\begin{aligned}S_{\\text{pre}}({C(\\mathbf{X}, M, \\phi)}) = s({C(\\symbf{X}, M, \\phi)},\\Omega)\\end{aligned}\n\\tag{4}\\]\nQuestionable reporting \\(S_Q\\), or as it is more commonly known, selective reporting, occurs when the reporting is opportunistic and contingent on the observed results, and optimised for desired outcomes \\(R^*\\):\n\\[\n\\begin{aligned}S_Q({C(\\mathbf{X}, M, \\phi)}; \\Psi_{\\text{opportunistic}}) = C^* \\\\ & \\text{ such that } h(C^*|R^*) \\geq h(C|R^*) \\\\\n& \\text{ for all } C \\in {C(\\mathbf{X}, M, \\phi)}\\end{aligned}\n\\tag{5}\\]\nI avoid defining \\(\\Psi_{\\text{systematic}}\\) reporting decisions, as we did for modelling decisions, and instead advocate modellers prespecify what results will be reported.\n\n\n\n\n\nHere, I present a typology of QRPs from which we designate the practices as questionable or defensible (Table 2). The typology considers combinations of the decision-making mechanism (a priori, defensible and questionable data-dependent decision-making), the target \\(T\\) of the practice (the model \\(M\\) or model outputs \\(M_j\\)), and the nature of reporting (prespecified or selective). This allows us to account for QRPs where the model and/or model outputs are directly affected by the questionable practice such that their realisations are different from what would have been observed if the practice was not undertaken (i.e. \\(M^*\\), \\(M_j^*\\)), as well as QRPs where the model and outputs remain unaffected, but are selectively reported.\nTable 2 summarises the key distinctions resulting from the workings in Box 2, helping to distinguish between questionable and defensible data-dependent decision-making, and to identify different classes of QRPs, with which we provide formal examples. The rows in Table 2 discriminate between a priori and (defensible versus questionable) data-dependent decisions, while the columns distinguish between prespecified and selective reporting.\n\n\nCode\n##| results: asis #equations do not render otherwise\n##| html-table-processing: none #equations do not render otherwise\nmagick::image_read(here::here(\"tables\", \"QRP_typology.png\")) |&gt;\n  magick::image_scale(geometry = \"70%\")\n\n\n\n\nTable 2: Distinguishing between questionable and defensible motivations for decision making during modelling, and the target of the decisions. We designate questionable practices in grey shaded cells and assign practices to classes of QRPs (described in Table 3). See Box 2 for notation and expanded definitions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo questionable practices occur when analytic and reporting decisions are made a priori (first row, first column) representing an idealised scenario (e.g. preregistered analyses), which is difficult to implement in practice for complex ecological modelling. Moving across to the right, all decisions about the modelling and analysis are made a priori, but the results are selectively reported to improve the apparent suitability of the model to the analysis problem (questionable).\nThe subsequent row represents situations where there are data-dependent choices made by the modeller, representing most situations in ecological modelling. In the first instance, there is some process stipulated a priori for deciding on how the modeller will resolve any data-dependent decisions (e.g. ‘registered flexibility,’ Gould et al. (2025)), and it is already decided what results will be reported (defensible). Moving to the next column over, the modeller uses registered flexibility to inform modelling choices, but in this case, they selectively report some results (questionable). In the first two rows, choices about the model and modelling analysis remain unaffected by QRPs, even when selective reporting occurs.\nThe final row of Table 2 indicates QRPs where data-contingent decisions are optimised for preferred results \\(R^*\\), affecting either the model and/or modelling outputs. Because QRPs involve a degree of non-disclosure and intransparency by definition (Definition 1), we have merged the two columns that distinguish between the presence of selective reporting. In Box 1 we formally illustrate this with two example QRPs, model fishing (affecting the model \\(M\\) itself) and scenario hacking (affecting the model outputs \\(M_j\\) only).\n\n\n\n\n\n\n\nDefinition 1 QRPs occur when a researcher makes opportunistic data-dependent analytic and/or reporting decisions; i.e., decisions that depend on an undisclosed desired outcome rather than a prespecified objective decision criterion, and which artificially inflate the apparent, accuracy, precision or performance of a model and/or its outputs, such that the model is perceived to be more publishable than it would be if the QRP had not occurred.\n\n\n\n\nIn Table 3, we present formal descriptions and practical examples of the different types of QRP in the typology. While some example scenarios may appear to be defensible data-dependent decisions (for example, testing different functional forms under ‘model fishing’), we include corresponding formal descriptions to remind the reader of the distinction between systematic and opportunistic analytic decision-making which denotes when data-contingent decisions are questionable, or not. We elaborate on these practices in the QRP map above (Section 1.2).\nIn summary, based on the mathematical formalism and typology, a clearer definition of QRPs in model-based research is apparent.\n\nCode\nlibrary(gt)\nlibrary(tidyverse)\nlibrary(gtExtras)\nlibrary(gluedown)\n\nQRP_definition_table &lt;-\n        readr::read_delim(\n                file = here::here(\"ms/Table_3.md\"),\n                delim = \"|\",\n                trim_ws = TRUE,\n                col_select = c(\n                        class,\n                        definition,\n                        example_scenarios,\n                        description,\n                        formal_description\n                ),\n                show_col_types = FALSE,\n                skip_empty_rows = TRUE\n        ) %&gt;%\n        slice(-1) %&gt;%\n        group_by(class, definition) %&gt;%\n        reframe(\n                example_scenarios = str_split(example_scenarios, \"- \"),\n                description, formal_description\n        ) %&gt;%\n        rowwise() %&gt;%\n        mutate(\n                example_scenarios = list(str_subset(example_scenarios, \".+\") %&gt;%\n                        gluedown::md_bullet() %&gt;%\n                        str_flatten(\"\\n\")),\n                class = trimws(class),\n                class = factor(class, levels = c(\"**Selective Reporting** Choosing to report only certain analyses, models, metrics, model results or comparisons that yield favourable results or desired conclusions without disclosing the full range of analyses performed.\", \"**Model Fishing**\", \"**S-hacking** Statistic-hacking\", \"**Sample Curation**\", \"**HARKing** Hypothesising After Results are Known\", \"**Overhyping**\"))\n        ) %&gt;%\n        unnest(example_scenarios) %&gt;%\n        arrange(class) %&gt;%\n        gt(\n                rowname_col = c(\"definition\"),\n                groupname_col = \"class\",\n                # row_group_as_column = TRUE,\n                process_md = TRUE,\n        ) %&gt;%\n        gt::fmt_markdown(columns = (everything())) %&gt;%\n        gt::cols_merge(\n                columns = c(\"class\", \"definition\"),\n                rows = 1\n        ) %&gt;%\n        gt::cols_label(\n                example_scenarios = \"Example Scenarios\",\n                description = \"Description\",\n                formal_description = \"Formal Description\"\n        ) %&gt;%\n        gt::cols_align(\"right\", formal_description) %&gt;%\n        gt::tab_stubhead(label = \"QRP Class and Definition\") %&gt;%\n        gtExtras::gt_theme_538(quiet = TRUE) %&gt;%\n        gt::tab_style(\n                style = cell_text(v_align = \"top\"),\n                locations = cells_body(columns = everything())\n        ) %&gt;%\n        gt::tab_style(\n                style = cell_text(v_align = \"top\", style = \"italic\"),\n                locations = cells_stub()\n        ) %&gt;%\n        gt::cols_width(example_scenarios ~ gt::px(400)) %&gt;%\n        gt::cols_merge(\n                columns = contains(\"description\"),\n                rows = 7, pattern = \"{1}\", hide_columns = FALSE\n        ) %&gt;%\n        gt::sub_missing() %&gt;%\n        opt_vertical_padding(scale = 0.1) %&gt;%\n        gt::tab_style(cell_text(weight = \"bold\", stretch = \"condensed\"), cells_column_labels()) %&gt;%\n        gt::tab_style(cell_text(weight = \"bold\", stretch = \"condensed\"), cells_stubhead())\n\nQRP_definition_table\n\n\nCode\n# gtExtras::gtsave_extra(QRP_definition_table,\n#                        path = here::here(\"tables\"),\n#                        filename =  \"QRP_definition_table.png\")\n# gt::gtsave(QRP_definition_table,\n#         path = here::here(\"tables/\"),\n#         filename = \"QRP_definition_table.html\"\n# )\n# webshot2::webshot(here::here(\"tables\", \"QRP_definition_table.html\"),\n#                   here::here(\"tables\", \"QRP_definition_table.png\"),\n#                   vheight = 1000,\n#                   vwidth = 900)\n\n\n\n\nTable 3: Formal descriptions of QRP classes, their definitions and some practical examples.\n\n\n\n\n\n\n\n\n\n\nQRP Class and Definition\nExample Scenarios\nDescription\nFormal Description\n\n\n\n\nSelective Reporting Choosing to report only certain analyses, models, metrics, model results or comparisons that yield favourable results or desired conclusions without disclosing the full range of analyses performed.\n\n\nMultiple model testing with selective reporting\nReporting only the model with best R² (testing linear, polynomial, and exponential models)Highlighting only the ecological model supporting preferred hypothesis\n\nReporting only the model with best R² (testing linear, polynomial, and exponential models)\nHighlighting only the ecological model supporting preferred hypothesis\n\n\nTesting multiple model structures \\((M_1, M_2, \\ldots, M_k)\\) and reporting only the model that produces results most align* with desired outcomes \\(R^*\\).\n\\[\\begin{align*} S_Q(\\{C(\\boldsymbol{X}, M_k, \\phi^{(M)})\\}_{k=1}^K; R^*) = C(\\boldsymbol{X}, M_{k^*}, \\phi^{(M)}) \\\\ \\text{ such that } \\\\ h(C(\\boldsymbol{X}, M_{k^*}, \\phi^{(M)}) \\vert R^*) \\geq h(C(\\boldsymbol{X}, M_k, \\phi^{(M)}) \\vert R^*) \\\\ \\text{ for all } k \\in \\{1,\\ldots,K\\} \\end{align*}\\]\n\n\nMultiple output evaluation with selective reporting\nReporting only significant goodness-of-fit metricsHighlighting only time periods showing desired trendsEmphasising model predictions supporting preferred conclusions\n\nReporting only significant goodness-of-fit metrics\nHighlighting only time periods showing desired trends\nEmphasising model predictions supporting preferred conclusions\n\n\nEvaluating multiple outputs \\(({M_j}_1, {M_j}_2, \\ldots, {M_j}_j)\\) from same model (different metrics, time periods, spatial scales, etc.) and reporting  only those that align with desired outcomes \\(R^*\\).\n\\[\\begin{align*} S_Q(\\{C(\\boldsymbol{X}, M, \\phi_j^{(M_j)})\\}_{j=1}^J; R^*) = C(\\boldsymbol{X}, M, \\phi_{j^*}^{(M_j)}) \\\\ \\text{ such that } h(C(\\boldsymbol{X}, M, \\phi_{j^*}^{(M_j)}) \\vert R^*) \\geq h(C(\\boldsymbol{X}, M, \\phi_j^{(M_j)}) \\vert R^*) \\\\ \\text{ for all } j \\in \\{1,\\ldots,J\\} \\end{align*}\\]\n\n\nModel Fishing\n\n\nExploring model specifications or (fitting) variable combinations without theoretical justification, seeking favourable results, undisclosed.\nAdopting the ecological process model that supports preferred management conclusionsChoosing model covariates giving desired effect directionSettling on model complexity producing significant results rather than optimal predictive performance\n\nAdopting the ecological process model that supports preferred management conclusions\nChoosing model covariates giving desired effect direction\nSettling on model complexity producing significant results rather than optimal predictive performance\n\n\nTesting multiple model \\((M_1, M_2, \\ldots M_k\\)) structures or specifications and selecting model based on results most align* with desired outcomes \\(R^*\\), rather than following pre-specified model selection criteria or theoretical justification.\n\\[\\begin{align*} \\phi_Q^{(M)}(\\boldsymbol{X}, M; R^*) = M_{k^*} \\text{ such that } \\\\ h(C(\\boldsymbol{X}, M_{k^*}, \\phi) \\vert R^*) \\geq h(C(\\boldsymbol{X}, M_k, \\phi) \\vert R^*) \\\\ \\text{ for all } k \\end{align*}\\]\n\n\nS-hacking Statistic-hacking\n\n\nManipulating model inputs, outputs or the model itself (random seeds, outcome variable and/or performance thresholds or metrics) to obtain a favourable value of a performance measure (statistic or metric), without disclosing.\nReporting only best performing goodness-of-fit metrics (after testing \\(R^2\\), AIC, RMSE, etc)Selecting validation approaches (cross-validation, holdout, bootstrap) based on most accurateAdjusting thresholds until desired statistical outcomes are achieved\n\nReporting only best performing goodness-of-fit metrics (after testing \\(R^2\\), AIC, RMSE, etc)\nSelecting validation approaches (cross-validation, holdout, bootstrap) based on most accurate\nAdjusting thresholds until desired statistical outcomes are achieved\n\n\nTrialling different performance metrics (\\({M_j}_1, {M_j}_2, \\ldots, {M_j}_j\\)), evaluation thresholds, or validation approaches and selecting metric with best model performance \\(R^*\\) (not pre-specified or theoretically justified).\n\\[\\begin{align*}\\phi_Q^{(Mj)}(\\boldsymbol{X}, M; R^*) = M_{j^*} \\text{ such that } \\\\ h(M_{j^*}(C(\\boldsymbol{X}, M, \\phi))  \\vert  R^*) \\geq h(M_j(C(\\boldsymbol{X}, M, \\phi))  \\vert  R^*) \\\\ \\text{ for all } j \\end{align*}\\]\n\n\nSample Curation\n\n\nSelectively including, excluding, or modifying data points in the sample used to develop or evaluate a model without disclosure, often to improve model performance (a specific instance of S-hacking).\nRemoving “inconvenient” sites that don’t fit expected patternsAdjusting temporal boundaries to exclude unfavourable periods\n\nRemoving “inconvenient” sites that don’t fit expected patterns\nAdjusting temporal boundaries to exclude unfavourable periods\n\n\nThe modeller trials different curated versions of the original dataset \\(\\boldsymbol{X}\\) in order to achieve a model that meets desired outcomes \\(R^*\\), without a priori exclusion rules or statistical/ theoretical basis.\n\\[\\begin{align*}\\phi_Q^{(\\boldsymbol{X})}(\\boldsymbol{X}, M; R^*) = \\boldsymbol{X}^* \\text{ such that } \\\\ h(C(\\boldsymbol{X}^*, M, \\phi)  \\vert  R^*) \\geq h(C(\\boldsymbol{X}', M, \\phi)  \\vert  R^*) \\\\ \\text{ for all } \\boldsymbol{X}' \\in \\mathcal{X}(\\boldsymbol{X})\\end{align*}\\]\n\n\nHARKing Hypothesising After Results are Known\n\n\nSelected model(s) and/or results presented as if pre-specified and theoretically justified before data exploration or parameterisation, and model selection procedure is not disclosed or adequately described. May accompany fishing expeditions, model dredging analyses and model selection procedures that have been selectively reported.\nPost-hoc ecological theories used to justify variable inclusion, when exploratory analyses reveals unexpected predictive variablesTheoretical explanation created post-hoc for model structures that unexpectedly perform well against other models\n\nPost-hoc ecological theories used to justify variable inclusion, when exploratory analyses reveals unexpected predictive variables\nTheoretical explanation created post-hoc for model structures that unexpectedly perform well against other models\n\n\nPost-hoc conceptual models are constructed to explain model results, serving desired outcomes \\(R^*\\).\n\\[\\begin{align*}H_Q(\\boldsymbol{X}, M, C; R^*) = H^* \\text{ such that } \\\\ \\text{consistency}(H^*, C(\\boldsymbol{X}, M, \\phi)) \\geq \\text{consistency}(H', C(\\boldsymbol{X}, M, \\phi)) \\\\ \\text{ for all } H' \\in \\mathcal{H}\\end{align*}\\] , where: \\(M_c^∗\\) represents the post-hoc conceptual model, \\(M_c\\) is the space of possible conceptual models, consistency() measures how well a conceptual model explains the observed results\n\n\nOverhyping\n\n\nExaggerating the capabilities, generalisability, or reliability of a model beyond what is justified by the evidence.\nTypically, overhyping features in the discussion section of a paper\n\nTypically, overhyping features in the discussion section of a paper\n\n\n—\n—\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearcher degrees of freedom threaten the credibility and reliability of model-based research, just as they do in hypothesis testing research. The findings of this Chapter underscore that researcher degrees of freedom abound in the modelling process, providing ample opportunity for QRPs that accompany researchers’ drive to publish. This aligns with Liu et al.’s (2020) qualitative analysis of how researchers make analytic decisions when faced with arbitrary choices or analytic uncertainty in the context of a research culture that promotes publication bias. We showed that QRPs can occur at any point in the modelling process, and may affect different model objects, including the model inputs, the model itself, or model outputs. While our analysis identified that classes of QRPs are analogous to those in hypothesis-testing research, we also showed that there are unique aspects of methodological practices in ecological modelling that mean we need to define ‘questionable’ research practices in our own terms – namely, in terms that accommodate the iterative and adaptive nature of the modelling process and the need to make data-contingent decisions when modelling. These features of model development have resulted in significant resistance to the idea of QRPs in model-based research, and to the applicability of preregistration for mitigating them in ecological modelling (MacEachern and Van Zandt 2019; Dwork et al. 2015). We explore these tensions below, emphasising how the conceptual framework provides a way forward through the tricky problem of delineating where the concept of QRPs apply in ecological modelling, and where it does not.\n\n\nMany of the practices I identified as ‘questionable’ could simply be considered ‘poor practice,’ especially when those practices result in biased or overfitted models. The modelling context, including constraints on feasibility, data availability and coverage, together with the model purpose (e.g. prediction versus inference) will delineate when such practices are questionable or methodologically flawed. The fundamental issue with QRPs is that they remain undisclosed. Given that QRPs are practices that artificially improve the way models are perceived, full transparency allows the reader to appraise the appropriateness of practices like altering data, changing model specifications, or calculating additional performance metrics, contingent on the modelling context (Woo, O’Boyle, and Spector 2017). Transparent reporting is essential for properly evaluating the credibility and suitability of the model for its intended application.\n\n\n\nGelman and Loken’s (2013) formalism describing the garden of forking paths implies that data-dependent decisions, at least in the context of null hypothesis significance testing, are inherently questionable. This has limited relevance in ecological modelling because it is inherently adaptive. My extension of the formalism to model-based research circumvents this incompatibility by distinguishing opportunistic from systematic data-dependent decisions. Based on our formalism, I argue that data-driven analysis decisions are not inherently questionable. Questionable practices occur when the decision is contingent on the observed results, and the choice is based on how well it serves undisclosed desired outcomes, whereas defensible data-contingent decisions follow a prespecified decision rule.\n\n\n\nThe nature of ecological data confers specific vulnerabilities to QRPs during ecological modelling. Small datasets are prevalent in ecology and often have inconsistent structure due to being collected intermittently or on a one-off occasion (Todman, Bush, and Hood 2023), or there are spatial constraints. Additionally, data collection is highly constrained by budget and logistical feasibility, consequently field ecologists often take a ‘kitchen sink’ approach to data collection, whereby they “often measure almost everything they can” (Mac Nally 2000, 669). Models that analyse small datasets are more likely to be overfitted due to the high number of parameters compared to the degrees of freedom in the data (Todman, Bush, and Hood 2023).\nThese conditions provide substantial opportunity for unconstrained dredging of model space whereby modellers include covariates with little or no theoretical justification or ecological relevance, leading to biologically implausible models being considered (Fourcade, Besnard, and Secondi 2018; Shmueli 2010; Franks, Ruxton, and Sherratt 2025). Although I classified these “causal salad” approaches to modelling (McElreath 2020) as ‘poor practice’ instead of QRPs, when modellers are engaged in model dredging without predetermined selection rules and criteria, the risk of both poor and questionable practices, like model fishing, is heightened under these conditions.\nThe same data constraints that facilitate model dredging also inhibit the detection of resulting problems. When datasets are small or incomplete, there is often insufficient data to perform model evaluation on independent data (Bodner, Fortin, and Molnár 2020; Dietze et al. 2018; Wood et al. 2020). When models are evaluated on training data only, data leakage causes biased estimates of performance making overfitting hard to detect (Lewis et al. 2023; Stock, Gregr, and Chan 2023; Christin, Hervet, and Lecomte 2020; Kapoor and Narayanan 2023).\n\n\n\nMany of the questionable practices I identified – particularly overhyping claims and misreporting correlative findings with causal language – specifically target perceptions of model reliability, accuracy, and generalisability rather than traditional statistical thresholds. This supports a model-centric definition of QRPs as practices that “artificially inflate the apparent accuracy or precision of a model, its predictions, and/or evaluation tests.” The mathematical formalism in the typology demonstrates the diverse ways researchers can manipulate both technical model properties and broader perceptions of model fitness for purpose, providing a comprehensive framework for understanding questionable practices in model-based research.\nQRPs collectively undermine the reliability and reproducibility of ecological modelling research in several ways:\n\nInflated performance estimates that do not reflect true performance and result in overfitting.\nSpurious model selection that identifies models based on chance rather than plausible biological mechanisms or predictive relationships.\nReduced reproducibility due to undisclosed researcher degrees of freedom.\nCompromised generalisability from overfitted models that fail to transfer to new contexts or make accurate forecasts.\nFalse confidence in ecological understanding and management recommendations.\n\nOverfitted models are fitted to both regular and irregular features of the sampled data but are unable to distinguish between them (Pu et al. 2019), generating spurious predictions that poorly generalise to new data (Todman, Bush, and Hood 2023; Lewis et al. 2023). In applied ecological modelling, where modelling is often focused on generating anticipatory predictions to inform management or policy decisions, this is particularly problematic.\nThe prevalence of these practices suggests systemic issues in training, incentives, and quality control within the ecological modelling community. The concentration of QRPs in the model construction and evaluation phases indicates particular vulnerabilities in how models are specified, fitted, and evaluated. The large number of decision-points where researchers can exercise degrees of freedom suggest that safeguards should target these critical phases of the modelling process.\n\n\n\n\n\nAwareness of the distinction between systematic and opportunistic data-dependent decisions is limited, and because some data-dependent decisions are a legitimate aspect of the modelling process, it may seem that all data-dependent decisions are acceptable. The impact of data-dependent decisions in machine learning is increasingly understood and is encapsulated within the term ‘data leakage,’ whereas in ecological modelling more broadly, the equivalent problem of ‘model selection bias’ remains mostly overlooked (Campbell 2021) and underappreciated within applied research contexts (Arnqvist 2020). Here, I emphasise an intersecting problem that has been attributed as a major cause of science’s reproducibility crisis, where data-contingent decisions may be opportunistically exploited to increase the likelihood of publication. I have formalised the distinction between defensible or questionable research practices, facilitating a modelling-appropriate conceptualisation of QRPs. As a first step in addressing the threat of QRPs to the credibility of model-based research, I wish to draw attention to this distinction – and the possible consequences of QRPs – among the ecological modelling community. However, given that cognitive biases are rarely deliberate, awareness alone is insufficient for preventing QRPs within a publish-or-perish research culture (Zvereva and Kozlov 2021).\n\n\n\nModelling is not typically transparent, leaving readers unable to assess whether appropriate models were used or to identify the primary research contribution (Arnqvist 2020). Given that the threat of QRPs largely stems from a lack of disclosure, ecological modelling is at significant risk of QRPs. I echo broader calls for improving transparency in ecology (Parker et al. 2016; Powers and Hampton 2018; Rose E. O’Dea et al. 2021), emphasising that transparency is a fundamental requirement for reducing the risk of QRPs in ecological modelling. It is acknowledged that modelling’s lack of transparency is, in large part, driven by sociocultural and institutional norms that restrict the length of a paper, require a neat and coherent narrative and favour some data analysis techniques and results over others (Rijnhart et al. 2021). Broader methodological reform in research culture, as well as specific tools, are needed to achieve improvements in transparency.\nReporting checklists and guidelines outline a minimum set of methodological elements and results to include in published research and are increasingly being adopted by journals in ecology (Nature 2018; Fidler et al. 2018; Hillebrand and Gurevitch 2013; Haddaway et al. 2018; R. E. O’Dea et al. 2021). However, there are only a handful of reporting checklists developed for ecological modelling, and no ecology journals have encouraged or mandated modelling-specific checklists at the time of writing. I leave the work of defining the content of reporting checklists up to the ecological modelling community, but reiterate repeated calls in the modelling community to articulate the model’s purpose, context and performance criteria, ideally before modelling begins (Wood et al. 2020; Bennett et al. 2013; Jakeman, Letcher, and Norton 2006). This chapter illustrates when practices are questionable, and that many QRPs target or alter model performance metrics – either through direct manipulation of the model and model outputs, or through selective reporting. Specifying these decisions a priori and reporting them reduces inadvertent engagement in QRPs and equips readers to evaluate the risk of QRPs.\n\n\n\nPreregistration, and registered reports, have been hailed as a solution for preventing QRPs, and recent metaresearch empirically supports its efficacy (Burgman et al. 2023; Purgar et al. 2024; Nakagawa et al. 2025). However, there has been substantial resistance in model-related fields (MacEachern and Van Zandt 2019; Dwork et al. 2015) because preregistration is geared towards a NHST-focused definition of QRPs, that is, data-dependent analytic decisions. I argue here that there is a distinction to be made between systematic and opportunistic data-dependent analytic decisions in ecological modelling, where only the latter are questionable. For preregistration to be applied to ecological modelling, its internal logic must reflect alternative conceptualisations of QRPs that accommodate legitimate data-contingent decisions and iteration. It should allow for model revision while avoiding premature commitment to one approach (Hämäläinen and Lahtinen 2016; Benning et al. 2019; Evans et al. 2023). In Gould et al. (2025), I develop, apply and evaluate Adaptive Preregistration as a potential solution. If a complete preregistration is impractical, then at the very least, specifying a minimum set of evaluation analyses, metrics, and their performance criteria a priori is essential for avoiding QRPs.\n\n\n\n\nThis paper has gone some way towards characterising QRPs to accommodate a diversity of modelling types within ecology. Further research could provide a deeper understanding of where and when these are applicable across ecology (or not). For example, looking across subfields, methodological approaches or model purposes: Do some QRPs pose more of a threat to reliability than others? Are some more likely than others? Are there specific forms they take? Preventative measures can then be tailored to particular use-cases.\nThis list of QRPs is not exhaustive, future research could also characterise additional QRPs not described here, perhaps turning to other fields utilising model-based research to understand where questionable practices are more widely appreciated, such as Machine Learning (Hildebrandt 2018; McDermott et al. 2021; Stock, Gregr, and Chan 2023; Garbin and Marques 2022; Rosenblatt et al. 2024; Hosseini et al. 2020; Meding and Hagendorff 2024). Further, understanding the prevalence of QRPs in ecological modelling would give an idea of the extent of the problem in the published literature and help prioritise potential reforms. Self-report surveys (e.g. Fraser et al. 2018) using our modelling-specific QRP classification would be a useful starting point. Empirical approaches to detecting the extent of QRPs might include approaches similar to p-curve analysis but investigating relevant model performance metrics (White et al. 2023).\n\n\n\nIn this paper, I aim to raise awareness among ecological modellers, and modellers among other scientific disciplines, about potential types of QRPs and their mechanisms for emergence in the modelling process. This is the first attempt to articulate how questionable research practices occur outside hypothesis testing research. The application is specific to ecological modelling, but the definition of QRPs presented here provides insights for modelling in other fields and other forms of non-hypothesis testing research. The conceptual framework and map of QRPs in this paper helps modellers understand the risks of QRPs in their research, so they are empowered to implement procedures that can mitigate their occurrence in their own research practice. Finally, meta-researchers and advocates of open-science can use the conceptual framework to underpin the design of modelling-appropriate methodological reforms that improve the credibility and robustness of model-based research in ecology and other fields."
  },
  {
    "objectID": "ms/QRPs in Ecological Modelling.html#conditions-for-questionable-research-practices-in-ecological-modelling",
    "href": "ms/QRPs in Ecological Modelling.html#conditions-for-questionable-research-practices-in-ecological-modelling",
    "title": "A Framework for Questionable Research Practices in Ecological Modelling",
    "section": "",
    "text": "Below we outline the conditions under which QRPs may arise in ecological modelling and give an overview of the modelling process and the ‘objects’ it produces (inputs, outputs, the model itself). This sets the scene for exploring how QRPs could unfold in model-based research.\n\n\nUnderstanding what makes ecological models “publishable” is crucial for identifying where QRPs might emerge, since publication bias provides a primary motivation for engaging in questionable practices (Ware and Munafò 2015). Unlike hypothesis testing research where p-values serve as the main target for manipulation, model-based research involves multiple attributes that collectively determine publishability. These attributes become potential targets for the QRP classes we later identify in the typology (Table 2).\n\n\nNovelty is an important factor influencing the publishability of modelling research. Publication bias and funding stipulations reward and require advantage over existing approaches; either through development of new methodological approaches, enhanced performance of existing algorithms and modelling methods, or application of existing models to new contexts, such as new environmental conditions or ecological contexts (Alexandrov et al. 2011). Conversely, publication bias disincentivises the evaluation and testing of existing published models (Babel, Vinck, and Karssenberg 2019; Houlahan et al. 2016). This requirement for novelty incentivises model fishing and selective reporting, where researchers may opportunistically explore new modelling approaches until they achieve apparent superiority over baseline approaches.\n\n\n\nModel credibility is based on the subjective degree of confidence that both the model and model-derived inferences about the real system can be used as claimed (Risbey et al. 2005; Augusiak, Van den Brink, and Grimm 2014; Hamilton et al. 2019). That is, can the model adequately answer the research question (Christin, Hervet, and Lecomte 2020), and can it be used reliably to inform management decisions (Alexandrov et al. 2011)? Credibility emerges gradually throughout the modelling process, by demonstrated adequacy (conceptual validity and predictive accuracy, Rykiel Jr 1996) and reliability (consistent performance and transferability, Schmolke et al. 2010; Yates et al. 2018).\nUnlike the binary nature of statistical significance, model credibility builds incrementally through multiple performance metrics and evaluation approaches (Figure 1, model outputs). This multi-faceted assessment creates numerous opportunities for statistic hacking, or “S-hacking” (Table 2 and Table 3), where researchers can manipulate i) performance metric selection and thresholds, ii) validation approaches and data partitioning strategies, and iii) evaluation timeframes and spatial scales. The subjective nature of credibility assessment also fosters overhyping (Table 3), where model capabilities are overstated beyond what evaluation results justify.\n\n\n\nA suite of modelling objects is generated throughout the modelling process (described below), collectively building a subjective picture of the publishability and credibility of the model. These outputs may be manipulated to improve the chance of model acceptance or publication. Models that serve different purposes are vulnerable to different QRPs and depending on the combination of modelling approach, model type (e.g. agnostic, correlative, or mechanistic) and purpose of the model, QRPs will target different model objects (Hoffmann et al. 2021). For instance, when the modeller’s estimand of interest (“the target quantity to be estimated in an analysis,” Borger and Ramesh 2025, 2) are parameter estimates, like in many cases of explanatory modelling in ecology, then QRPs are likely to affect parameter estimates, parameter uncertainty, goodness-of-fit metrics, or variable importance measures. While for analyses concerned with model predictions, QRPs are more likely to affect model components, like forecast accuracy metrics and measures of model transferability. Different questionable practices are concentrated at different locations across modelling phases (Figure 2).\n\n\n\n\nTo help conceptualise where in the modelling process QRPs might emerge, and which ‘model objects’ QRPs may affect, we first give an overview of the modelling process, articulating various inputs and outputs, including the model itself, model fit statistics, summary measures and other evaluation results (Figure 1), to which we ascribe mathematical notation.1 This framework will also provide the foundations for extending Gelman and Loken’s (2013) mathematical formalism to modelling (which we do in Section 1.3).\nWe acknowledge the plurality and lack of consensus in how the modelling process is described (Lahtinen, Guillaume, and Hämäläinen 2017), including the terminology used for different modelling phases, steps and tasks (Schmolke et al. 2010; Augusiak, Van den Brink, and Grimm 2014). Rather than adopting a comprehensive taxonomy that captures all distinct processes and categories of modelling, we instead describe the modelling process at a high-level that can be generalised across different model purposes, contexts, types and methods. There will, of course, be exceptions. Some aspects may not apply in every modelling problem, and the specific collection of model objects, their relative weighting in informing study conclusions, and the relative weighting of publishable attributes, will differ depending on the model purpose, context and methodology applied to the problem at hand. We also recognise that analysis decisions are procedurally dependent (Liu, Althoff, and Heer 2020), for instance, the way models are specified and parameterised depends on the model type (i.e. whether using a correlative, mechanistic, or agnostic model, sensu Hoffmann et al. 2021) and modelling purpose (i.e. exploration, inference, prediction, see Tredennick et al. 2021).\nWe have divided the modelling process into three phases; 1) model construction, 2) model evaluation, and 3) model application. These distinctions align with the phases underpinning the preregistration template in Gould et al. (2025).\n\n\n\n\n\n\n\n\nFigure 1: Three phases of model development: model construction, where the conceptual model \\(M_c\\) is specified into the formal model (\\({M_c\\rightarrow M}_s\\)) then parameterised (\\(M_s{\\rightarrow M}_p\\)); model evaluation, where the calibration and validation fits are evaluated, possibly leading to re-specification and re-parameterisation (dashed arrow); and model application, where the model is analysed to answer the research question.  Modelling generates objects, including the conceptual, specified and parameterised model, (\\(\\symbf{M}=\\left\\{M_c,M_s,M_p\\right\\}\\), purple circles); model inputs (blue squares), including hyper parameters \\(\\lambda\\) and calibration settings, data \\(\\symbf{X}\\) for model parameterisation, evaluation and application. Model outputs (\\(M_j\\) green hexagons) include model predictions \\(\\hat{y}\\), which are used to characterise model performance during model evaluation or subject to further aggregation, transformation, analysis and visualisation during model application.  Note: Data analyses may also inform model specification during construction. New or alternative input data may be used during scenario analysis to make predictions or projections about how the system will respond to intervention \\(\\hat{y}\\).\n\n\n\n\n\n\n\nTo begin the model development process, a conceptual model, \\(M_c\\) or candidate set of models, \\(\\symbf{M_c}\\) is specified by the modeller, synthesising their understanding of the ecological system. Conceptual models may be represented by a set of qualitative statements, mathematical formulas, or else visually as plots or directed acyclic graphs (Shmueli 2010). A candidate set of multiple models at this stage may represent competing hypotheses, where differences in the structure and/or parameterisation of the models represents critical uncertainty about the ecological system.\n\n\n\nNext, the modeller formalises each conceptual model mathematically or statistically, \\(M_s\\) (Figure 1). The modeller chooses which variables should be included in the model, how to operationalise or represent them in the chosen framework, what the appropriate dependencies are between variables and the model type, and the functional form of the model (if relevant). Because the variables in the conceptual model are not directly observable, they are operationalised into measurable outcomes \\(Y\\) and input variables \\(X\\) in a data matrix \\(\\symbf{X} = {X_1, X_2,\\ldots,X_p}\\), where \\(p\\) is the number of input variables, and f represents a function relating \\(Y\\) to \\(X\\) such that \\(E\\left(Y\\right) = f\\left(\\symbf{X}\\right)\\) (Hoffmann et al. 2021; Shmueli 2010). Note that, for some predictive modelling contexts, such as data-driven modelling employing black-box algorithmic approaches, like machine-learning, \\(f\\) may not be specified and is instead represented by \\(\\mathscr{I}_{\\lambda}\\) where \\(\\mathscr{I}\\) represents some learning algorithm and \\(\\lambda\\) denotes its hyperparameters (following Bischl et al. 2023). Exploratory analyses are often conducted at this stage to inform variable selection, for example by analysing variable importance and examining collinearity among variables (Kass et al. 2025).\n\n\n\nNext, each specified model \\(M_s\\) is parameterised yielding \\(M_p\\) (Figure 1). Model parameters refer to any component of a model that can be quantified or estimated, such as slopes or intercepts in a linear regression or growth rate in a population model (García-Díaz et al. 2019, 2). Regardless of the overarching model purpose (e.g. explanation or prediction, Shmueli 2010), for correlative or agnostic (e.g. machine-learning) models, parameterisation typically occurs by estimation, or calibration, whereby the modeller applies techniques, like maximum-likelihood estimation or Bayesian inference, to the data matrix \\(\\symbf{X}\\) (Figure 1) to estimate the parameters specified by \\(f\\), with uncertainty (García-Díaz et al. 2019; Hoffmann et al. 2021), yielding \\(\\hat{y}=\\hat{f}\\left(\\symbf{X}\\right)\\). In the case of agnostic models, the algorithm \\(\\mathscr{I }_{\\lambda}\\) returns the fitted model and its parameters when applied to \\(\\symbf{X}\\), \\(\\hat{y}=\\hat{f}_{\\hat{\\lambda}}\\left(\\symbf{X}\\right)\\). Parameters of mechanistic models are typically provided as inputs to the specified model \\(f\\), gleaned from expert knowledge, published literature or via calibration (Hoffmann et al. 2021).\nWhen conducting inference or explanatory modelling, the estimand(s) of interest are the parameters \\(\\symbf{\\hat{\\theta}}\\), like standardised mean differences, correlation coefficients or response ratios (Williams et al. 2025), whereas for predictive modelling, predicted values \\(\\hat{y}\\) constitute the estimand(s) of interest (Tredennick et al. 2021; Hoffmann et al. 2021; Shmueli 2010). This is true regardless of whether the model is correlative Silk, Harrison, and Hodgson (2020), mechanistic (e.g. a population viability model), or agnostic (e.g. a machine learning or deep learning models, Pichler and Hartig 2023). However, different types of models are more likely to be used for inference or prediction in practice, for example, agnostic models are more likely to be used for prediction, but inferences about parameters are certainly possible (Lucas 2020). Note that agnostic modelling approaches require the modeller to supply hyperparameters \\(\\symbf{\\lambda}\\) (Figure 1), which may be decided by the modeller, or else estimated by some tuning or optimisation method \\(\\symbf{\\hat{\\lambda}}\\). Hyperparameters may influence the model learning process, such that with each set of hyperparameters the model will provide a different set of results (Ahmed et al. 2025).\n\n\n\nArriving at the optimal final model or collection of final models is typically iterative, determined by the outcomes of model validation and evaluation (Shmueli 2010) whereby the model is subjected to a series of analyses that generate performance measures that are used to establish its validity, reliability and credibility and ensuring that the model is fit for intended use (Bennett et al. 2013; Eker et al. 2018; García-Díaz et al. 2019; Rykiel Jr 1996).\nI distinguish between model validation and model evaluation. Model validation checks that the fitted model \\(M_p\\) suitably approximates the data \\(\\symbf{X}\\), and is evaluated using goodness-of-fit tests, and model diagnoses like residual analyses (Shmueli 2010). Model selection whittles down the candidate set of models into a single ‘best’ model or a smaller subset of ‘best models’ (e.g. AIC within \\(\\Delta2\\)), after which the modeller might choose to consider multiple models or conduct model averaging. Outside of formal model selection approaches, the procedure for determining the best model(s) may involve a degree of trial and error of different model structures that is not always preplanned (i.e. new \\(M_s\\) are specified after validation).\nFollowing validation, model evaluation of the best model(s) is undertaken, assessing the fitness of \\(M_p\\) for purpose by calculating additional performance measures to fully understand the model’s capabilities, like constructing confusion matrices or calculating omission and commission rates. Ultimately, model evaluation is case-dependent and context-specific insofar as the overall evaluation process, types of analyses, metrics, estimand of interest, and desirable properties of the model differing depending on the overarching modelling purpose and type of model and modelling approach (Tredennick et al. 2021; Bokulich 2013).\n\n\n\nOnce \\(M_p\\) is considered plausible and fit for purpose, the modeller shifts to model application (Engelschalt et al. 2023), querying the model and using the model to undertake analyses that inform the stated research questions (Figure 1). Prior to analysis, model output may be subject to further processing, for example, continuous predictions may be aggregated or transformed into binary predictions for visualisation and communication purposes (Feng et al. 2019). Explanatory model output may be visualised with coefficient plots, or effect size plots to inform the relevance of observed effects (Lüdecke et al. 2020). In applied settings, forecasts or anticipatory predictions into the future or across space are generated from the model based on plausible scenarios or to simulate outcomes under different management actions or policies (Paniw et al. 2023), which may be subject to a range of visualisations (e.g. Barros et al. 2023, fig. 2).\nTo summarise, a collection of model outputs are generated in modelling, which may variously be the target of QRPs, including: point-estimates; such as means, medians and effect-sizes; uncertainty measures, like confidence intervals, prediction intervals, standard errors; model performance metrics, like \\(R^2\\) / AIC / BIC; inference results, like p-values, credible intervals, and significance determinations; predictions and forecasts, like future values or classification outcomes; or, the model itself."
  },
  {
    "objectID": "ms/QRPs in Ecological Modelling.html#sec-QRP-roadmap",
    "href": "ms/QRPs in Ecological Modelling.html#sec-QRP-roadmap",
    "title": "A Framework for Questionable Research Practices in Ecological Modelling",
    "section": "",
    "text": "Here, we present a taxonomy and map of QRPs derived from the modelling literature, which aims to illustrate the different types of QRPs that might occur throughout different points in the modelling process. We follow with a synthetic example that reveals how these different types of QRPs might look in practice (Box 1).\n\n\nWe first surveyed the modelling literature to identify potential QRPs in ecological modelling and their location in the modelling process. QRPs were categorised into broader classes corresponding to families of similar practices using well-known published classifications (e.g. Table 1), adopting new classes when there was no analogue in the existing QRP literature. We coded the phase and sub-phase of modelling in which the practice occurs, as well as the target of the practice (input, model, output). After initial coding of the QRPs we generalised the descriptions of individual practices and categorised them according to a QRP class schema. The literature review and coding are described in further detail in Appendix 1.6.\n\n\n\nWe identified six classes of QRPs: sample curation, model fishing, selective reporting, S-hacking, overhyping, HARKing (Table 1). All classes of QRPs have analogous practices under NHST, but the practices themselves are not directly comparable. The list of QRPs we identified is not exhaustive and instead illustrates a range of practices that can occur in model-based research (See Appendix 1.6, Table 4 for the complete list).\nParadigmatic examples of QRPs are presented for each class in a roadmap (Figure 2), illustrating that QRPs occur throughout all phases of the modelling process, and some may occur at multiple stages. Sample curation, S-hacking and fishing were the classes of QRPs most likely to occur during model construction. The iterative nature of model validation and evaluation creates multiple opportunities for opportunistic optimisation of apparent model performance during model evaluation, with selective reporting, S-hacking and model fishing primarily affecting this phase of modelling. Fewer, but distinct, QRP types were identified for the model application phase, primarily concerning the misrepresentation of model capabilities and findings.\nThe target objects affected by QRPs across modelling phases reflected the focus of modelling activities and tasks, with QRPs affecting the model itself occurring primarily during model construction and evaluation, and QRPs affecting the model outputs being concentrated in model evaluation and model application phases. Below, we briefly describe the different classes of QRPs, providing illustrative examples and explaining how they can bias results.\n\n\nCode\nwaffle_plot_data &lt;-\n  tidy_QRP_database(here::here(\"data/QRP_database_2025-10-20.csv\")) %&gt;%\n  filter(include) %&gt;%\n  select(\n    -practice_notes,\n    -model_subphase,\n    -source,\n    -qrp_reason,\n    -include,\n    -starts_with(\"practice_\"),\n    -file_name,\n    practice_target\n  ) %&gt;%\n  distinct() %&gt;% #rm duplicate qrp_coded #TODO next merge duplicates while keeping source\n  drop_na() %&gt;% #interim approach until datachecks in place\n  mutate(\n    model_phase = str_split(model_phase, \", \"),\n    practice_target = str_split(practice_target, \", \"),\n    values = 1\n  ) %&gt;%\n  unnest(model_phase) %&gt;%\n  unnest(practice_target) %&gt;%\n  complete(\n    qrp_description,\n    model_phase,\n    practice_target,\n    fill = list(values = 0.0001) # see https://github.com/hrbrmstr/waffle/issues/66\n  ) %&gt;%\n  group_by(qrp_description) %&gt;%\n  fill(qrp_coded, model_phase, .direction = \"downup\") %&gt;%\n  ungroup()\n\n\n\nCode\nwaffle_plot &lt;- waffle_plot_data %&gt;%\n  distinct() %&gt;% #TODO identify duplicated rows - suspect non-unique practice_target coding is cause\n  mutate(\n    model_phase = forcats::as_factor(model_phase) %&gt;%\n      forcats::fct_relevel(c(\n        \"Model Construction\",\n        \"Model Evaluation\",\n        \"Model Application\"\n      )),\n    qrp_coded = forcats::as_factor(qrp_coded) |&gt;\n      forcats::fct_relevel(c(\n        \"Sample Curation\",\n        \"Model Fishing\",\n        \"Selective Reporting\",\n        \"S-hacking\",\n        \"Overhyping\",\n        \"Poor Practice\"\n      ))\n  ) %&gt;%\n  arrange(model_phase, qrp_coded) %&gt;%\n  ggplot(aes(fill = practice_target, values = values)) +\n  geom_waffle(\n    size = 2,\n    color = \"white\",\n    make_proportional = FALSE,\n    flip = TRUE\n  ) +\n  scale_x_discrete(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  scale_fill_manual(\n    name = \"Target Model Object\",\n    values = c(\n      \"input\" = \"#0F9ED5\", # blue\n      \"model\" = \"#A02B93\", # purple\n      \"output\" = \"#4EA72E\"\n    ), # green\n    labels = c(\n      \"input\" = \"Model Input\",\n      \"model\" = \"Model\",\n      \"output\" = \"Model Output\"\n    )\n  ) +\n  # ggsci::scale_fill_futurama(name = \"QRP Target\") +\n  coord_equal() +\n  facet_nested(\n    qrp_coded + qrp_description ~ model_phase,\n    switch = \"y\",\n    labeller = labeller(\n      qrp_description = label_wrap_gen(width = 80, multi_line = TRUE),\n      qrp_coded = label_wrap_gen(width = 10, multi_line = TRUE),\n      model_phase = label_wrap_gen(width = 10, multi_line = TRUE)\n    ),\n    nest_line = element_line(linetype = 1),\n    solo_line = TRUE,\n    space = \"free\",\n    strip = strip_nested(size = \"variable\", clip = \"off\")\n  ) +\n  # facet_grid(qrp_description ~ model_phase,\n  #            switch = \"y\",\n  #            labeller = labeller(qrp_description =\n  #                                        label_wrap_gen(width = 100))) +\n  # theme_no_axes() +\n  hrbrthemes::theme_ipsum_rc() +\n  waffle::theme_enhance_waffle() +\n  theme(\n    strip.text.y.left = element_text(angle = 0, size = 16),\n    strip.text.x = element_text(size = 18, vjust = 0),\n    strip.background.x = element_part_rect(side = \"b\"),\n    strip.background.y = element_blank(),\n    legend.text = element_text(size = 12),\n    legend.title = element_text(size = 14)\n    # legend.position = \"bottom\"\n  ) +\n  ggh4x::force_panelsizes(rows = unit(1.1, \"cm\"), cols = unit(3, \"cm\")) +\n  guides(fill = guide_legend(\"QRP Target:\", position = \"bottom\"))\nwaffle_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Synthesis of questionable research practices (QRPs) in ecological modelling. QRPs may target model inputs (blue squares), the model itself (purple squares), and/or model outputs (green squares), and may occur at different phases in the modelling cycle. QRPs are grouped according to broader classes defined in Table 3. See Table 4 for the full list of QRPs identified.\n\n\n\n\n\nSelective reporting involves failure to disclose methods and/or results. Selective reporting can be distinguished from other practices, such as S-hacking and model fishing, in that it lends unwarranted credibility to the model, but the model and model outputs remain unaffected. Instead of analytic decisions being data-dependent, the communication of those results is data-dependent. The ‘garden of forking paths’ is not altered by selective reporting but rather is not fully transparent.\n\n\n\nWe expanded the concept of p-hacking and termed it ‘S-hacking’, or ‘statistic hacking’, which encompasses analogous practices in modelling that target metrics that contribute to the publishability of a model. S-hacking involves an element of selective reporting, but a critical point of difference is that S-hacking includes the execution of alternative analyses and manipulation of data, models, or outputs to obtain a favourable result. For example, a modeller may systematically trial multiple different evaluation metrics, selectively reporting only those that present the model in a favourable light (Hildebrandt 2018). In this instance the model remains unaffected by S-hacking. Alternatively, random seeds in model tuning can be changed after observing test set performance which can drastically alter model results (Liu, Althoff, and Heer 2020). If S-hacking is performed during model construction or validation, or alternative model specifications are trialled after observing model performance results, the model itself is altered, and overfitted to the training data. If S-hacking is performed during model construction or validation, the model is overfitted to the training data and poorly generalises to new data. S-hacking artificially inflates model performance, resulting in spuriously selected models that that may not reflect genuine ecological or predictive relationships. Any performance metric with a threshold dependent outcome (e.g. AUC, TSS, partial ROC, sensitivity, specificity, Feng et al. 2019) will be subject to the same types of practices as p-hacking.\n\n\n\nWe distinguish ‘model fishing’ from the methodological technique of ‘model dredging’ for the purposes of exploration or model selection. In the case of formal model selection procedures employing dredging, there is some a priori chosen objective criteria on which the model is selected, and the model space (usually, though not always) is constrained by a priori specification of candidate models that are theoretically or ecologically motivated. In contrast, when conducting model dredging for the purposes of exploration in pursuit of generating new hypotheses, the initial model space may not be as constrained, but the dredging procedure is transparently reported, and the exploratory nature of the modelling exercise is disclosed and not mispresented post-hoc as otherwise. Model fishing occurs when the dredging procedure is not disclosed, and/or there is no formal criterion for model selection, and the overarching purpose is not exploration. Alternatively, model fishing can occur without dredging through a large model space, but by conducting alternative analyses or new model variations and selectively reporting only those with favourable results. Model fishing is problematic because of the risk of cognitive biases, such as hindsight bias, where post-hoc rationalisation combined with haphazard model selection leads to spuriously selected models. Model fishing therefore involves an element of systematic exploration of researcher degrees of freedom that is not necessarily planned, nor transparent.\n\n\n\nSample curation (sensu Nagy et al. 2025) includes a range of data-dependent decisions about model inputs without justification or prior planning, i.e. after model fitting or observing model evaluation or application results. Sample curation may include removing observations in order to make a correlation of interest become significant and generating a data-dependent criterion for the exclusion of particular observations (Nagy et al. 2025). Opportunistic handling of missing data could occur in a number of ways, for instance when a researcher attempts list-wise deletion, multiple imputation or inverse probability weighting. The expected results may only appear with one of those options, which is problematic if the researcher only reports this strategy in the paper, and omits the results from the other data handling methods (Nagy et al. 2025). Similarly, opportunistic stopping occurs when new data is collected and is used to re-parameterise the model after previously observing model validation and model evaluation results, without reporting results of earlier iterations (Table 3, Table 4).\n\n\n\nAlthough the overarching purpose of ecological modelling in applied contexts is not hypothesis-testing, it is important to acknowledge that ecological models implicitly encapsulate hypotheses in the form of assumptions about which patterns, relationships, or predictors are most relevant to the system being modelled (Bodner, Fortin, and Molnár 2020; Prosperi et al. 2019; Schuwirth et al. 2019). For example, the choice of which variables to include or exclude from a model are based on implicit hypotheses about which processes are relevant to the system. In the case of modelling, HARKing can occur when a researcher presents a post-hoc explanation and justification for the variables or model structure that performed best, while failing to disclose the initial exploration of other variables or model structures. As such, HARKing in ecological modelling for purposes other than hypothesis testing is likely to occur as an effect of other related QRPs (Table 3) rather than as the motivating practice.\n\n\n\nOverhyping involves claims about the models’ performance that are not substantiated by model evaluation results, such as claiming the model has greater generalisability than it does (Corneille et al. 2023). A specific form of overhyping involves misreporting correlative claims using causal language, which is particularly common in studies evaluating conservation interventions using observational study designs (Josefsson et al. 2020). The practice of implying causation from correlation can cause false confidence in the intervention’s effectiveness while ignoring the real mechanisms for the observed effect.\n\nBox 1: Synthetic Example Of Questionable Research Practices In Applied Ecological Modelling\nA modeller seeks to predict species abundance on the basis of habitat quality to help inform conservation management (Figure 3). When the modeller evaluates how two different management actions affecting habitat quality influence species abundance, the initial a priori model does not provide sufficient certainty for choosing between one action and another (Panel A, Stage 1). The modeller revises the model without theoretical justification, instead opportunistically trialling different models and selecting the one with the best Performance Score (Panel B). On checking the predicted species abundance for the two actions on the overfitted model, the modeller finds that the actions are still not clearly distinguishable in terms of their predicted outcomes (Panel A, Stage 2), so the modeller adjusts the scenario input values for the two management actions, and plots the predicted outcomes (Panel B, Stage 3). They are sufficiently happy that the model now clearly supports their preferred management action B and proceed to publish the overfitted model, its predictions and management recommendations without ever disclosing their model fishing and scenario hacking, effectively a form of HARKing (‘hypothesising after results are known’). The impacts of the modeller’s actions are summarised in Panel C.\n\n\n\n\nCode\nmagick::image_read_pdf(here::here(\"figures/synthetic_example_QRPs.pdf\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: A synthetic illustration of model fishing & scenario hacking (HARKing). A. Violin boxplots of predicted species abundance for two management actions from two models constructed at different stages of the modelling process: a priori model (stage 1), a model generated from a model fishing exercise (stage 2), and the same model, but illustrating scenario hacking (stage 3). Dots are predicted values. Violin outlines illustrate kernel density probability distributions, where the width of the shaded area represents the corresponding proportion of data. The model-estimated median and quartiles are displayed for each action. Colours correspond to the scenario actions displayed in Panel C. B. Performance Scores calculated from multiple model performance measures for the Initial Model and a new, superior Overfitted Model derived from model fishing (greater overall Performance Score). See Lüdecke et al. (2020) for metric calculation details. C. Predicted species abundance as a function of habitat quality for the Initial Model (yellow line) and the Overfitted Model generated from the model fishing exercise (dashed orange line). The management action scenarios used in the first two stages of modelling are shown as solid light blue and light green lines. Scenario hacking occurs when the modeller selects two new management scenarios with a greater difference in mean predicted species abundance under the Overfitted Model. See Appendix 1.7 for code."
  },
  {
    "objectID": "ms/QRPs in Ecological Modelling.html#sec-QRP-formalism",
    "href": "ms/QRPs in Ecological Modelling.html#sec-QRP-formalism",
    "title": "A Framework for Questionable Research Practices in Ecological Modelling",
    "section": "",
    "text": "In this section, I extend Gelman and Loken’s (2013) mathematical formalism explaining the emergence of QRPs, or the “garden of forking paths” to model-based research. Outlining the mathematical formulation of QRPs for ecological modelling helps to formally differentiate defensible and questionable data-dependent decisions.\nAs emphasised above, I hesitate to designate all data-dependent analytic decisions as questionable, as is implied in the prevailing literature on preregistration and QRPs. There are situations in modelling where decisions are necessarily dependent on the outcome of previous analytic decisions within the modelling workflow (Liu, Althoff, and Heer 2020), and so not all data-dependent analytic decisions are automatically questionable within the context of modelling. For example, many modelling decisions are data-driven, like the choice to remove correlated variables or checking for distributional assumptions to aid in deciding the most appropriate model functional form (See Gould et al. (2025), Figure 4for an example from the case study). Liu, Althoff, and Heer (2020) distinguish defensible from questionable motivations for engaging in data-dependent analytic decision-making, by classifying them as either systematic or opportunistic, respectively.\nBox 2 helps us to formally distinguish between defensible and opportunistic data-dependent decisions, for both analytic and reporting decisions. This, in turn, helps to identify and distinguish between different types of QRPs in model-based research.\n\nBox 2: Formal Description of Questionable Research Practices\nThe modeller makes a series of analytic decisions to derive \\(M_p\\) from \\(M_s\\), from \\(M_c\\), referred to hereafter as \\(M\\) for simplicity (see Section 1.1.2 for notation definitions). We term the sequence of modelling choices throughout the modelling process the realised “modelling path.” Analytic uncertainty, or analogously ‘researcher degrees of freedom,’ propagates combinatorially along each decision-point to inform a multiplicity of plausible analysis strategies (Hoffmann et al. 2021) constituting the “garden of forking paths” (Gelman and Loken 2013). Consider that a modeller faces some decision \\(C\\) along that path about a modelling task concerning model \\(M\\) and some observed data \\(\\symbf{X}\\), with a predetermined choice or decision rule \\(\\phi\\). Decisions made before observing data or model outputs reflect idealised practice where choices are predetermined and independent of results \\(C(\\mathbf{X}, M; \\phi)\\). When the modelling choice is “data-contingent” \\(\\phi(\\symbf{X},M, M_j)\\) insofar as it hinges on the observed state of the model(s) \\(M\\) and/or any associated outputs \\(M_j\\) and data \\(\\symbf{X}\\) at that point along the modelling path, it satisfies a broad definition of ‘questionable.’\nI define defensible data-dependent decisions \\(\\phi_D\\) as following a systematic process \\(\\Psi_{\\text{systematic}}\\):\n\\[\n\\begin{aligned}\n\\phi_D(\\mathbf{X}, M; \\Omega, \\Psi_{\\text{systematic}}) = g(\\mathbf{X}, M, \\Omega)\n\\end{aligned}\n\\tag{1}\\]\nWhere \\(g(x)\\) is a deterministic function of the data, model state, and predefined decision-rule \\(\\Omega\\), based on systematic objectives such as, model adequacy, predictive accuracy or theoretical consistency.\nIn contrast, we define questionable practices \\(\\phi_Q\\) as:\n\\[\n\\begin{aligned}\n\\phi_Q(\\mathbf{X}, M; \\Psi_{\\text{opportunistic}}) = \\phi^* \\\\\n& \\text{ such that } h(\\phi^* | \\mathbf{X}, M, R^*) \\geq h(\\phi | \\mathbf{X}, M, R^*) \\\\\n&\\text{ for all } \\phi \\in \\Phi\n\\end{aligned}\n\\tag{2}\\]\nWhere the decision-making is opportunistic and result-seeking \\(\\Psi_{\\text{opportunistic}}\\), and \\(h(\\phi | \\mathbf{X}, M, R^*)\\) represents how well decision \\(\\phi\\) serves the desired outcome \\(R^*\\). Data-contingent decisions are therefore questionable when a researcher’s drive to make their research publishable influences the direction that the realised modelling path takes.\nA defining aspect of QRPs is that they remain undisclosed. Just as decisions about the modelling process can be questionable, so too can reporting practices. We apply the same logic describing questionable modelling practices to reporting practices:\n\\[\n\\begin{aligned}C_{\\text{Reported}} = S({C(\\mathbf{X}, M, \\phi)})\\end{aligned}\n\\tag{3}\\]\nWhere \\(S\\) is a selecting function that determines what to report from a set of conducted analyses.\nJournal method or article length conventions restrict complete transparency, and not all results can be reported. The decision about what to report from a set of conducted analyses \\(S\\) is made following predetermined plans \\(\\Omega\\), not contingent on observed results \\(S_{\\text{pre}}\\):\n\\[\n\\begin{aligned}S_{\\text{pre}}({C(\\mathbf{X}, M, \\phi)}) = s({C(\\symbf{X}, M, \\phi)},\\Omega)\\end{aligned}\n\\tag{4}\\]\nQuestionable reporting \\(S_Q\\), or as it is more commonly known, selective reporting, occurs when the reporting is opportunistic and contingent on the observed results, and optimised for desired outcomes \\(R^*\\):\n\\[\n\\begin{aligned}S_Q({C(\\mathbf{X}, M, \\phi)}; \\Psi_{\\text{opportunistic}}) = C^* \\\\ & \\text{ such that } h(C^*|R^*) \\geq h(C|R^*) \\\\\n& \\text{ for all } C \\in {C(\\mathbf{X}, M, \\phi)}\\end{aligned}\n\\tag{5}\\]\nI avoid defining \\(\\Psi_{\\text{systematic}}\\) reporting decisions, as we did for modelling decisions, and instead advocate modellers prespecify what results will be reported."
  },
  {
    "objectID": "ms/QRPs in Ecological Modelling.html#a-typology-of-qrps",
    "href": "ms/QRPs in Ecological Modelling.html#a-typology-of-qrps",
    "title": "A Framework for Questionable Research Practices in Ecological Modelling",
    "section": "",
    "text": "Here, I present a typology of QRPs from which we designate the practices as questionable or defensible (Table 2). The typology considers combinations of the decision-making mechanism (a priori, defensible and questionable data-dependent decision-making), the target \\(T\\) of the practice (the model \\(M\\) or model outputs \\(M_j\\)), and the nature of reporting (prespecified or selective). This allows us to account for QRPs where the model and/or model outputs are directly affected by the questionable practice such that their realisations are different from what would have been observed if the practice was not undertaken (i.e. \\(M^*\\), \\(M_j^*\\)), as well as QRPs where the model and outputs remain unaffected, but are selectively reported.\nTable 2 summarises the key distinctions resulting from the workings in Box 2, helping to distinguish between questionable and defensible data-dependent decision-making, and to identify different classes of QRPs, with which we provide formal examples. The rows in Table 2 discriminate between a priori and (defensible versus questionable) data-dependent decisions, while the columns distinguish between prespecified and selective reporting.\n\n\nCode\n##| results: asis #equations do not render otherwise\n##| html-table-processing: none #equations do not render otherwise\nmagick::image_read(here::here(\"tables\", \"QRP_typology.png\")) |&gt;\n  magick::image_scale(geometry = \"70%\")\n\n\n\n\nTable 2: Distinguishing between questionable and defensible motivations for decision making during modelling, and the target of the decisions. We designate questionable practices in grey shaded cells and assign practices to classes of QRPs (described in Table 3). See Box 2 for notation and expanded definitions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo questionable practices occur when analytic and reporting decisions are made a priori (first row, first column) representing an idealised scenario (e.g. preregistered analyses), which is difficult to implement in practice for complex ecological modelling. Moving across to the right, all decisions about the modelling and analysis are made a priori, but the results are selectively reported to improve the apparent suitability of the model to the analysis problem (questionable).\nThe subsequent row represents situations where there are data-dependent choices made by the modeller, representing most situations in ecological modelling. In the first instance, there is some process stipulated a priori for deciding on how the modeller will resolve any data-dependent decisions (e.g. ‘registered flexibility,’ Gould et al. (2025)), and it is already decided what results will be reported (defensible). Moving to the next column over, the modeller uses registered flexibility to inform modelling choices, but in this case, they selectively report some results (questionable). In the first two rows, choices about the model and modelling analysis remain unaffected by QRPs, even when selective reporting occurs.\nThe final row of Table 2 indicates QRPs where data-contingent decisions are optimised for preferred results \\(R^*\\), affecting either the model and/or modelling outputs. Because QRPs involve a degree of non-disclosure and intransparency by definition (Definition 1), we have merged the two columns that distinguish between the presence of selective reporting. In Box 1 we formally illustrate this with two example QRPs, model fishing (affecting the model \\(M\\) itself) and scenario hacking (affecting the model outputs \\(M_j\\) only).\n\n\n\n\n\n\n\nDefinition 1 QRPs occur when a researcher makes opportunistic data-dependent analytic and/or reporting decisions; i.e., decisions that depend on an undisclosed desired outcome rather than a prespecified objective decision criterion, and which artificially inflate the apparent, accuracy, precision or performance of a model and/or its outputs, such that the model is perceived to be more publishable than it would be if the QRP had not occurred.\n\n\n\n\nIn Table 3, we present formal descriptions and practical examples of the different types of QRP in the typology. While some example scenarios may appear to be defensible data-dependent decisions (for example, testing different functional forms under ‘model fishing’), we include corresponding formal descriptions to remind the reader of the distinction between systematic and opportunistic analytic decision-making which denotes when data-contingent decisions are questionable, or not. We elaborate on these practices in the QRP map above (Section 1.2).\nIn summary, based on the mathematical formalism and typology, a clearer definition of QRPs in model-based research is apparent.\n\nCode\nlibrary(gt)\nlibrary(tidyverse)\nlibrary(gtExtras)\nlibrary(gluedown)\n\nQRP_definition_table &lt;-\n        readr::read_delim(\n                file = here::here(\"ms/Table_3.md\"),\n                delim = \"|\",\n                trim_ws = TRUE,\n                col_select = c(\n                        class,\n                        definition,\n                        example_scenarios,\n                        description,\n                        formal_description\n                ),\n                show_col_types = FALSE,\n                skip_empty_rows = TRUE\n        ) %&gt;%\n        slice(-1) %&gt;%\n        group_by(class, definition) %&gt;%\n        reframe(\n                example_scenarios = str_split(example_scenarios, \"- \"),\n                description, formal_description\n        ) %&gt;%\n        rowwise() %&gt;%\n        mutate(\n                example_scenarios = list(str_subset(example_scenarios, \".+\") %&gt;%\n                        gluedown::md_bullet() %&gt;%\n                        str_flatten(\"\\n\")),\n                class = trimws(class),\n                class = factor(class, levels = c(\"**Selective Reporting** Choosing to report only certain analyses, models, metrics, model results or comparisons that yield favourable results or desired conclusions without disclosing the full range of analyses performed.\", \"**Model Fishing**\", \"**S-hacking** Statistic-hacking\", \"**Sample Curation**\", \"**HARKing** Hypothesising After Results are Known\", \"**Overhyping**\"))\n        ) %&gt;%\n        unnest(example_scenarios) %&gt;%\n        arrange(class) %&gt;%\n        gt(\n                rowname_col = c(\"definition\"),\n                groupname_col = \"class\",\n                # row_group_as_column = TRUE,\n                process_md = TRUE,\n        ) %&gt;%\n        gt::fmt_markdown(columns = (everything())) %&gt;%\n        gt::cols_merge(\n                columns = c(\"class\", \"definition\"),\n                rows = 1\n        ) %&gt;%\n        gt::cols_label(\n                example_scenarios = \"Example Scenarios\",\n                description = \"Description\",\n                formal_description = \"Formal Description\"\n        ) %&gt;%\n        gt::cols_align(\"right\", formal_description) %&gt;%\n        gt::tab_stubhead(label = \"QRP Class and Definition\") %&gt;%\n        gtExtras::gt_theme_538(quiet = TRUE) %&gt;%\n        gt::tab_style(\n                style = cell_text(v_align = \"top\"),\n                locations = cells_body(columns = everything())\n        ) %&gt;%\n        gt::tab_style(\n                style = cell_text(v_align = \"top\", style = \"italic\"),\n                locations = cells_stub()\n        ) %&gt;%\n        gt::cols_width(example_scenarios ~ gt::px(400)) %&gt;%\n        gt::cols_merge(\n                columns = contains(\"description\"),\n                rows = 7, pattern = \"{1}\", hide_columns = FALSE\n        ) %&gt;%\n        gt::sub_missing() %&gt;%\n        opt_vertical_padding(scale = 0.1) %&gt;%\n        gt::tab_style(cell_text(weight = \"bold\", stretch = \"condensed\"), cells_column_labels()) %&gt;%\n        gt::tab_style(cell_text(weight = \"bold\", stretch = \"condensed\"), cells_stubhead())\n\nQRP_definition_table\n\n\nCode\n# gtExtras::gtsave_extra(QRP_definition_table,\n#                        path = here::here(\"tables\"),\n#                        filename =  \"QRP_definition_table.png\")\n# gt::gtsave(QRP_definition_table,\n#         path = here::here(\"tables/\"),\n#         filename = \"QRP_definition_table.html\"\n# )\n# webshot2::webshot(here::here(\"tables\", \"QRP_definition_table.html\"),\n#                   here::here(\"tables\", \"QRP_definition_table.png\"),\n#                   vheight = 1000,\n#                   vwidth = 900)\n\n\n\n\nTable 3: Formal descriptions of QRP classes, their definitions and some practical examples.\n\n\n\n\n\n\n\n\n\n\nQRP Class and Definition\nExample Scenarios\nDescription\nFormal Description\n\n\n\n\nSelective Reporting Choosing to report only certain analyses, models, metrics, model results or comparisons that yield favourable results or desired conclusions without disclosing the full range of analyses performed.\n\n\nMultiple model testing with selective reporting\nReporting only the model with best R² (testing linear, polynomial, and exponential models)Highlighting only the ecological model supporting preferred hypothesis\n\nReporting only the model with best R² (testing linear, polynomial, and exponential models)\nHighlighting only the ecological model supporting preferred hypothesis\n\n\nTesting multiple model structures \\((M_1, M_2, \\ldots, M_k)\\) and reporting only the model that produces results most align* with desired outcomes \\(R^*\\).\n\\[\\begin{align*} S_Q(\\{C(\\boldsymbol{X}, M_k, \\phi^{(M)})\\}_{k=1}^K; R^*) = C(\\boldsymbol{X}, M_{k^*}, \\phi^{(M)}) \\\\ \\text{ such that } \\\\ h(C(\\boldsymbol{X}, M_{k^*}, \\phi^{(M)}) \\vert R^*) \\geq h(C(\\boldsymbol{X}, M_k, \\phi^{(M)}) \\vert R^*) \\\\ \\text{ for all } k \\in \\{1,\\ldots,K\\} \\end{align*}\\]\n\n\nMultiple output evaluation with selective reporting\nReporting only significant goodness-of-fit metricsHighlighting only time periods showing desired trendsEmphasising model predictions supporting preferred conclusions\n\nReporting only significant goodness-of-fit metrics\nHighlighting only time periods showing desired trends\nEmphasising model predictions supporting preferred conclusions\n\n\nEvaluating multiple outputs \\(({M_j}_1, {M_j}_2, \\ldots, {M_j}_j)\\) from same model (different metrics, time periods, spatial scales, etc.) and reporting  only those that align with desired outcomes \\(R^*\\).\n\\[\\begin{align*} S_Q(\\{C(\\boldsymbol{X}, M, \\phi_j^{(M_j)})\\}_{j=1}^J; R^*) = C(\\boldsymbol{X}, M, \\phi_{j^*}^{(M_j)}) \\\\ \\text{ such that } h(C(\\boldsymbol{X}, M, \\phi_{j^*}^{(M_j)}) \\vert R^*) \\geq h(C(\\boldsymbol{X}, M, \\phi_j^{(M_j)}) \\vert R^*) \\\\ \\text{ for all } j \\in \\{1,\\ldots,J\\} \\end{align*}\\]\n\n\nModel Fishing\n\n\nExploring model specifications or (fitting) variable combinations without theoretical justification, seeking favourable results, undisclosed.\nAdopting the ecological process model that supports preferred management conclusionsChoosing model covariates giving desired effect directionSettling on model complexity producing significant results rather than optimal predictive performance\n\nAdopting the ecological process model that supports preferred management conclusions\nChoosing model covariates giving desired effect direction\nSettling on model complexity producing significant results rather than optimal predictive performance\n\n\nTesting multiple model \\((M_1, M_2, \\ldots M_k\\)) structures or specifications and selecting model based on results most align* with desired outcomes \\(R^*\\), rather than following pre-specified model selection criteria or theoretical justification.\n\\[\\begin{align*} \\phi_Q^{(M)}(\\boldsymbol{X}, M; R^*) = M_{k^*} \\text{ such that } \\\\ h(C(\\boldsymbol{X}, M_{k^*}, \\phi) \\vert R^*) \\geq h(C(\\boldsymbol{X}, M_k, \\phi) \\vert R^*) \\\\ \\text{ for all } k \\end{align*}\\]\n\n\nS-hacking Statistic-hacking\n\n\nManipulating model inputs, outputs or the model itself (random seeds, outcome variable and/or performance thresholds or metrics) to obtain a favourable value of a performance measure (statistic or metric), without disclosing.\nReporting only best performing goodness-of-fit metrics (after testing \\(R^2\\), AIC, RMSE, etc)Selecting validation approaches (cross-validation, holdout, bootstrap) based on most accurateAdjusting thresholds until desired statistical outcomes are achieved\n\nReporting only best performing goodness-of-fit metrics (after testing \\(R^2\\), AIC, RMSE, etc)\nSelecting validation approaches (cross-validation, holdout, bootstrap) based on most accurate\nAdjusting thresholds until desired statistical outcomes are achieved\n\n\nTrialling different performance metrics (\\({M_j}_1, {M_j}_2, \\ldots, {M_j}_j\\)), evaluation thresholds, or validation approaches and selecting metric with best model performance \\(R^*\\) (not pre-specified or theoretically justified).\n\\[\\begin{align*}\\phi_Q^{(Mj)}(\\boldsymbol{X}, M; R^*) = M_{j^*} \\text{ such that } \\\\ h(M_{j^*}(C(\\boldsymbol{X}, M, \\phi))  \\vert  R^*) \\geq h(M_j(C(\\boldsymbol{X}, M, \\phi))  \\vert  R^*) \\\\ \\text{ for all } j \\end{align*}\\]\n\n\nSample Curation\n\n\nSelectively including, excluding, or modifying data points in the sample used to develop or evaluate a model without disclosure, often to improve model performance (a specific instance of S-hacking).\nRemoving “inconvenient” sites that don’t fit expected patternsAdjusting temporal boundaries to exclude unfavourable periods\n\nRemoving “inconvenient” sites that don’t fit expected patterns\nAdjusting temporal boundaries to exclude unfavourable periods\n\n\nThe modeller trials different curated versions of the original dataset \\(\\boldsymbol{X}\\) in order to achieve a model that meets desired outcomes \\(R^*\\), without a priori exclusion rules or statistical/ theoretical basis.\n\\[\\begin{align*}\\phi_Q^{(\\boldsymbol{X})}(\\boldsymbol{X}, M; R^*) = \\boldsymbol{X}^* \\text{ such that } \\\\ h(C(\\boldsymbol{X}^*, M, \\phi)  \\vert  R^*) \\geq h(C(\\boldsymbol{X}', M, \\phi)  \\vert  R^*) \\\\ \\text{ for all } \\boldsymbol{X}' \\in \\mathcal{X}(\\boldsymbol{X})\\end{align*}\\]\n\n\nHARKing Hypothesising After Results are Known\n\n\nSelected model(s) and/or results presented as if pre-specified and theoretically justified before data exploration or parameterisation, and model selection procedure is not disclosed or adequately described. May accompany fishing expeditions, model dredging analyses and model selection procedures that have been selectively reported.\nPost-hoc ecological theories used to justify variable inclusion, when exploratory analyses reveals unexpected predictive variablesTheoretical explanation created post-hoc for model structures that unexpectedly perform well against other models\n\nPost-hoc ecological theories used to justify variable inclusion, when exploratory analyses reveals unexpected predictive variables\nTheoretical explanation created post-hoc for model structures that unexpectedly perform well against other models\n\n\nPost-hoc conceptual models are constructed to explain model results, serving desired outcomes \\(R^*\\).\n\\[\\begin{align*}H_Q(\\boldsymbol{X}, M, C; R^*) = H^* \\text{ such that } \\\\ \\text{consistency}(H^*, C(\\boldsymbol{X}, M, \\phi)) \\geq \\text{consistency}(H', C(\\boldsymbol{X}, M, \\phi)) \\\\ \\text{ for all } H' \\in \\mathcal{H}\\end{align*}\\] , where: \\(M_c^∗\\) represents the post-hoc conceptual model, \\(M_c\\) is the space of possible conceptual models, consistency() measures how well a conceptual model explains the observed results\n\n\nOverhyping\n\n\nExaggerating the capabilities, generalisability, or reliability of a model beyond what is justified by the evidence.\nTypically, overhyping features in the discussion section of a paper\n\nTypically, overhyping features in the discussion section of a paper\n\n\n—\n—"
  },
  {
    "objectID": "ms/QRPs in Ecological Modelling.html#discussion",
    "href": "ms/QRPs in Ecological Modelling.html#discussion",
    "title": "A Framework for Questionable Research Practices in Ecological Modelling",
    "section": "",
    "text": "Researcher degrees of freedom threaten the credibility and reliability of model-based research, just as they do in hypothesis testing research. The findings of this Chapter underscore that researcher degrees of freedom abound in the modelling process, providing ample opportunity for QRPs that accompany researchers’ drive to publish. This aligns with Liu et al.’s (2020) qualitative analysis of how researchers make analytic decisions when faced with arbitrary choices or analytic uncertainty in the context of a research culture that promotes publication bias. We showed that QRPs can occur at any point in the modelling process, and may affect different model objects, including the model inputs, the model itself, or model outputs. While our analysis identified that classes of QRPs are analogous to those in hypothesis-testing research, we also showed that there are unique aspects of methodological practices in ecological modelling that mean we need to define ‘questionable’ research practices in our own terms – namely, in terms that accommodate the iterative and adaptive nature of the modelling process and the need to make data-contingent decisions when modelling. These features of model development have resulted in significant resistance to the idea of QRPs in model-based research, and to the applicability of preregistration for mitigating them in ecological modelling (MacEachern and Van Zandt 2019; Dwork et al. 2015). We explore these tensions below, emphasising how the conceptual framework provides a way forward through the tricky problem of delineating where the concept of QRPs apply in ecological modelling, and where it does not.\n\n\nMany of the practices I identified as ‘questionable’ could simply be considered ‘poor practice,’ especially when those practices result in biased or overfitted models. The modelling context, including constraints on feasibility, data availability and coverage, together with the model purpose (e.g. prediction versus inference) will delineate when such practices are questionable or methodologically flawed. The fundamental issue with QRPs is that they remain undisclosed. Given that QRPs are practices that artificially improve the way models are perceived, full transparency allows the reader to appraise the appropriateness of practices like altering data, changing model specifications, or calculating additional performance metrics, contingent on the modelling context (Woo, O’Boyle, and Spector 2017). Transparent reporting is essential for properly evaluating the credibility and suitability of the model for its intended application.\n\n\n\nGelman and Loken’s (2013) formalism describing the garden of forking paths implies that data-dependent decisions, at least in the context of null hypothesis significance testing, are inherently questionable. This has limited relevance in ecological modelling because it is inherently adaptive. My extension of the formalism to model-based research circumvents this incompatibility by distinguishing opportunistic from systematic data-dependent decisions. Based on our formalism, I argue that data-driven analysis decisions are not inherently questionable. Questionable practices occur when the decision is contingent on the observed results, and the choice is based on how well it serves undisclosed desired outcomes, whereas defensible data-contingent decisions follow a prespecified decision rule.\n\n\n\nThe nature of ecological data confers specific vulnerabilities to QRPs during ecological modelling. Small datasets are prevalent in ecology and often have inconsistent structure due to being collected intermittently or on a one-off occasion (Todman, Bush, and Hood 2023), or there are spatial constraints. Additionally, data collection is highly constrained by budget and logistical feasibility, consequently field ecologists often take a ‘kitchen sink’ approach to data collection, whereby they “often measure almost everything they can” (Mac Nally 2000, 669). Models that analyse small datasets are more likely to be overfitted due to the high number of parameters compared to the degrees of freedom in the data (Todman, Bush, and Hood 2023).\nThese conditions provide substantial opportunity for unconstrained dredging of model space whereby modellers include covariates with little or no theoretical justification or ecological relevance, leading to biologically implausible models being considered (Fourcade, Besnard, and Secondi 2018; Shmueli 2010; Franks, Ruxton, and Sherratt 2025). Although I classified these “causal salad” approaches to modelling (McElreath 2020) as ‘poor practice’ instead of QRPs, when modellers are engaged in model dredging without predetermined selection rules and criteria, the risk of both poor and questionable practices, like model fishing, is heightened under these conditions.\nThe same data constraints that facilitate model dredging also inhibit the detection of resulting problems. When datasets are small or incomplete, there is often insufficient data to perform model evaluation on independent data (Bodner, Fortin, and Molnár 2020; Dietze et al. 2018; Wood et al. 2020). When models are evaluated on training data only, data leakage causes biased estimates of performance making overfitting hard to detect (Lewis et al. 2023; Stock, Gregr, and Chan 2023; Christin, Hervet, and Lecomte 2020; Kapoor and Narayanan 2023).\n\n\n\nMany of the questionable practices I identified – particularly overhyping claims and misreporting correlative findings with causal language – specifically target perceptions of model reliability, accuracy, and generalisability rather than traditional statistical thresholds. This supports a model-centric definition of QRPs as practices that “artificially inflate the apparent accuracy or precision of a model, its predictions, and/or evaluation tests.” The mathematical formalism in the typology demonstrates the diverse ways researchers can manipulate both technical model properties and broader perceptions of model fitness for purpose, providing a comprehensive framework for understanding questionable practices in model-based research.\nQRPs collectively undermine the reliability and reproducibility of ecological modelling research in several ways:\n\nInflated performance estimates that do not reflect true performance and result in overfitting.\nSpurious model selection that identifies models based on chance rather than plausible biological mechanisms or predictive relationships.\nReduced reproducibility due to undisclosed researcher degrees of freedom.\nCompromised generalisability from overfitted models that fail to transfer to new contexts or make accurate forecasts.\nFalse confidence in ecological understanding and management recommendations.\n\nOverfitted models are fitted to both regular and irregular features of the sampled data but are unable to distinguish between them (Pu et al. 2019), generating spurious predictions that poorly generalise to new data (Todman, Bush, and Hood 2023; Lewis et al. 2023). In applied ecological modelling, where modelling is often focused on generating anticipatory predictions to inform management or policy decisions, this is particularly problematic.\nThe prevalence of these practices suggests systemic issues in training, incentives, and quality control within the ecological modelling community. The concentration of QRPs in the model construction and evaluation phases indicates particular vulnerabilities in how models are specified, fitted, and evaluated. The large number of decision-points where researchers can exercise degrees of freedom suggest that safeguards should target these critical phases of the modelling process.\n\n\n\n\n\nAwareness of the distinction between systematic and opportunistic data-dependent decisions is limited, and because some data-dependent decisions are a legitimate aspect of the modelling process, it may seem that all data-dependent decisions are acceptable. The impact of data-dependent decisions in machine learning is increasingly understood and is encapsulated within the term ‘data leakage,’ whereas in ecological modelling more broadly, the equivalent problem of ‘model selection bias’ remains mostly overlooked (Campbell 2021) and underappreciated within applied research contexts (Arnqvist 2020). Here, I emphasise an intersecting problem that has been attributed as a major cause of science’s reproducibility crisis, where data-contingent decisions may be opportunistically exploited to increase the likelihood of publication. I have formalised the distinction between defensible or questionable research practices, facilitating a modelling-appropriate conceptualisation of QRPs. As a first step in addressing the threat of QRPs to the credibility of model-based research, I wish to draw attention to this distinction – and the possible consequences of QRPs – among the ecological modelling community. However, given that cognitive biases are rarely deliberate, awareness alone is insufficient for preventing QRPs within a publish-or-perish research culture (Zvereva and Kozlov 2021).\n\n\n\nModelling is not typically transparent, leaving readers unable to assess whether appropriate models were used or to identify the primary research contribution (Arnqvist 2020). Given that the threat of QRPs largely stems from a lack of disclosure, ecological modelling is at significant risk of QRPs. I echo broader calls for improving transparency in ecology (Parker et al. 2016; Powers and Hampton 2018; Rose E. O’Dea et al. 2021), emphasising that transparency is a fundamental requirement for reducing the risk of QRPs in ecological modelling. It is acknowledged that modelling’s lack of transparency is, in large part, driven by sociocultural and institutional norms that restrict the length of a paper, require a neat and coherent narrative and favour some data analysis techniques and results over others (Rijnhart et al. 2021). Broader methodological reform in research culture, as well as specific tools, are needed to achieve improvements in transparency.\nReporting checklists and guidelines outline a minimum set of methodological elements and results to include in published research and are increasingly being adopted by journals in ecology (Nature 2018; Fidler et al. 2018; Hillebrand and Gurevitch 2013; Haddaway et al. 2018; R. E. O’Dea et al. 2021). However, there are only a handful of reporting checklists developed for ecological modelling, and no ecology journals have encouraged or mandated modelling-specific checklists at the time of writing. I leave the work of defining the content of reporting checklists up to the ecological modelling community, but reiterate repeated calls in the modelling community to articulate the model’s purpose, context and performance criteria, ideally before modelling begins (Wood et al. 2020; Bennett et al. 2013; Jakeman, Letcher, and Norton 2006). This chapter illustrates when practices are questionable, and that many QRPs target or alter model performance metrics – either through direct manipulation of the model and model outputs, or through selective reporting. Specifying these decisions a priori and reporting them reduces inadvertent engagement in QRPs and equips readers to evaluate the risk of QRPs.\n\n\n\nPreregistration, and registered reports, have been hailed as a solution for preventing QRPs, and recent metaresearch empirically supports its efficacy (Burgman et al. 2023; Purgar et al. 2024; Nakagawa et al. 2025). However, there has been substantial resistance in model-related fields (MacEachern and Van Zandt 2019; Dwork et al. 2015) because preregistration is geared towards a NHST-focused definition of QRPs, that is, data-dependent analytic decisions. I argue here that there is a distinction to be made between systematic and opportunistic data-dependent analytic decisions in ecological modelling, where only the latter are questionable. For preregistration to be applied to ecological modelling, its internal logic must reflect alternative conceptualisations of QRPs that accommodate legitimate data-contingent decisions and iteration. It should allow for model revision while avoiding premature commitment to one approach (Hämäläinen and Lahtinen 2016; Benning et al. 2019; Evans et al. 2023). In Gould et al. (2025), I develop, apply and evaluate Adaptive Preregistration as a potential solution. If a complete preregistration is impractical, then at the very least, specifying a minimum set of evaluation analyses, metrics, and their performance criteria a priori is essential for avoiding QRPs.\n\n\n\n\nThis paper has gone some way towards characterising QRPs to accommodate a diversity of modelling types within ecology. Further research could provide a deeper understanding of where and when these are applicable across ecology (or not). For example, looking across subfields, methodological approaches or model purposes: Do some QRPs pose more of a threat to reliability than others? Are some more likely than others? Are there specific forms they take? Preventative measures can then be tailored to particular use-cases.\nThis list of QRPs is not exhaustive, future research could also characterise additional QRPs not described here, perhaps turning to other fields utilising model-based research to understand where questionable practices are more widely appreciated, such as Machine Learning (Hildebrandt 2018; McDermott et al. 2021; Stock, Gregr, and Chan 2023; Garbin and Marques 2022; Rosenblatt et al. 2024; Hosseini et al. 2020; Meding and Hagendorff 2024). Further, understanding the prevalence of QRPs in ecological modelling would give an idea of the extent of the problem in the published literature and help prioritise potential reforms. Self-report surveys (e.g. Fraser et al. 2018) using our modelling-specific QRP classification would be a useful starting point. Empirical approaches to detecting the extent of QRPs might include approaches similar to p-curve analysis but investigating relevant model performance metrics (White et al. 2023).\n\n\n\nIn this paper, I aim to raise awareness among ecological modellers, and modellers among other scientific disciplines, about potential types of QRPs and their mechanisms for emergence in the modelling process. This is the first attempt to articulate how questionable research practices occur outside hypothesis testing research. The application is specific to ecological modelling, but the definition of QRPs presented here provides insights for modelling in other fields and other forms of non-hypothesis testing research. The conceptual framework and map of QRPs in this paper helps modellers understand the risks of QRPs in their research, so they are empowered to implement procedures that can mitigate their occurrence in their own research practice. Finally, meta-researchers and advocates of open-science can use the conceptual framework to underpin the design of modelling-appropriate methodological reforms that improve the credibility and robustness of model-based research in ecology and other fields."
  },
  {
    "objectID": "ms/QRPs in Ecological Modelling.html#sec-appendix-2",
    "href": "ms/QRPs in Ecological Modelling.html#sec-appendix-2",
    "title": "A Framework for Questionable Research Practices in Ecological Modelling",
    "section": "1.6 QRP Literature Review",
    "text": "1.6 QRP Literature Review"
  },
  {
    "objectID": "ms/QRPs in Ecological Modelling.html#sec-QRP-app-synth-code",
    "href": "ms/QRPs in Ecological Modelling.html#sec-QRP-app-synth-code",
    "title": "A Framework for Questionable Research Practices in Ecological Modelling",
    "section": "1.7 Synthetic Example Code",
    "text": "1.7 Synthetic Example Code\n\n\n\n\n\n\nListing 1: Code used to generate synthetic worked example in Figure 3.\n\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(patchwork)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(grDevices)\nlibrary(marquee)\n\n# ---- Simulate Data ----\n\n# Generate synthetic data\nset.seed(123)\nn &lt;- 100\nhabitat_quality &lt;- runif(n, 0, 10)\n# True relationship with some noise\nabundance &lt;- 2 + 1.5 * habitat_quality + rnorm(n, 0, 2)\n\ndata &lt;- tibble(\n  habitat_quality = habitat_quality,\n  abundance = abundance\n)\n\n# Define management scenarios & expected values under each scenario\nmanagement_scenarios &lt;- tibble(\n  action = c(\"Action A\", \"Action B\"),\n  habitat_quality_mean = c(6.0, 6.5), # Small difference initially\n  habitat_quality_sd = c(0.5, 0.5)\n)\n\nhabitat_values &lt;- management_scenarios |&gt;\n  rowwise() |&gt;\n  mutate(\n    habitat_values = list(rnorm(1000, habitat_quality_mean, habitat_quality_sd))\n  )\n\n# Stage 1: Initial simple model (defensible)\nmodel_initial &lt;- lm(abundance ~ habitat_quality, data = data)\n\n# Predict for management scenarios\npred_initial &lt;- habitat_values |&gt;\n  rowwise() |&gt;\n  mutate(\n    predictions = list(\n      predict(\n        model_initial,\n        newdata = tibble(habitat_quality = habitat_values),\n        interval = \"prediction\"\n      )\n    ),\n    pred_mean = mean(predictions[, 1]),\n    pred_lower = quantile(predictions[, 1], 0.025),\n    pred_upper = quantile(predictions[, 1], 0.975),\n    predictions = list(predictions[, 1]),\n    model = \"Initial Model\",\n    stage = \"Stage 1: A Priori Model\"\n  )\n\n# Stage 2: Overfitted model (model fishing)\n# Add polynomial and interaction terms to artificially reduce uncertainty\nmodel_overfitted &lt;- lm(\n  abundance ~ poly(habitat_quality, 3) +\n    I(habitat_quality^2 * (habitat_quality &gt; 5)),\n  data = data\n)\n\npred_overfitted &lt;- habitat_values |&gt;\n  rowwise() |&gt;\n  mutate(\n    predictions = list(\n      predict(\n        model_overfitted,\n        newdata = tibble(habitat_quality = habitat_values),\n        interval = \"prediction\"\n      )\n    ),\n    pred_mean = mean(predictions[, 1]),\n    pred_lower = quantile(predictions[, 1], 0.025),\n    pred_upper = quantile(predictions[, 1], 0.975),\n    stage = \"Stage 2: Model Fishing\",\n    model = \"Overfitted Model\",\n    predictions = list(predictions[, 1])\n  )\n\n# Stage 3: Scenario hacking - artificially increase difference\nmanagement_scenarios_hacked &lt;- management_scenarios |&gt;\n  mutate(\n    habitat_quality_mean = case_when(\n      action == \"Action A\" ~ 5.5, # Artificially reduced\n      action == \"Action B\" ~ 7.5 # Artificially increased\n    )\n  )\n\npred_hacked &lt;- management_scenarios_hacked |&gt;\n  rowwise() |&gt;\n  mutate(\n    habitat_values = list(rnorm(\n      1000,\n      habitat_quality_mean,\n      habitat_quality_sd\n    )),\n    predictions = list(\n      predict(\n        model_initial,\n        newdata = tibble(habitat_quality = habitat_values),\n        interval = \"prediction\"\n      )\n    ),\n    pred_mean = mean(predictions[, 1]),\n    pred_lower = quantile(predictions[, 1], 0.025),\n    pred_upper = quantile(predictions[, 1], 0.975),\n    stage = \"Stage 3: Scenario Hacking\",\n    model = \"Scenario Hacked\",\n    predictions = list(predictions[, 1])\n  )\n\n\n# Get descriptive statistics for violin plots\nall_predictions &lt;- bind_rows(\n  pred_hacked |&gt;\n    select(model, action, pred_mean, pred_lower, pred_upper, stage) |&gt;\n    mutate(\n      action_color = case_when(\n        action == \"Action A\" ~ \"#0072B2\",\n        action == \"Action B\" ~ \"#2C5F41\"\n      )\n    ),\n  pred_initial |&gt;\n    select(model, action, pred_mean, pred_lower, pred_upper, stage) |&gt;\n    mutate(\n      action_color = case_when(\n        action == \"Action A\" ~ \"#56B4E9\",\n        action == \"Action B\" ~ \"#009E73\"\n      )\n    ),\n  pred_overfitted |&gt;\n    select(model, action, pred_mean, pred_lower, pred_upper, stage) |&gt;\n    mutate(\n      action_color = case_when(\n        action == \"Action A\" ~ \"#56B4E9\",\n        action == \"Action B\" ~ \"#009E73\"\n      )\n    )\n) |&gt;\n  mutate(\n    stage = factor(\n      stage,\n      levels = c(\n        \"Stage 1: A Priori Model\",\n        \"Stage 2: Model Fishing\",\n        \"Stage 3: Scenario Hacking\"\n      )\n    )\n  )\n\n# Plot Coefficients\npred_distributions &lt;- bind_rows(\n  # Stage 1: Initial model\n  pred_initial |&gt;\n    select(action, predictions, stage) |&gt;\n    unnest(predictions),\n  # Stage 2: Overfitted model\n  pred_overfitted |&gt;\n    rowwise() |&gt;\n    select(action, predictions, stage) |&gt;\n    unnest(predictions),\n  # Stage 3: Scenario hacked\n  pred_hacked |&gt;\n    select(action, predictions, stage) |&gt;\n    unnest(predictions)\n) |&gt;\n  mutate(\n    stage = factor(\n      stage,\n      levels = c(\n        \"Stage 1: A Priori Model\",\n        \"Stage 2: Model Fishing\",\n        \"Stage 3: Scenario Hacking\"\n      )\n    ),\n    action_color = case_when(\n      stage %in%\n        c(\"Stage 1: A Priori Model\", \"Stage 2: Model Fishing\") &\n        action == \"Action A\" ~ \"#56B4E9\",\n      stage %in%\n        c(\"Stage 1: A Priori Model\", \"Stage 2: Model Fishing\") &\n        action == \"Action B\" ~ \"#009E73\",\n      stage == \"Stage 3: Scenario Hacking\" & action == \"Action A\" ~ \"#0072B2\",\n      stage == \"Stage 3: Scenario Hacking\" & action == \"Action B\" ~ \"#2C5F41\"\n    )\n  )\n\n# ---- Construct Plots ----\n# Violin Plots\np1 &lt;- ggplot(pred_distributions, aes(x = action, y = predictions)) +\n  geom_violin(aes(fill = I(action_color)), alpha = 0.7, trim = FALSE) +\n  geom_boxplot(aes(color = I(action_color)), width = 0.1, alpha = 0.8) +\n  stat_summary(\n    aes(color = I(action_color)),\n    fun = mean,\n    geom = \"point\",\n    size = 3,\n    shape = 18\n  ) +\n  facet_wrap(~stage, ncol = 3) +\n  labs(\n    y = \"Predicted Species Abundance\",\n    x = \"Management Action\"\n  ) +\n  theme_minimal() +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12),\n    axis.text.x = element_text(size = 14, angle = 45, hjust = 1),\n    axis.title.x = element_text(size = 16),\n    axis.title.y = element_text(size = 16),\n    strip.text = element_text(size = 14),\n    legend.position = \"none\"\n  )\n\n\neffect_sizes &lt;- all_predictions |&gt; # Calculate effect sizes at each stage\n  select(stage, action, pred_mean) |&gt;\n  pivot_wider(names_from = action, values_from = pred_mean) |&gt;\n  mutate(\n    difference = `Action B` - `Action A`,\n    effect_size = difference / 2 # Rough standardization\n  )\n\n# Model comparison plot showing overfitting with management actions\nmodel_comparison &lt;- tibble(\n  habitat_quality = seq(0, 10, 0.1)\n) |&gt;\n  mutate(\n    initial_pred = predict(model_initial, newdata = .),\n    overfitted_pred = predict(model_overfitted, newdata = .)\n  ) |&gt;\n  pivot_longer(\n    cols = c(initial_pred, overfitted_pred),\n    names_to = \"model_type\",\n    values_to = \"prediction\"\n  ) |&gt;\n  mutate(\n    model_type = case_when(\n      model_type == \"initial_pred\" ~ \"Initial Model\",\n      model_type == \"overfitted_pred\" ~ \"Overfitted Model\"\n    )\n  )\n\np3 &lt;- ggplot() +\n  geom_point(\n    data = data,\n    aes(x = habitat_quality, y = abundance),\n    alpha = 0.6,\n    color = \"grey50\"\n  ) +\n  geom_line(\n    data = model_comparison,\n    aes(\n      x = habitat_quality,\n      y = prediction,\n      color = model_type,\n      linetype = model_type\n    ),\n    linewidth = 1\n  ) +\n  # Initial management actions\n  geom_vline(\n    xintercept = management_scenarios |&gt;\n      pluck(\"habitat_quality_mean\", 1),\n    linetype = \"solid\",\n    color = \"#56B4E9\",\n    linewidth = 1,\n    alpha = 0.7\n  ) +\n  geom_vline(\n    xintercept = management_scenarios |&gt;\n      pluck(\"habitat_quality_mean\", 2),\n    linetype = \"solid\",\n    color = \"#009E73\",\n    linewidth = 1,\n    alpha = 0.7\n  ) +\n  # Add hacked actions\n  geom_vline(\n    xintercept = management_scenarios_hacked |&gt;\n      pluck(\"habitat_quality_mean\", 1),\n    linetype = \"dashed\",\n    color = \"#0072B2\",\n    linewidth = 1.2\n  ) +\n  geom_vline(\n    xintercept = management_scenarios_hacked |&gt;\n      pluck(\"habitat_quality_mean\", 2),\n    linetype = \"dashed\",\n    color = \"#2C5F41\",\n    linewidth = 1.2\n  ) +\n  # Arrows showing the manipulation\n  annotate(\n    \"segment\",\n    x = management_scenarios |&gt;\n      pluck(\"habitat_quality_mean\", 1),\n    xend = management_scenarios_hacked |&gt;\n      pluck(\"habitat_quality_mean\", 1),\n    y = 16,\n    yend = 16,\n    arrow = arrow(length = unit(0.3, \"cm\")),\n    color = \"#0072B2\",\n    linewidth = 1\n  ) +\n  annotate(\n    \"segment\",\n    x = management_scenarios |&gt;\n      pluck(\"habitat_quality_mean\", 2),\n    xend = management_scenarios_hacked |&gt;\n      pluck(\"habitat_quality_mean\", 2),\n    y = 16,\n    yend = 16,\n    arrow = arrow(length = unit(0.3, \"cm\")),\n    color = \"#2C5F41\",\n    linewidth = 1\n  ) +\n  labs(\n    x = \"Habitat Quality\",\n    y = \"Predicted Species Abundance\",\n  ) +\n  theme_minimal() +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(\n    legend.position = c(0.99, 0.01),\n    legend.justification = c(1, 0),\n    legend.background = element_rect(\n      fill = \"white\",\n      color = \"black\",\n      linewidth = 0.5\n    ),\n    legend.margin = margin(5, 5, 5, 5),\n    legend.text = element_text(size = 12),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12),\n    axis.text.x = element_text(size = 14, angle = 45, hjust = 1),\n    axis.title.x = element_text(size = 16),\n    axis.title.y = element_text(size = 16),\n  ) +\n  scale_color_manual(\n    \"Model Version\",\n    values = c(\"Initial Model\" = \"#E69F00\", \"Overfitted Model\" = \"#D55E00\")\n  ) +\n  scale_linetype_manual(\n    \"Model Version\",\n    values = c(\"Initial Model\" = \"solid\", \"Overfitted Model\" = \"dashed\")\n  ) +\n  # Action labels\n  annotate(\n    \"text\",\n    x = management_scenarios |&gt;\n      pluck(\"habitat_quality_mean\", 1),\n    y = 19,\n    label = \"Initial\\nAction A\",\n    color = \"#56B4E9\",\n    size = 3.5,\n    hjust = 1.1,\n    fontface = \"bold\"\n  ) +\n  annotate(\n    \"text\",\n    x = management_scenarios |&gt;\n      pluck(\"habitat_quality_mean\", 2),\n    y = 19,\n    label = \"Initial\\nAction B\",\n    color = \"#009E73\",\n    size = 3.5,\n    hjust = -0.3,\n    fontface = \"bold\"\n  ) +\n  annotate(\n    \"text\",\n    x = management_scenarios_hacked |&gt;\n      pluck(\"habitat_quality_mean\", 1),\n    y = 19,\n    label = \"Hacked\\nAction A\",\n    color = \"#0072B2\",\n    size = 3.5,\n    hjust = 1.3,\n    fontface = \"bold\"\n  ) +\n  annotate(\n    \"text\",\n    x = management_scenarios_hacked |&gt;\n      pluck(\"habitat_quality_mean\", 2),\n    y = 19,\n    label = \"Hacked\\nAction B\",\n    color = \"#2C5F41\",\n    size = 3.5,\n    hjust = -0.3,\n    fontface = \"bold\"\n  )\n\n# Table of Summary Statistics\nmetric_labs &lt;- c(\n  \"R2\" = \"R^2\",\n  \"R2 adjusted\" = \"{R^2}_{adjusted}\",\n  \"AIC wt\" = \"{AIC}_{wt}\",\n  \"AICc wt\" = \"{AICc}_{wt}\",\n  \"BIC wt\" = \"{BIC}_{wt}\",\n  \"Sigma\" = \"\\\\sigma\"\n) |&gt;\n  map_chr(~ glue::glue(\"${.}$\"))\n\nperformance_table &lt;-\n  performance::compare_performance(\n    model_initial,\n    model_overfitted,\n    rank = TRUE\n  ) |&gt;\n  select(-Model) |&gt;\n  mutate(\n    Name = stringr::str_replace(\n      Name,\n      \"model_overfitted\",\n      \"Overfitted Model\"\n    ) |&gt;\n      stringr::str_replace(\"model_initial\", \"Initial Model\")\n  ) |&gt;\n  mutate(across(-c(Name), ~ round(.x, digits = 2))) |&gt;\n  pivot_longer(-Name) |&gt;\n  pivot_wider(names_from = Name, values_from = value) |&gt;\n  mutate(name = str_replace(name, \"_\", \" \")) |&gt;\n  mutate(name = recode(name, !!!metric_labs)) |&gt;\n  mutate(name = vec_fmt_markdown(name)) |&gt;\n  gt(rowname_col = \"name\") |&gt;\n  text_transform(gt::md, cells_row_groups()) |&gt;\n  fmt_markdown(columns = name, rows = contains(\"$\")) |&gt;\n  tab_header(\n    title = \"Stage 2: Model Fishing\",\n    subtitle = glue::glue(\n      \"The modeller compares the two models \",\n      \"and chooses the overfitted model based on these statistics.\"\n    )\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#D55E00\"),\n    locations = cells_body(columns = \"Overfitted Model\", rows = everything())\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"#E69F00\"),\n    locations = cells_body(columns = \"Initial Model\", rows = everything())\n  ) |&gt;\n  tab_style(\n    style = cell_text(color = \"#D55E00\"),\n    locations = cells_column_labels(columns = \"Overfitted Model\")\n  ) |&gt;\n  tab_style(\n    style = cell_text(color = \"#E69F00\"),\n    locations = cells_column_labels(columns = \"Initial Model\")\n  ) |&gt;\n  tab_style(\n    style = cell_text(\n      font = google_font(\"Chivo\"),\n      size = \"medium\",\n      weight = \"bolder\"\n    ),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = cell_text(\n      font = google_font(\"Cairo\"),\n      color = \"black\",\n      size = \"medium\",\n      weight = 500\n    ),\n    locations = cells_body()\n  ) |&gt;\n  tab_style(\n    style = cell_text(\n      color = \"black\",\n      font = google_font(\"Cairo\"),\n      size = \"medium\",\n      weight = 400\n    ),\n    locations = cells_stub()\n  ) |&gt;\n  tab_style(\n    style = cell_text(\n      color = \"black\",\n      font = google_font(\"Roboto Condensed\"),\n      size = \"large\",\n      weight = 400\n    ),\n    locations = cells_title(groups = \"title\")\n  ) |&gt;\n  tab_style(\n    style = cell_text(\n      color = \"black\",\n      font = google_font(\"Cairo\"),\n      size = \"medium\",\n      weight = 400\n    ),\n    locations = cells_title(groups = \"subtitle\")\n  ) |&gt;\n  cols_width(stub() ~ px(170), everything() ~ px(100)) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_body(\n      rows = name == \"Performance Score\",\n      columns = \"Overfitted Model\"\n    )\n  ) |&gt;\n  tab_stub_indent(rows = name != \"Performance Score\", indent = 5)\n\n# ----- Construct Patchwork Plot -----\n\ntmp &lt;- tempfile(fileext = \".png\")\ngtExtras::gtsave_extra(\n  performance_table,\n  tmp,\n  zoom = 2,\n  expand = 0,\n  vwidth = 420,\n)\ntable_png &lt;- png::readPNG(tmp, native = TRUE)\n\npatch &lt;- (p1 + table_png) + plot_layout(widths = c(2, 1))\ncombined_plot &lt;- patch /\n  p3 +\n  plot_annotation(tag_levels = c(\"A\")) +\n  plot_layout(heights = c(2, 3))\n\nggsave(\n  filename = here::here(\"synthetic_example_QRPs.pdf\"),\n  device = grDevices::cairo_pdf,\n  combined_plot,\n  width = 17,\n  height = 12,\n  dpi = 600\n)"
  },
  {
    "objectID": "ms/QRPs in Ecological Modelling.html#footnotes",
    "href": "ms/QRPs in Ecological Modelling.html#footnotes",
    "title": "A Framework for Questionable Research Practices in Ecological Modelling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBoldface notation represents a vector or a set, indicating where multiples of those objects could be generated, e.g. there may be multiple ways to operationalise a conceptual model.↩︎"
  }
]
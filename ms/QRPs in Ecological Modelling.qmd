---
title: A Roadmap of Questionable Research Practices in Ecological Modelling
running-head: Questionable Research Practices in Ecological Modelling
author:
  - name: Elliot Gould
    email: elliot.gould@unimelb.edu.au
    orcid: 0000-0002-6585-538X
    url: https://github.com/egouldo
    corresponding: true
    affiliation:
      - ref: 1
  - name: Hannah S. Fraser
    orcid: 0000-0003-2443-4463
    affiliation:
      - ref: 1
  - name: Libby Rumpff
    orcid: 0000-0001-9400-8086
    affiliation:
     -  ref: 1
  - name: Fiona Fidler
    orcid: 0000-0002-2700-2562
    affiliation:
      - ref: 2
affiliations:
  - id: 1
    name: The University of Melbourne
    department: School of Agriculture, Food and Ecosystem Sciences
  - id: 2
    name: The University of Melbourne
    department: School of Historical and Philosophical Studies
format:
  html:
    page-layout: full
    title-block-banner: true
    code-fold: true
  pdf:
    keep-tex: true
    documentclass: scrartcl
    papersize: A4
    fontsize: 11pt
    toc: true
    toc-depth: 2
    number-sections: true
    linkcolor: blue
    include: false
    header-includes:
    - \usepackage{mathtools}
    - \usepackage{amssymb}
    - \usepackage{xcolor}
    - \usepackage{booktabs, caption, longtable, colortbl, array}
custom-numbered-blocks:
  classes: 
    Box:
      boxstyle: foldbox.simple
      collapse: false
pdf-engine: xelatex
abstract: |
  1. Questionable research practices (QRPs), result in low rates of replicability and contribute to biased accounts of studied phenomena in the published literature, producing a literature full of apparently strong and conclusive results. Recent metaresearch has revealed that ecology is at risk of a ‘replicability crisis’ due to the presence of QRPs and a lack of transparency in published research. However, the work to date focusses on QRPs as they occur in hypothesis testing research which is not a good fit for use in ecology and related fields that conduct exploratory or predictive research using complex models. 
  2. In order to protect against bias caused by QRPs in ecology, it is essential to identify QRPs relevant to model-based research and understand why they occur, how frequent they are and how severe the consequences might be. In this paper we propose a conceptual framework for describing QRPs in ecological model-based research. 
  3. We describe a generalised workflow for model development, evaluation and analysis as it is typically undertaken in ecology and related fields. We develop a typology of QRPs that describes potential mechanisms for emergence of QRPs throughout the modelling process. From this conceptual framework, we derive a definition of QRPs that captures the specificities and features of the modelling process particular to ecology. Finally, we create a diagrammatic roadmap that map QRPs onto common decision-points in the modelling process, highlighting points of caution for modelers. 
  4. Our analysis and framework highlight that QRPs in the modelling process are a threat to the credibility of model-based research. QRPs may target the model itself, or the outputs of model analyses, including model checking, or testing and evaluation performed on the model.  Finally, our proposed definition of QRPs de-emphasises the idea of finding false positive results and instead points to other measures of model credibility that affect publishability, such as the accuracy and precision of the model and or its outputs. 
  5. In this paper we aim to raise awareness among modelers about the potential types of QRPs and their mechanisms for emergence in the modelling process. The roadmaps in this paper will help modelers to understand the risks of QRPs in their research so they are empowered to implement procedures and practices that can reduce the occurrence and impact of QRPs.
keywords: 
  - questionable research practices
  - ecological modelling
  - metaresearch
  - transparency
  - reproducibility
  - model credibility
  - researcher degrees of freedom
date: last-modified
bibliography: references.bib
filters:
- ute/custom-numbered-blocks
- callout-box.lua
- addcalloutnumber.lua
---

```{r}
#| label: setup
#| warning: false
#| message: false
library(tidyverse)
library(here)
library(janitor)

library(gt)
library(gtExtras)

library(ggh4x)
library(ggforce)
library(waffle)
library(ggsci)
library(hrbrthemes)

library(extrafont)
library(firasans)
# extrafont::loadfonts()

source(here::here("R/tidy_QRP_database.R"))
source(here::here("R/make_table_data.R"))
source(here::here("R/make_QRP_table.R"))
```

# Introduction

Self-report surveys of researchers’ statistical practices suggest high rates of Questionable Research Practices (QRPs e.g., p-hacking, cherry picking, see [@tbl-QRP-examples-html]{.content-visible when-format="html"} [@tbl-QRP-examples]{.content-visible unless-format="html"}) in several different disciplines: psychology (ref), education (ref) and ecology [@Fraser:2018cl]. 
Widespread QRPs, accompanied by a lack of transparency and openness in research reporting [@culina2020] can leave disciplines at risk of a ‘reproducibility crisis’ [@Fidler:2017he] . To understand the implications of this for ecology, we need a more comprehensive account of what constitutes Questionable Research Practice. QRPs are a set of methodological and statistical practices that fall into an ‘ethical grey zone’ between ideal responsible research conduct and unacceptable research behaviours like fabrication and plagiarism [@Steneck2006] . When researchers are faced with ambiguous decisions where there is no single acceptable decision or strategy according to scientific standards and consensus, they are likely to select the decision that results in the most ‘favourable’ finding with “convincing self-justification” [@Hoffmann2021; @Simmons:2011iw]. QRPs may not be the product of conscious ‘fishing expeditions’ [@Gelman2013] but rather occur because researchers are motivated actors prone to cognitive biases incentivised by a ‘publish or perish’ research culture in which publication bias selects for particular types of findings [@Agnoli:2017kl]. These findings are generally significant results or ‘unexpected’ /surprising findings [@Nissen2016; @Nakagawa2025]. In NHST these are generally statistically significant findings, correlations rather than absence of correlations ‘unexpected’ findings (ref). Findings that are not deemed ‘publishable’ are either sent to the ‘file drawer’ or else subject to practices, such as ‘p-hacking,’ that make the results more likely to be published. Publication bias and the file drawer effect facilitate the dissemination of false knowledge, consequently, QRPs bias the literature and ultimately undermine the credibility and reproducibility of research findings [@Cockburn2020; @Agnoli:2017kl]. Current definitions of QRPs are focused on hypothesis testing research (specifically null-hypothesis significance testing), with QRPs typically defined as practices that inflate the chance of finding a false positive result (type I error) (ref? Simons maybe?). Common examples of QRPs are centred around practices that manipulate p-values. Whilst Null Hypothesis Significance Testing has been and still is in popular use in ecology [@Fidler:2017he; @Stephens2007], a large amount of applied problem-solving work in the field is model-based. Ecological modellers may be inclined to think that the concerns associated with QRPs and reproducibility do not apply to them. In practice, this work is quite different to null hypothesis significance testing: Instead of relying on a single quantitative measure, studies describing ecological models usually generate a suite of measures and analyses that pertain to different desirable attributes of the model and collectively inform claims about the model being sufficient to fulfil its intended purpose. This begs the question of whether QRPs are relevant to model-based research? If specific QRPs related to p-values, like p-hacking, don’t apply, are there parallel or counterpart practices that do? What form might they take? And where in the research process would we be likely to find them? This paper addresses each of these questions. Our primary aim in this paper is to highlight the range of particular decision-points and practices that could be questionable so as to identify potential QRPs in model-based research. We encourage modellers to pause at such junctions and assess for themselves what the relative risk of QRPs is.
SEE Box \ref{boxqrps}

```{r}
#| label: create-QRP-table
#| message: false
#| warning: false
#| echo: false

QRP_tbl <- readr::read_csv(here::here("data", "tbl-QRP-frequency.csv"),
                show_col_types = FALSE) %>% 
  mutate(Category = here::here("data", "icons", 
                               glue::glue("{Category}.png"))) %>%
  gt::gt(groupname_col = "Category",
         row_group_as_column = TRUE
           ) %>% 
  gt::tab_footnote(footnote = "Makel et al. (2019)", 
                   locations = cells_column_labels(columns = Abbreviation)) %>% 
  gt::tab_footnote(footnote = "Fraser et al. (2018), n=494", 
                   locations = cells_column_labels(columns = 
                                                     c("Ecology", 
                                                       "Evolution"))) %>% 
  gt::cols_align(align = "center", columns = "Category")

image_out <- function(x, height = 50){
  out <- map_chr(x, gt::local_image, height = height)
  out <- map(out, gt::html)
  out
}

QRP_tbl_html <- QRP_tbl %>% 
  gt::text_transform(locations = gt::cells_row_groups(everything()),
                     fn = image_out
  ) %>% 
   gtExtras::gt_theme_nytimes() %>%
  gtExtras::gt_plt_bar_pct(
    fill = "green",
    column = c("Ecology"),
    scaled = TRUE,
    width = 100,
    labels = TRUE,
    background = "lightgrey"
  ) %>% 
          gtExtras::gt_plt_bar_pct(
    fill = "blue",
    column = c("Evolution"),
    scaled = TRUE,
    width = 100,
    labels = TRUE,
    background = "lightgrey"
  ) %>% 
  gt::cols_width(
    Category ~ gt::pct(8),
    `Questionable Research Practice` ~ gt::pct(30),
    Abbreviation ~ gt::pct(20),
    Ecology ~ gt::pct(10),
    Evolution ~ gt::pct(10)
  )

QRP_tbl_html %>% 
  gt::gtsave(here::here("figures", "tbl-QRP-examples.png"), 
             vwidth = 800,
             vheight = 1000)
```

::: {.Box title="Common QRPs associated with Null Hypothesis Significance Testing in Ecology" #boxqrps } 

::: {.content-visible when-format="html"}


```{r}
#| label: tbl-QRP-examples-html
#| tbl-cap: 'Examples and self-reported frequency of questionable research practices in hypothesis-testing research in Ecology and Evolutionary Biology. QRPs are categorised as "cherry-picking", "p-hacking", and "methodologically flawed", indicated by the cherry, saw, and cross icons respectively.'
QRP_tbl_html
```

:::




The table has been moved outside of the box to isolate whether the problem is a box / float one, or a gt one.



A table

:::


:::{#box-1 .callout-box}



```{r}
#| include: true
#| label: tbl-QRP-examples
#| tbl-cap: "A test caption"
#| echo: false
#| out-width: "100"
#| out-height: "100"
magick::image_read(here::here("figures/tbl-QRP-examples.png") )
```

:::

```{r}
#| include: false
#| eval: true
#| results: asis
#| echo: false
#| label: table-QRP
##| label: tbl-QRP-examples
rm_icon_path <- function(x){
  out <- map_chr(x, fs::path_file) %>% 
    map_chr(fs::path_ext_remove)
  out
}

QRP_tbl %>% 
    gt::text_transform(locations = gt::cells_row_groups(everything()),
                     fn = rm_icon_path ) %>% 
  gt::fmt_percent(
    columns = c(Ecology, Evolution),
    scale_values = FALSE,
    decimals = 0
  ) %>% 
  gt::cols_width(
    Category ~ gt::px(65),
    `Questionable Research Practice` ~ gt::px(250),
    Abbreviation ~ gt::px(100),
    Ecology ~ gt::px(60),
    Evolution ~ gt::px(60)
  ) %>% 
   tab_style(
    style = cell_text(size = pct(50)),
    locations = cells_body()
  ) %>% 
  tab_style(
    style = cell_text(size = pct(45)),
    locations = cells_column_labels()
  )   %>%
   tab_style(
    style = cell_text(size = pct(45)),
    locations = cells_stub()
  ) %>% 
  gt::tab_options(
    # table.font.size = "smaller",
    quarto.disable_processing = TRUE,
    quarto.use_bootstrap = TRUE) 
```




### Why QRPs in Hypothesis Testing don’t directly translate to modelling

One principle underlying the classification of these practices as ‘questionable’ relates to data driven decision making. In contrast to hypothesis testers, modellers makes a series of analytic decisions on the basis of both objectively identified model performance criteria and subjective judgements [@Babel2019; @Bennett2013], wherein which they shift between subjecting the model to analysis, validation and testing, and refining the model in response to those results [@Getz2017]. This process is non-linear, iterative, and generates many interim versions of the model preceding publication [@Augusiak2014], unfolding much like the ‘survival of the fittest’ model(s) [@Prosperi2019] given a specified purpose and the available resources. In modelling, analytic decision-making is adaptive and data-dependent. The emphasis of existing QRP definitions on type I errors is unhelpful for model-based research. Firstly, multiple sources and types of error may arise in the modelling process - there is model structural uncertainty, uncertainty in parameter estimates, and uncertainty in predictions. What constitutes an ‘error’, what the source of that error is, as well as the relative weighting of those different errors, depend on some combination of the model’s purpose (prediction, explanation, description), the type of model used (phenomenological or mechanistic) and the context for the model. Rather than relying on p-value as a metric, model performance metrics consist of both qualitative and quantitative measures that incrementally build a subjective picture of model credibility rather than providing dichotomous responses assessing model credibility [@Augusiak2014; @Hamilton2019]. Since the publishability of a model-based study hinges on the collection of these model outputs and their contribution to overall model credibility, each of these model outputs may be susceptible to questionable research practices in order to strategically alter the perceived credibility of the model. What can be ‘tweaked’, ‘hacked’ or ‘exploited’ in hypothesis testing are *p*-values and whilst there may not be a direct counterpart in model-based research, we do expect similar a similar motivation from publishability to apply.

We argue that a conceptual framework of QRPs in model-based research must account for certain kinds of data-dependent decisions that are an appropriate and justifiable part of the modelling process while highlighting the primacy of guarding against data-dependent decision-making at other decision-points that might be questionable. Moreover, the conceptual framework should de-emphasise the risk of type I errors and false positive findings to account for other biases more relevant to how complex models are evaluated and used.

This paper aims to raise awareness among ecologists (and modelers within other disciplines) about the potential for QRPs at various points in the modelling process. Various solutions for mitigating the risk of QRPs have been proposed for hypothesis testing research, and in our conclusion, we adapt on expand on these to recommend tools and practices that may improve the reproducibility and reliability of model-based research. It will also facilitate future attempts to estimate the severity and extent of QRPs in ecology by providing a more comprehensive account of what constitutes questionable practice in non-hypothesis testing research.

## A Conceptual Framework of Questionable Research Practices in Ecological Modelling

### What makes a model publishable? Identifying reasons for QRPs

We begin with an exposition of ‘publishability’ of the modelling process, since these attributes are the primary targets of QRPs. In identifying these attributes we can consider what linked objects corresponding to those attributes might be the target of QRPs, just as p-values are for NHST research. From there we can explicate specific practices that target those attributes, and where in the modelling cycle those practices might emerge. Since ‘publishability’ seems to be the main target and motivator of QRPs, understanding how publishability is defined in MBR can help us identify the types and location of QRPs in the modelling process. What features of a modelling study might QRPs seek to target / work on? What publishable attributes does a QRP seek to ‘hack’, ‘tweak’ or ‘exploit’? What research objects / modelling outputs correspond to those different publishable attributes? From there we can identify potential practices that might seek to bias these different publishabile attributes. Understanding how publishability is defined in MBR and what modelling outputs or research objects correspond to different aspects of publishability can help us identify potential QRPs, and where those practices might emerge in the modelling process. Attributes of publishability \<-\> what research outputs correspond to these different attributes. And therefore what QRPs might exist for modelling, and where in the modelling process. By articulating different attributes of a model / modelling study that affect the publishability of a model we can identify potential practices that are vulnerable to QRPs. Below, we highlight and discuss several properties of a model that contribute to its publishability: that the model has a certain advantage over a prior art, and that the analysis of model application demonstrates the credibility of the model. The relative weighting of these properties in evaluating the model depends on the overarching model purpose (Eker et al., 2018; Guisan & Zimmermann, 2000; Hamilton et al., 2019), where model purpose includes the management, problem and project contexts (Hamilton et al. 2022). Depending on the particular purpose of the model / modelling study, different attributes of publishability are more or less important, and therefore depending on the purpose/weighting, different parts of the model / model outputs are likely to be the target of QRPs (this forms part of the exposition below).

#### Advantage over the prior art

Published ecological models are often extensions of existing tools or new modelling frameworks for a specific intended purpose because ecological modellers are disincentivised to test existing models on new data due to both publication bias and funder stipulations that rewards constructing new models and novel modelling approaches (Babel et al., 2019; Houlahan, 2016). Models are therefore often ‘sold’ on their improvement over previous models or modelling frameworks, either by amendments to a model’s conceptual scheme (the framing and abstraction and representation of the mental model of the problem), or by improvements to the mathematical formulation or computational implementation of the model (Alexandrov, 2011). For example, the model addresses new environmental situations that weren’t addressed by the prior model, the model code is more efficient than the prior model, or perhaps needs less initial information to generate outputs for analysis, or the specific parameterisation shows better performance than the prior model in addressing certain environmental conditions (Alexandrov, 2011).

#### Model Credibility

Model credibility arises gradually throughout the modelling process (Augusiak et al. 2014) and is based on a subjective confidence level or degree of trust that both the model and the model results can be used as claimed (Guisan & Zimmerman, 2000; Hamilton et al. 2019). Model credibility does not relate to the representational accuracy or ‘truth’ of the model, but rather that the model adequately captures the phenomenon of interest given existing evidence and the model purpose (Augusiak, 2014). A model is only one of many subjective and plausible representations of ecological phenomena and one representation may be preferable to another depending on how useful it is for a particular problem (Augusiak et al. 2014; Risbey, 2005). Thus, a model may be defined as credible if the user has sufficient confidence to support the relevant scientific or management decisions (Rykiel, 1996).

*Adequacy*

Model adequacy refers to the ability of the model to both “capture an underlying reality” (conceptual validity) and predict accurately. Determining model adequacy involves not only verifying that model output matches observations (Rykiel, 1996), but that the model’s “conceptual schema” is adequate for its purported application (Aldexandrov, 2011) – i.e. can the model adequately answer the research question (Christin et al. 2020)? It is common to have a range of scientific opinions regarding the ‘best’ representation of the model. Nevertheless, it is always possible to make the distinction between environmental conditions that may fall within the conceptual scope of the model and those that may not (Alexandrov, 2011).

*Reliability*

Reliability may be defined as “how much confidence to place in inferences about the real system that are based on model results” so that model inferences can be transferred to the real world (Schmolke, 2010). Reliability can be interpreted in several ways, typically it is thought of as the ability of a model’s prediction to fall within a specified range of accuracy (Alexandrov, 2011). But it also speaks to the model’s generalisability, versatility (Rykiel Jr, 1996) or transferability (Yates et al., 2018) — its ability to meet some specified threshold of performance in situations for which it was not originally designed, or to capture extreme conditions beyond which the calibration data do not reflect (Alexandrov, 2011). Regardless of whether reliability is referring to accuracy/precision of predictions in both its original scope or within a model transfer, evaluation of reliability is grounded within the model’s usefulness for its intended user, whether this be purely for scientific purposes, or for evidence-informed decision-making. The ultimate test of model reliability, is whether the model user has sufficient confidence in the model to use it to inform management decisions (Alexandrov, 2011).

### What components of models are affected by QRPs?

Here we outline the model development process and identify ‘model objects’ generated throughout the model development process. Various modelling outputs are generated throughout the model development process, including the model itself, model fit statistics and summary measures, and other model evaluation results (Figure 1). The outputs of model development, evaluation and analysis collectively inform the overall publishability and credibility of the model, and therefore are vulnerable to QRPs. Model outputs may be manipulated to improve the chances of model acceptance or publication, and the targeted outputs will depend on the model’s purpose. The purpose of this section is to 1. aid in identifying potential QRPs by highlighting model outputs that questionable practices seek to alter, 2. Lay the groundwork for an extension of Gelman and Loken’s mathematical formalism explaining the emergence of QRPs or the ‘garden of forking paths’ to model-based research, and 3. Identify steps and phases in the modelling process on which to build our QRP roadmaps.

The ecological modelling literature is filled with prescriptive and normative accounts of the modelling process for different purposes and modelling contexts, and we acknowledge the plurality and lack of consensus among these accounts in terms of the overall process, as well as the linguistic terminology for different phases, steps and tasks within the modelling process. Rather than selecting a single taxonomy with which to generate multiple distinct process descriptions and model object ontologies for different categories of modelling, we describe a process that can be generalised across different model purposes, contexts, types and methods. Some aspects may not apply in every modelling problem, and the specific collection of model outputs, their relative importance on which any model-based reasoning is based, as well as the weighting of various publishable attributes of the model, will differ depending on the model purpose, context and approach of the modelling problem at hand. We step through key phases of the modelling process below.

![During the model development process, the model undergoes specification, and testing, generating the following objects and attributes: i. Model specification or structure, M_s; ii. data collection; iii. parameterisation, yielding M_p and associated model parameters, iv. Model predictions, v. Model evaluation results including model fit statistics such as AIC, BIC, and model validation testing results such as confusion matrices, commission / commission rates and other measures of model validity, and sensitivity analyses. B. The suite of analyses from model evaluation v. and model predictions iv. collectively sum / contribute to the overall credibility of the model, which in turn determine the publishability, or appropriateness of the model for application in real-world decisions. In this description of the model development process, we distinguish between data / model input (yellow, y) and model output (green, j).](images/clipboard-2357433507.png){#fig-model-objects}

#### Conceptual Model

#### Model Specification

#### Parameterised Model

#### Model Evaluation

### A Provisional Typology of QRPs


```{r}
#| label: tbl-QRP-typology
#| html-table-processing: none
#| tbl-cap: |
#|   A model-based typology of Questionable Research Practices (QRPs), based on @Gelman2013. QRPs may affect different components of a model (table rows) indirectly altering the publishable attributes of a model by selectively reporting or directly altering objects generated throughout the modelling proces, targeting the model itself ($M$, purple cells) or model outputs ($M_j$, green cells). The final row represents the unambiguous ideal where no QRP occurs, that is, analytical decisions $\\phi$ are pre-specified rather than contingent on the model and/or data at that decision-point $\\phi(\\mathbf{X}, M)$.
tibble::tibble(QRP_outcome = c("The Model or Model Outputs<br>$(M, M_j)$", "Publishable Attributes of a model, via selective reporting or 'fishing'", "No QRP, e.g. pre-specified analytic decisions"),
               QRP_target_M = c("$$C (\\mathbf{X}, M;\\phi (\\mathbf{X},M))$$", 
                                "$$C (\\mathbf{X}, M;{\\phi}^{best} (\\mathbf{X},M)) $$", 
                                "$$C (\\mathbf{X}, M;\\phi ) $$"),
               QRP_target_Mj = c("$$ C (\\mathbf{X}, M;{\\phi}_{{M}_{j}} (\\mathbf{X},M))$$",
                                 "$$C (\\mathbf{X}, M;{{\\phi}_{{M}_{j}}}^{best} (\\mathbf{X},M))$$",
                                 "$$ C (\\mathbf{X}, M;{\\phi}_{{M}_{j}})$$"),
               row_group = c("QRP", "QRP", "OK")) %>% 
  gt::gt() %>% 
  gt::tab_spanner(label = "The practice may target:",
                  columns = c(QRP_target_M, QRP_target_Mj),) %>% 
  # gt::tab_row_group(label = "Questionable Research Practice", rows = row_group == "QRP" ) %>% 
  #   gt::tab_row_group(label = "Acceptable Research Practice", rows = row_group == "OK" ) %>% 
  gt::cols_label(QRP_outcome = "It may alter:",
                 QRP_target_M = gt::md("The model $$M$$"),
                 QRP_target_Mj = gt::md("Any model output $$M_j$$")) %>% 
  gtExtras::gt_theme_nytimes() %>% 
  gt::tab_style(style = gt::cell_fill(color = "#A02B93", alpha = 0.8),
                locations = gt::cells_body(columns = QRP_target_M,  rows = row_group == "QRP")) %>% 
  gt::tab_style(style = gt::cell_fill(color = "#4EA72E", alpha = 0.8),
                locations = gt::cells_body(columns = QRP_target_Mj, rows = row_group == "QRP")) %>% 
  gt::cols_hide(row_group) %>% 
  gt::fmt_markdown(columns = c(QRP_outcome)) %>% 
  gt::cols_align(columns = starts_with("QRP_target"),
                 align = "center")
```

## A Roadmap of QRPs in the Ecological Modelling Process

### Method

We surveyed the modelling literature to identify potential QRPs in ecological modelling and their location in the modelling process. We categorised QRPs into broader classes corresponding to families of similar practices using well-known published classifications (e.g. @tbl-QRP-examples-html @tbl-QRP-examples) and adopted new classes when there was no NHST-analogue. We coded the phase and sub-phases of modelling in which the practice occurs, as well as the model object (input, model, output) directly affected by the QRP, i.e. the *target* of the practice. After initial coding of the QRPs we refined the practice descriptions to encapsulate similar practices, as well as the broader QRP classes into which practices were grouped. Our coding is described in further detail in @sec-appendix-2.

```{r}
#| label: waffle_data_prep
#| message: false
#| warning: false
waffle_plot_data <- 
        tidy_QRP_database(here::here("data/QRP_database_2025-10-20.csv")) %>% 
        filter(include) %>% 
        select(-practice_notes, -model_subphase, -source, -qrp_reason, -include, -starts_with("practice_"), -file_name, practice_target) %>% 
        distinct() %>% #rm duplicate qrp_coded #TODO next merge duplicates while keeping source
        drop_na() %>% #interim approach until datachecks in place
        mutate(model_phase = str_split(model_phase, ", "),
               practice_target = str_split(practice_target, ", "),
               values = 1) %>% 
        unnest(model_phase) %>% 
        unnest(practice_target) %>% 
        complete(qrp_description, 
                 model_phase, 
                 practice_target, 
                 fill = list(values = 0.0001) # see https://github.com/hrbrmstr/waffle/issues/66
                 ) %>%  
        group_by(qrp_description) %>% 
        fill(qrp_coded, model_phase, .direction = "downup") %>% 
        ungroup()
```

```{r}
#| label: tbl-QRP-categories
#| tbl-cap: "Classes of Questionable Research Practices (QRPs) in ecological modelling."
#| echo: false

qrp_class_descriptions <- tribble(~ qrp_coded, ~ description,
        "Selective Debugging", "The practice of only checking for and correcting errors in a model when results are unexpected or unfavourable, rather than performing systematic error checking regardless of outcomes.",
        "Overfitting", "Developing a model that fits the training data too closely, including noise and peculiarities specific to that dataset, resulting in poor generalisability to new data. This often occurs when models are made overly complex relative to the amount of training data available.",
        "Executing Alternative Analyses", "Conducting multiple different analyses or model variations and selectively reporting only those that yield favourable results, without disclosing the full range of analyses performed.",
        "S-hacking", 
        "Manipulating model inputs, outputs or the model itself to obtain a favourable value of a performance measure, usually by systematically altering the modelling until a desired level of a statistic or metric is obtained [@Nakagawa2025; @Nagy2025]. This can include changing random seeds, outcome variable and/or performance thresholds or metrics and re-executing the analysis, fitting, validation or evaluation without disclosing [@feng2019]. ",
        "Overhyping", 
        "Typically, overyhyping features in the discussion section of a paper and involves exaggerating or overstating the capabilities, generalisability, or reliability of a model beyond what is justified by the evidence.",
        "Data Curation", 
        "Selectively including, excluding, or modifying data points in the sample used to develop or evaluate a model, often to improve apparent model performance (a specific instance of S-hacking).",
        "Model Fishing", 
        "Searching through a large number of possible model specifications or variable combinations without theoretical justification, in an attempt to find favourable results, and not reporting all models and/or the dredging process. Alternatively, conducting alternative analyses or fitting new model variations, and selectively reporting only those that yield favourable results, without disclosing the full range of analyses performed.",
        "Selective Reporting", 
        "Choosing to report only certain analyses, models, metrics, model results or comparisons that yield favourable results or desired conclusions without disclosing the full range of analyses performed, or their results.  This can create a misleading impression of model performance or study outcomes. In contrast to ‘fishing expeditions’, models have theoretical justification.",
        "HARKing", "May accompany fishing expeditions, or model dredging analyses and model selection procedures that have been selectively reported. The selected model(s) is presented as if it was pre-specified and theoretically justified before parameterisation, and/or, the model dredging process is not disclosed.")

waffle_plot_data %>% 
  distinct(qrp_coded) %>% 
  filter(qrp_coded != "Poor Practice") %>% 
  arrange(match(qrp_coded, c("Selective Reporting", "S-hacking", "Model Fishing", "Data Curation", "Overhyping"))) %>% 
  left_join(qrp_class_descriptions, by = join_by(qrp_coded)) %>% 
  bind_rows(qrp_class_descriptions %>% filter(qrp_coded == "HARKing")) %>% 
  gt::gt() %>% 
  gt::cols_label(qrp_coded = "QRP Class") %>% 
  gtExtras::gt_theme_nytimes() %>% 
  gt::cols_width(
    qrp_coded ~ gt::pct(15),
    description ~ gt::pct(85)
  ) %>% 
  gt::fmt_markdown(columns = description) 

```

### Roadmap

We summarise a subset of QRPs, their location in the modelling process, and their target modelling object in @fig-waffle_plot, highlighting a mixture of common practices, or practices that have the potential for critically influencing the perceived credibility of a model and its publishability. The complete set of QRPs identified is presented in @tbl-QRP.

We grouped QRPs into broader classes described in @tbl-QRP-categories. We applied some umbrella terms identified by Nagy et al. [@Nagy2025] and Liu et al. [-@Liu2020], including: "Sample Curation", "Selective Reporting" or "Cherry-picking," "Overhyping" and "Executing Alternative Analyses." We used additional terms for groups of practices without an existing umbrella term, e.g. 'Overfitting' and 'Model Dredging', 'Selective Debugging'. In the same vein as Nagy et al.'s [-@Nagy2025] expansion of the concept of *p*-hacking to encompass other analysis metrics, we expanded the concept of *p*-hacking and renamed it "S-hacking", or statistic-hacking, encapsulating analogous practices that occur in modelling.

HARKing (*hypothesising after results are known*) is a well-known type of QRP, but was not explicitly included in our final classification of QRPs. However, some practices, like Overhyping, could be considered a form of HARKing.

Although overfitting is a common outcome of many QRPs, especially S-hacking and Sample Curation (see also @tbl-QRP for descriptions of the effects or impacts of each practice), we included Overfitting as a distinct class of QRPs because overfitting may also be the primary goal of the practice. Overfitting is like S-hacking but perhaps is less constrained in practice, because the 'model space' is less determined by a specific set of performance measures / statistics, whereas practices that specifically aim to 'overfit' the model to the calibration data operate on a much larger decision-space, so the risk of a spurious finding is greater.

Similarly, Sample Curation includes practices that directly affect or target input data, but the intent of the practice might be to S-hack. For instance, Optional Stopping rules directly affect the input data, but indirectly target some performance measure by continuing to collect and update the data until some desirable threshold in the performance measure is met.

We categorised QRPs according to the model object they directly target, i.e. the model inputs, the model itself, or the model outputs, corresponding to the the model objects in Figure X.

We also grouped QRPs according to the phase of the modelling process in which they occur, i.e. Data, Model Construction, Model Evaluation and Model Application. Some classes of QRPs can occur in multiple phases of the modelling process, e.g. Sample Curation can occur in both Data and Model Construction phases, and Overfitting can occur in Model Construction and Model Evaluation phases while S-hacking can occur during Model Construction, Evaluation and Application. More narrowly defined classes of QRPs are restricted to one phase, e.g. Selective Debugging occurs only in Model Evaluation, or one type of target, e.g. Overfitting and Model Dredging target the model itself. Sometimes the QRP might occur in a second iteration of the phase, e.g. after model evaluation, but during model specification, i.e. the model specification is revised based on observing model evaluation results.

```{r}
#| label: fig-waffle_plot
#| fig-cap: "Summary of Questionable Research Practices (QRPs) in ecological modelling. QRPs may target model inputs (yellow squares), the model itself (red squares), and/or model outputs (green squares), and may occur at different phases in the modelling cycle. QRPs are grouped according to broader classes defined in @tbl-QRP-categories. See @tbl-QRP for the full list of QRPs we identified."
#| fig-width: 14
#| fig-height: 18

# #| dev: "cairo_pdf" # when output is pdf
# options(hrbrthemes.loadfonts = TRUE)
waffle_plot <- waffle_plot_data %>% 
        distinct() %>% #TODO identify duplicated rows - suspect non-unique practice_target coding is cause
        mutate(model_phase = 
                       forcats::as_factor(model_phase) %>% 
                       forcats::fct_relevel( c("Data", 
                                               "Model Construction", 
                                               "Model Evaluation",
                                               "Model Application"))) %>% 
        arrange(model_phase, 
                qrp_coded) %>% 
        ggplot(aes(fill = practice_target, 
                   values = values)) +
        geom_waffle(size = 2, 
                    color = "white", 
                    make_proportional = FALSE, 
                    flip = TRUE) +
        scale_x_discrete(expand = c(0,0)) +
        scale_y_discrete(expand = c(0,0)) +
  scale_fill_manual(
          name = "Target Model Object",
          values = c("input" = "#0F9ED5", # blue
                     "model" = "#A02B93", # purple
                     "output" = "#4EA72E"), # green
          labels = c("input" = "Model Input",
                     "model" = "Model",
                     "output" = "Model Output")
        ) +
        # ggsci::scale_fill_futurama(name = "QRP Target") +
        coord_equal() +
        facet_nested(qrp_coded + qrp_description ~ model_phase, 
                     switch = "y", 
                     labeller = 
                             labeller(
                                     qrp_description =
                                             label_wrap_gen(width = 80,
                                                            multi_line = TRUE),
                                     qrp_coded =
                                             label_wrap_gen(width = 10,
                                                            multi_line = TRUE),
                                     model_phase = 
                                             label_wrap_gen(width = 10, 
                                                            multi_line = TRUE)),
                     nest_line = element_line(linetype = 1),
                     solo_line = TRUE,
                     space = "free",
                     strip = strip_nested(size = "variable",
                                          clip = "off")
        ) +
        # facet_grid(qrp_description ~ model_phase, 
        #            switch = "y", 
        #            labeller = labeller(qrp_description = 
        #                                        label_wrap_gen(width = 100))) +
        # theme_no_axes() +
        hrbrthemes::theme_ipsum_rc() +
        waffle::theme_enhance_waffle() +
        theme(strip.text.y.left = element_text(angle = 0, 
                                               size = 12),
              strip.text.x = element_text(size = 14,vjust = 0),
              strip.background.x = element_part_rect(side = "b"),
              strip.background.y = element_blank()
        ) +
        ggh4x::force_panelsizes(rows = unit(1, "cm"), 
                                cols = unit(3, "cm"))

print(waffle_plot)


#TODO update based on revised definitions
#DONE reorder the model process levels
#TODO exclude less important QRPs
#DONE Make strips pretty
#DONE recolour model object colours
#DONE group the QRPs into their umbrella terms -- nested facets?

```

## Takeaway messages

Our analysis identified *55* (but 28 included) distinct (QRPs) in ecological modelling, spanning the entire modelling workflow from problem formulation through to model application. We categorised these practices into seven primary types: model dredging, overfitting, S-hacking, sample curation, selective reporting, overhyping, and poor practice. The distribution of QRPs across modelling phases highlights systematic vulnerabilities in how ecological models are constructed, evaluated, and applied.

### Data Handling

Data-related QRPs $(n=10)$ included *sample curation* and *missing data hacking* and *employing optional stopping rules*. All of these practices involve making post-hoc decisions about model inputs without justification and prior planning, i.e. after fitting model and observing model results. Sample curation may include removing observations that should have been included in order to make a correlation of interest become significant, and generating a post-hoc exclusion criteria for those observations being excluded [@Nagy2025]. Opportunistic handling of missing data could occur when a researcher tries multiple ways of handling missing data, for instance, list-wise deletion, multiple imputation and inverse probability weighting. But the expected results only appear with inverse probability weighting and so the researcher only reports this strategy in the paper, omitting the results when the other data handling methods were used [@Nagy2025]. *Opportunistic stopping* occurs when new data is collected and is used to re-parameterise the model after previously observing model validation and model evaluation results, without reporting results of earlier iterations (Table X).

### Model Construction

The Model Construction phase exhibited the highest number and diversity of QRPs $(n=15)$, with practices occurring across multiple sub-phases including Model Selection, Model Specification, Model Tuning, and Method Selection with particularly concerning practices around model specification and selection.

*Model dredging* emerged as a prominant concern, characterised by unconstrained exploration of model space without theoretical guidance or consideration of biological plausibiliy [@Shmueli2010]. As MacNally [-@MacNally2000] observed:

> "Ecologists and conservation biologists too often measure almost everything they can (especially in these days of automated probes and data loggers, remote sensing and GIS) and hope that something important will by 'fished out' of the resultant murky data."

25 years later the "kitchen sink" or "causal salad" approach is still a common approach to ecological modelling [@Franks2025]. @Fourcade2018's simulation analysis provides a pedagogical example of the consequences of this practice, with paintings of the Mona Lisa successfully predicting species' distributions and being selected over models with plausible predictors when coupled with methodologically flawed use of model performance metrics. 

*Overfitting* was identified as another critical issue, manifesting in multiple forms including the inclusion of excessive predictors relative to dataset size and complexity. Nakagawa et al. [-@Nakagawa2017] highlighted the prevalence of this practice, and the consequences are severe when the modelling purpose is to generate anticipatory predictions to inform decisions because overfitted models lose predictive power and generalisability to new datasets and generate spurious results [@lewis2023]. Overfitting is of particular concern in ecology where datasets are small and noisy (ref), heightening the risk of spurious results. 

*S-hacking* practices in this phase included changing model specifications after observing test set performance and manipulating random seeds to improve results. @Liu2020 noted that manipulating random seeds during model tuning could drastically alter model results, underscoring that seemingly minor technical decisions can be exploited to inflate performance. S-hacking artificially inflate model performance, resulting in spuriously selected models that that may not reflect genuine predictive relationships.

Several QRPs revealed systematic biases towards confirming expectations rather than rigorous testing. *Selective debugging* exemplifies the problem of confirmation bias,  where error checks during computational implementation of the modelling tend to occur only or mostly when results deviate from expectation, creating systematic bias because any silent errors when model results align with expectations are cursory, such that errors will be uncorrected [@Risbey2005].

### Model Evaluation

The model evaluation phase contained 14 distinct QRPs, with *selective reporting* and *executing alternative analyses* being particularly prevalent. *S-hacking* occurs when these practices are combined, for example a modeller may trial multiple evaluation metrics, selectively reporting those that present the model in the most favourable light [@Hildebrandt2018]. For models of systems that are difficult to interpret, such as ecological systems, high accuracy may not be meaningful, with the model being inaccurate despite a high measure accuracy [@Hildebrandt2018]. This problem is compounded further in ecological modelling, for example, because ecological models are rarely tested on independent data due to data constraints (ref). Evaluating models only on training data or 'resubstitution' is problematic because it leads to biased estimates of performance, insofar as the model is fitted to regular and irregular features of the sampled data, but is unable to distinguish between them [@Pu2018].

The potential for data leakage during this phase emerged as a second critical concern, particularly the practice of continuing to optimise models after already evaluating it on the test set ('*overhyping*'). Overhyping occurs in tandem with other post-hoc modifications described previously, for example, trialling alternative hyperparameter settings in cross-validation or systematically trialling different feature combinations on performance until test performance was improved [@hofman2023]. Alarmingly, interviews with modellers reveals that these practices are routine in the machine learning community [@hofman2023] (check this ref, P1 vs P2).

### Model Application

The Model Application phase showed fewer but distinct QRP types focused on the misrepresentation of model capabilities and findings. Overhyping involves claims about the models' performance that are not substantiated by model evaluation results, such as claiming the model has greater generalisability than it does. A specific form of overhyping involves misreporting correlative claims using causal language, which is particularly common in studies of conservation intervention evaluations using observational study designs [@Josefsson2020]. The practice of implying causation from correlation can cause false confidence in the intervention's effectiveness while ignoring the real reasons for the observed effect. 

# Discussion

Far from being irrelevant to model-based research, researcher degrees of freedom are abound in the modelling process, providing ample opportunity for questionable research practices to operate. While some QRPs in modelling are analogous to those in hypothesis-testing research, such as S-hacking, some are specific to modelling. Regardless, all QRPs are motivated by external pressures related to the apparent publishability of the analysis. The iterative modelling process and the need to make data-based decisions during modelling has resulted in significant resistance to the idea of QRPs in model-based research, we explore this tension further here to clarify the distinction between legitimate and opportunistic data-dependent decisions, highlighting where the concept of QRPs apply and where they do not. 

## A mathematical formalism of the garden of forking paths for model-based research

QRPs identified in our roadmap illustrate the two-axes of our typology of QRPs. During model construction, many QRPs directly alter the model itself, $C (\mathbf{X}, M;\phi (\mathbf{X},M))$, for example model dredging in model construction, where model selections are data-dependent, overfitting practices that modify the model specification $M_s$ based on observed performance, and S-hacking, involving changes to the model structure after seeing results. QRPs also targeted model outputs, without necessarily changing the underlying model $C(\mathbf{X}, M;{\phi}_{{M}_{j}})$, for example, selective reporting of performance metrics, trialling different model evaluation metrics, and changing model output thresholds or transformations during model application.

The prevalence of QRPs in Model Construction (affecting the transition from $M_s$ to $M_p$) and Model Evaluation (affecting the assessment of $Mp$ fitness for purpose) suggests these are the critical junctures where $\phi (\mathbf{X},M)$ problematic decision-making occurs most frequently. Practices like model dredging and overfitting during Model Construction exemplify how researchers make data-dependent choices about model structure and complexity after observing initial results, directly contradicting the idealised practice of pre-specified analytic decisions represented by $C(\mathbf{X}, M;\phi )$. Similarly, the abundance of QRPs in Model Evaluation, where researchers assess $M_p$ fitness for purpose, demonstrates how the iterative nature of model validation creates multiple opportunities for post-hoc optimisation of apparent model performance.

Practices targeting the model itself $C (\mathbf{X}, M;\phi (\mathbf{X},M))$ were prevalent throughout Model Construction, including model dredging, overfitting, and specification changes made after observing results. These practices directly alter the realised model, changing what would have been observed if the questionable practice had not occurred. Conversely, practices targeting model outputs $C (\mathbf{X}, M;{\phi}_{{M}_{j}} (\mathbf{X},M))$ were concentrated in Model Evaluation and Application phases, including selective reporting of performance metrics, manipulation of thresholds in model performance metrics. These practices leave the underlying model unchanged while altering perceptions of its performance or credibility.

The distinction between direct manipulation and selective reporting—central to our typology—was also reflected in our QRP Roadmaps. Model Evaluation phases showed both types: direct manipulation of model performance through threshold changes and output transformations $C(\mathbf{X}, M;{\phi}_{{M}_{j}} (\mathbf{X},M))$ , and selective reporting through cherry-picking of favourable performance metrics $C(\mathbf{X}, M;{{\phi}_{{M}_{j}}}^{best} (\mathbf{X},M))$. This exemplifies the contention in our formalism that QRPs can affect both the actual models and model outputs as well as the attributes that determine perceived model credibility. 

As described in our framework, analytic uncertainty propagates combinatorially along each decision-point in the modelling process. The identification of QRPs that span multiple phases of the modelling workflow exemplifies our mathematical formalism $C (\mathbf{X}, M;\phi (\mathbf{X},M))$, capturing not just isolated questionable decisions, but interconnected sequences of data-dependent choices that collectively bias the final model and its perceived credibility. Sample curation, S-hacking, and overfitting practices demonstrate how questionable practices conducted early in the process can cascade through subsequent phases, with bias propagating along the realised modelling path, and the effects of questionable decisions compounding across the modelling process.  For instance, post-hoc data exclusions during the data curation and preparation phase can inflate apparent model performance leading to unwarranted claims about the model performance and generating unreliable findings during model application. This pattern highlights that the proliferation of questionable practices follows the same combinatorial logic that characterises analytical flexibility in traditional hypothesis testing.

## Implications for Model-Based Research Integrity 

The empirical patterns validate our emphasis on model credibility rather than statistical significance as the target of QRPs in modelling contexts. Many identified practices—particularly overhyping claims and misreporting correlative findings with causal language—specifically target perceptions of model reliability, accuracy, and generalisability rather than traditional statistical thresholds. This supports our model-centric definition of QRPs as practices that "artificially inflate the apparent accuracy or precision of a model, its predictions, and/or evaluation tests." The database demonstrates that the mathematical formalism successfully captures the diverse ways researchers can manipulate both technical model properties and broader perceptions of model fitness for purpose, providing a comprehensive framework for understanding questionable practices in model-based research.

QRPs collectively undermine the reliability and reproducibility of ecological modelling research in several ways:

1. **Inflated performance estimates** that do not reflect true performance.
2. **Spurious model selection** that identifies models based on chance rather than plausible biological mechanisms or predictive relationships.
3. **Reduced reproducibility** due to undisclosed researcher degrees of freedom.
4. **Compromised generalisability** from overfitted models that fail to transfer to new contexts or make accurate forecasts.
5. **False confidence** in ecological understanding and management recommendations.

The prevalence of these practices suggests systematic issues in training, incentives, and quality control within the ecological modelling community. The concentration of QRPs in the model construction and evaluation phases indicates particular vulnerabilities in how models are specified, fitted, and evaluated. The large number of decision-points where researchers can exercise degrees of freedom suggest that interventions and safeguards should target these critical phases of the modelling process.

## Distinguishing "poor" from "questionable" practices

Our analysis revealed the context-dependent nature of questionable research practices, between clear QRPs and "poor practices" that may not always constitute questionable behaviour. For example, 'resubstitution' or assessing model performance on the training set only, was classified as poor practice rather than as questionable when done due to data limitations rather than intentional manipulation. However, as our analysis highlighted, flawed practices have the potential to compound the effects of QRPs. In this example, failing to evaluate the model on independent data can provide the opportunity for QRPs like S-hacking to remain hidden. Similarly, the continued misuse of AUC as a measure of predictive performance can result in biologically implausible models being selected when combined with model dredging behaviours.

Clearly, context matters when it comes to delineating questionable from poor practice. We must consider constraints on feasibility, like data availability, and the stated model purpose (e.g. assessing generalisability on new data is important for predictive modelling, but less so for explanatory modelling). However, a shared feature of *all* QRPs is that the practices are undisclosed so that the potential for these practices to bias the model and its results are hidden. Transparency in reporting is essential for being able to properly evaluate the credibility of the model.

### Data-dependent modelling choices: when are they ‘questionable’ ?

Our formalism emphasised the data-dependency in analytic decision-making  $\phi (\mathbf{X},M)$ as a hallmark of QRPs. For instance, practices like "changing the model specification after observing model evaluation results" or "modifying exclusion criteria after fitting the model and observing results" exemplify the formulation $\phi (\mathbf{X},M)$ where choices depend on observed model states. 

However, we hesitate to designate all data-dependent analytic decisions as questionable research practices, as is implied in Gelman and Loken's (2013) framework. That is because there are situations in modelling where decisions are necessarily dependent on previous analytic decisions within the modelling workflow [@Liu2020]. We describe and illustrate these further in Chapter 6, but as an illustrative example, the choice about an appropriate model-fitting method to derive $M_p$ might depend on the specified model structure $M_s$. Thus, not all data-dependent analytic decisions are automatically ‘questionable’ within the context of modelling. 

@Liu2020 helpfully distinguish defensible from questionable motivations for engaging in data-dependent analytic decision-making through the dichotomy of _systematicity_ and _opportunism_, or the desire to find a particular result. In the case of model evaluation, systematicity can be distinguished from opportunism when an objective metric is used to evaluate outcomes, and the choice of metric does not hinge on anticipated conclusions. While during model construction, formal model selection procedures, wherein reasonable alternative models are systematically enumerated and implemented, can be characterised as ‘defensible’ data-analytic decision-making or ‘exploration’, on the grounds that alternative models are chosen based on objective metrics and values, such as goodness of fit, which are selected a priori (Liu, 2020). Or, if the outcome determining the modelling choice is a qualitative measure, a rule of thumb for choosing one alternative over another is articulated *a priori*.



## References {.unnumbered}

::: {#refs}
:::

::: landscape
```{r}
#| label: tbl-QRP
#| tbl-cap: "Full list of Questionable Research Practices identified from the literature. We categorised QRPs into broader classes"
#| column: body-outset
#| warning: false
#| message: false

gt_qrp <- tidy_QRP_database(here::here("data/QRP_database_2025-10-20.csv")) %>%
        make_qrp_table_data() %>% 
        select(-practice_notes) %>% 
        group_by(qrp_coded) %>% 
        arrange(qrp_coded) %>% 
        make_QRP_table() %>% 
        gt::fmt_markdown(columns = c(qrp_coded, qrp_description, source)) %>% 
        gt::cols_hide(columns = c(-starts_with("practice_"), -source, -practice_target) ) %>% 
        gt::cols_label(qrp_coded = "QRP Class") %>% 
        gtExtras::gt_theme_nytimes()

# gt_qrp


tidy_QRP_database(here::here("data", 
                             "QRP_database_2025-10-20.csv")) %>%
  select(-file_name, -description) %>% 
  mutate(across(c(starts_with("practice_"), -practice_target), 
                ~ str_c(.x, source))) %>% #annotate `practice_*` with `source`
  group_by(qrp_coded, qrp_description) %>%
  mutate(across(c(starts_with("model_"), 
                  "practice_target",
                  "qrp_reason", 
                  "practice_notes"), 
                ~ str_split(.x, ", ")),
         source = str_split(source, "; ")
         ) %>% 
  summarise(across(c(starts_with("model_"), 
                     "qrp_reason",
                     "practice_notes",
                     starts_with("practice_")), 
                   list)) %>% 
  rowwise() %>% 
  mutate(across(c(starts_with("model_"), 
                  "practice_target",  
                  "qrp_reason", 
                  "practice_notes"), 
                ~ flatten_chr(unique(.x)) %>% 
                  unique() %>% 
                  str_flatten_comma(na.rm = TRUE)),
         source = list(
                         str_remove_all(string = "source", pattern = c("\\[|\\]")) %>% 
                         str_squish() %>% 
                         unique() %>% 
                         str_flatten("; ") %>% 
                         str_c("[", ., "]")),
         across(c(starts_with("practice_"), -"practice_target"), 
                ~ str_squish(.x) %>% 
                         str_flatten(collapse = "<br>"))) %>% 
  gt::gt(groupname_col = c("qrp_coded", "qrp_description")) %>% 
  gt::cols_hide(source) %>% 
  # gt::cols_width(ends_with("_source") ~ gt::pct(20),
                 # qrp_reason ~ gt::pct(20)) %>%
  gtExtras::gt_theme_pff() %>% 
  gt::tab_options(table.font.size = 8) %>% 
  gt::fmt_markdown(columns = source)


```
:::


::: appendices
## Appendix 1: Modelling Process Literature Review {#sec-appendix-1}

## Appendix 2: QRP Roadmap Literature Review {#sec-appendix-2}

### Step 1 - Identify and collect QRPs

We unsystematically screened the published literature to generate an initial list of terms for QRPs in NHST research to guide search term selection in ecological modelling and related modelling fields. We used the following search terms to identify potential QRPs in different areas of ecological modelling:

-   "`modelling_area` AND type I error"
-   "`modelling_area` AND false positive"
-   "`modelling_area` AND modelling choice"
-   "`modelling_area` AND subjective judgment"
-   "`modelling_area` AND prediction error"
-   "`modelling_area` AND confirmation bias"
-   "`modelling_area` AND publication bias"
-   "`modelling_area` AND questionable research practice"
-   "`modelling_area` AND researcher degrees of freedom"
-   "`modelling_area` AND cherry picking"
-   "`modelling_area` AND p-hacking"
-   "`modelling_area` AND HARKING"
-   "`modelling_area` AND bias"
-   "`modelling_area` AND good modelling practice"
-   "`modelling_area` AND best modelling practice"
-   "`modelling_area` AND bad modelling practice"
-   Where `modelling_area` included 'predictive modelling', 'habitat modelling', 'Species Distribution Modelling (SDM)', 'Ecological Niche Modelling', 'Ecological Modelling', 'Environmental Modelling'.

We inferred QRPs from practices described by authors with value judgements, such as "good" or "best practice," "bad" or "poor practice." For positively ascribed practices, we took the logical inverse of these practices as the QRP. We ignored perceived 'inconsequential' practices, and instead included practices that were commonly or routinely conducted and where authors argued strongly for the culture of research practice to change. We excluded QRPs that pertained to fraud, misconduct, or nefarious intent.

**Collating & Coding**

For each QRP identified, we collected a description of the research practice `practice_description`, the reason or justification for why the practice is 'questionable' `practice_reason`, including any quantitative and/or empirical evidence for: a) the negative consequences on research outcomes such as credibility, reliability, accuracy, precision, transparency, reproducibility and/or b) evidence for the use or occurrence of this practice; `practice_evidence`. We assigned each QRP to phases and sub-phases of the modelling process (section X) where the practice occurs. For each description, reason and evidence, we coded each into short descriptions of the practice `QRP_description`, reason for the practice's 'questionable' nature `QRP_reason`. Using the model phases and sub-phases identified in (Section X), we classified the location of the QRP in the modelling process, ascribing the `model_phase` and `model_subphase` in which the practice occurs. We then coded the `target` of the practice, i.e. the model object (Figure X) directly affected by the practice. Where mitigation measures or solutions to the practice were suggested alongside the description of the practice, we also coded the `practice_solution`.

**Categorising QRPs into Classes**

We adopt Nagy et al.'s [-@Nagy2025] approach and grouped QRPs consisting of the same family of research behaviours into broad classes of QRPs `QRP_coded`. Some umbrella terms were common QRPs in hypothesis testing research, some were hypothesis-testing analogues, while others were modelling-specific. Where possible we used existing umbrella terms used by Nagy et al. [-@Nagy2025] and others [e.g. @Liu2020], but created our own terms if no existing terms were applicable.

### *Step 3 - Refine QRP and QRP Class descriptions, aggregate QRPs*

We refined our coded descriptions of QRPs `QRP_description` into unifying or general descriptions effectively aggregating similar practices identified from different published sources. After categorising QRPs into broader classes of QRPs, we revised their description to be more modelling-specific, better reflecting the surveyed practices (Table X).

### QRP Code-Book / Metadata

-   [ ] TBD

## Session Info {.unnumbered}

```{r}
#| label: session-info

devtools::session_info()
```
:::

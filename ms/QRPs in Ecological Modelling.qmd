---
title: A Roadmap of Questionable Research Practices in Ecological Modelling
running-head: Questionable Research Practices in Ecological Modelling
author:
  - name: Elliot Gould
    email: elliot.gould@unimelb.edu.au
    orcid: 0000-0002-6585-538X
    url: https://github.com/egouldo
    corresponding: true
    affiliation:
      - ref: 1
  - name: Hannah S. Fraser
    orcid: 0000-0003-2443-4463
    affiliation:
      - ref: 1
  - name: Libby Rumpff
    orcid: 0000-0001-9400-8086
    affiliation:
     -  ref: 1
  - name: Fiona Fidler
    orcid: 0000-0002-2700-2562
    affiliation:
      - ref: 2
affiliations:
  - id: 1
    name: The University of Melbourne
    department: School of Agriculture, Food and Ecosystem Sciences
  - id: 2
    name: The University of Melbourne
    department: School of Historical and Philosophical Studies
format:
  preprint-typst:
    keep-md: true
    keep-typ: true
    css-property-processing: translate
    wordcount: true
    execute:
      echo: false
      message: false
      include-before-body:
      - text: |
          #show figure: set block(breakable: true)
    # Common options - uncomment (and remove 'default') to use
    # theme-jou: true              # Journal theme (2-column)
    # line-number: true            # Line numbering
    # fontsize: 11pt               # Font size
    # leading: 0.6em               # Line spacing
    # first-line-indent: 1.8em     # Paragraph indent
  html:
    page-layout: full
    title-block-banner: true
    code-fold: true
pdf-engine: xelatex
abstract: |
  1. Questionable research practices (QRPs), result in low rates of replicability and contribute to biased accounts of studied phenomena in the published literature, producing a literature full of apparently strong and conclusive results. Recent metaresearch has revealed that ecology is at risk of a ‘replicability crisis’ due to the presence of QRPs and a lack of transparency in published research. However, the work to date focusses on QRPs as they occur in hypothesis testing research which is not a good fit for use in ecology and related fields that conduct exploratory or predictive research using complex models. 
  2. In order to protect against bias caused by QRPs in ecology, it is essential to identify QRPs relevant to model-based research and understand why they occur, how frequent they are and how severe the consequences might be. In this paper we propose a conceptual framework for describing QRPs in ecological model-based research. 
  3. We describe a generalised workflow for model development, evaluation and analysis as it is typically undertaken in ecology and related fields. We develop a typology of QRPs that describes potential mechanisms for emergence of QRPs throughout the modelling process. From this conceptual framework, we derive a definition of QRPs that captures the specificities and features of the modelling process particular to ecology. Finally, we create a diagrammatic roadmap that map QRPs onto common decision-points in the modelling process, highlighting points of caution for modelers. 
  4. Our analysis and framework highlight that QRPs in the modelling process are a threat to the credibility of model-based research. QRPs may target the model itself, or the outputs of model analyses, including model checking, or testing and evaluation performed on the model.  Finally, our proposed definition of QRPs de-emphasises the idea of finding false positive results and instead points to other measures of model credibility that affect publishability, such as the accuracy and precision of the model and or its outputs. 
  5. In this paper we aim to raise awareness among modelers about the potential types of QRPs and their mechanisms for emergence in the modelling process. The roadmaps in this paper will help modelers to understand the risks of QRPs in their research so they are empowered to implement procedures and practices that can reduce the occurrence and impact of QRPs.
keywords: 
  - questionable research practices
  - ecological modelling
  - metaresearch
  - transparency
  - reproducibility
  - model credibility
  - researcher degrees of freedom
date: last-modified
bibliography: references.bib
---

```{r}
#| label: setup
#| warning: false
#| message: false
library(tidyverse)
library(here)
library(janitor)

library(gt)
library(gtExtras)

library(ggh4x)
library(ggforce)
library(waffle)
library(ggsci)
library(hrbrthemes)

library(extrafont)
library(firasans)
extrafont::loadfonts()

source(here::here("R/tidy_QRP_database.R"))
source(here::here("R/make_table_data.R"))
source(here::here("R/make_QRP_table.R"))
```

## Introduction

Self-report surveys of researchers’ statistical practices suggest high rates of Questionable Research Practices (QRPs e.g., p-hacking, cherry picking, see @tbl-QRP-examples-typst @QRP_tbl_html) in several different disciplines: psychology (ref), education (ref) and ecology [@Fraser:2018cl]. Widespread QRPs, accompanied by a lack of transparency and openness in research reporting [@culina2020] can leave disciplines at risk of a ‘reproducibility crisis’ [@Fidler:2017he] . To understand the implications of this for ecology, we need a more comprehensive account of what constitutes Questionable Research Practice. QRPs are a set of methodological and statistical practices that fall into an ‘ethical grey zone’ between ideal responsible research conduct and unacceptable research behaviours like fabrication and plagiarism [@Steneck2006] . When researchers are faced with ambiguous decisions where there is no single acceptable decision or strategy according to scientific standards and consensus, they are likely to select the decision that results in the most ‘favourable’ finding with “convincing self-justification” [@Hoffmann2021; @Simmons:2011iw]. QRPs may not be the product of conscious ‘fishing expeditions’ [@Gelman2013] but rather occur because researchers are motivated actors prone to cognitive biases incentivised by a ‘publish or perish’ research culture in which publication bias selects for particular types of findings [@Agnoli:2017kl]. These findings are generally significant results or ‘unexpected’ /surprising findings [@Nissen2016; @Nakagawa2025]. In NHST these are generally statistically significant findings, correlations rather than absence of correlations ‘unexpected’ findings (ref). Findings that are not deemed ‘publishable’ are either sent to the ‘file drawer’ or else subject to practices, such as ‘p-hacking,’ that make the results more likely to be published. Publication bias and the file drawer effect facilitate the dissemination of false knowledge, consequently, QRPs bias the literature and ultimately undermine the credibility and reproducibility of research findings [@Cockburn2020; @Agnoli:2017kl]. Current definitions of QRPs are focused on hypothesis testing research (specifically null-hypothesis significance testing), with QRPs typically defined as practices that inflate the chance of finding a false positive result (type I error) (ref? Simons maybe?). Common examples of QRPs are centred around practices that manipulate p-values. Whilst Null Hypothesis Significance Testing has been and still is in popular use in ecology [@Fidler:2017he; @Stephens2007], a large amount of applied problem-solving work in the field is model-based. Ecological modellers may be inclined to think that the concerns associated with QRPs and reproducibility do not apply to them. In practice, this work is quite different to null hypothesis significance testing: Instead of relying on a single quantitative measure, studies describing ecological models usually generate a suite of measures and analyses that pertain to different desirable attributes of the model and collectively inform claims about the model being sufficient to fulfil its intended purpose. This begs the question of whether QRPs are relevant to model-based research? If specific QRPs related to p-values, like p-hacking, don’t apply, are there parallel or counterpart practices that do? What form might they take? And where in the research process would we be likely to find them? This paper addresses each of these questions. Our primary aim in this paper is to highlight the range of particular decision-points and practices that could be questionable so as to identify potential QRPs in model-based research. We encourage modellers to pause at such junctions and assess for themselves what the relative risk of QRPs is.

```{r}
#| label: create-QRP-table
#| message: false
#| warning: false
#| echo: false

image_out <- function(x, height = 50){
  out <- map_chr(x, gt::local_image, height = height)
  out <- map(out, gt::html)
  out
}

QRP_tbl <- readr::read_csv(here::here("data", "tbl-QRP-frequency.csv"),
                show_col_types = FALSE) %>% 
  mutate(Category = here::here("data", "icons", 
                               glue::glue("{Category}.png"))) %>%
  gt::gt(groupname_col = "Category",
         row_group_as_column = TRUE,
         caption = 'Examples and self-reported frequency of questionable research practices in hypothesis-testing research. QRPs are categorised as "cherry-picking", "p-hacking", and "methodologically flawed", indicated by the cherry, saw, and cross icons respectively.') %>% 
  gt::tab_footnote(footnote = "Makel et al. (2019)", 
                   locations = cells_column_labels(columns = Abbreviation)) %>% 
  gt::tab_footnote(footnote = gt::md("Fraser et al. (2018), $n=494$"), 
                   locations = cells_column_labels(columns = 
                                                     c("Ecology", 
                                                       "Evolution"))) %>% 
  gt::text_transform(locations = gt::cells_row_groups(everything()),
                     fn = image_out
  ) %>% 
  gtExtras::gt_theme_nytimes() %>% 
gt::cols_align(align = "center", columns = "Category")


QRP_tbl_html <- QRP_tbl %>% 
  gtExtras::gt_plt_bar_pct(
    fill = "purple",
    column = c("Ecology"),
    scaled = TRUE,
    width = 100,
    labels = TRUE,
    background = "lightgrey"
  ) %>% 
          gtExtras::gt_plt_bar_pct(
    fill = "purple",
    column = c("Evolution"),
    scaled = TRUE,
    width = 100,
    labels = TRUE,
    background = "lightgrey"
  ) %>% 
  gt::cols_width(
    Category ~ gt::pct(10),
    `Questionable Research Practice` ~ gt::pct(30),
    Abbreviation ~ gt::pct(2),
    Ecology ~ gt::pct(20),
    Evolution ~ gt::pct(20)
  ) %>% 
  gt::tab_options(quarto.disable_processing = FALSE) 

```

::::: landscape
::: {.content-visible when-format="html"}
```{r}
#| label: tbl-QRP-examples-html
#| column: body
QRP_tbl_html
```
:::

::: {.content-visible when-format="typst"}
```{r}
#| label: tbl-QRP-examples-typst
#| column: body
QRP_tbl %>% 
  gt::fmt_percent(
    columns = Ecology,
    scale_values = FALSE,
    decimals = 0
  )
```
:::
:::::

### Why QRPs in Hypothesis Testing don’t directly translate to modelling

One principle underlying the classification of these practices as ‘questionable’ relates to data driven decision making. In contrast to hypothesis testers, modellers makes a series of analytic decisions on the basis of both objectively identified model performance criteria and subjective judgements [@Babel2019; @Bennett2013], wherein which they shift between subjecting the model to analysis, validation and testing, and refining the model in response to those results [@Getz2017]. This process is non-linear, iterative, and generates many interim versions of the model preceding publication [@Augusiak2014], unfolding much like the ‘survival of the fittest’ model(s) [@Prosperi2019] given a specified purpose and the available resources. In modelling, analytic decision-making is adaptive and data-dependent. The emphasis of existing QRP definitions on type I errors is unhelpful for model-based research. Firstly, multiple sources and types of error may arise in the modelling process - there is model structural uncertainty, uncertainty in parameter estimates, and uncertainty in predictions. What constitutes an ‘error’, what the source of that error is, as well as the relative weighting of those different errors, depend on some combination of the model’s purpose (prediction, explanation, description), the type of model used (phenomenological or mechanistic) and the context for the model. Rather than relying on p-value as a metric, model performance metrics consist of both qualitative and quantitative measures that incrementally build a subjective picture of model credibility rather than providing dichotomous responses assessing model credibility [@Augusiak2014; @Hamilton2019]. Since the publishability of a model-based study hinges on the collection of these model outputs and their contribution to overall model credibility, each of these model outputs may be susceptible to questionable research practices in order to strategically alter the perceived credibility of the model. What can be ‘tweaked’, ‘hacked’ or ‘exploited’ in hypothesis testing are *p*-values and whilst there may not be a direct counterpart in model-based research, we do expect similar a similar motivation from publishability to apply.



## A Conceptual Framework of Questionable Research Practices in Ecological Modelling

## A Roadmap of QRPs in the Ecological Modelling Process

### Method

We surveyed the modelling literature to identify potential QRPs in ecological modelling and their location in the modelling process. We categorised QRPs into broader classes corresponding to families of similar practices using well-known published classifications (e.g. @tbl-QRP-examples-typst ) and adopted new classes when there was no NHST-analogue. We coded the phase and sub-phases of modelling in which the practice occurs, as well as the model object (input, model, output) directly affected by the QRP, i.e. the *target* of the practice. After initial coding of the QRPs we refined the practice descriptions to encapsulate similar practices, as well as the broader QRP classes into which practices were grouped. Our coding is described in further detail in @sec-appendix-2.


```{r}
#| label: waffle_data_prep
#| message: false
#| warning: false
waffle_plot_data <- 
        tidy_QRP_database(here::here("data", "QRP_database.csv")) %>% 
        filter(include) %>% 
        select(-notes, -model_subphase, -source, -reason_coded, -include, -ends_with("_source")) %>% 
        distinct() %>% #rm duplicate practice_coded #TODO next merge duplicates while keeping source
        drop_na() %>% #interim approach until datachecks in place
        mutate(model_phase = str_split(model_phase, ", "),
               target = str_split(target, ", "),
               values = 1) %>% 
        unnest(model_phase) %>% 
        unnest(target) %>% 
        complete(practice_coded, 
                 model_phase, 
                 target, 
                 fill = list(values = 0.0001) # see https://github.com/hrbrmstr/waffle/issues/66
                 ) %>%  
        group_by(practice_coded) %>% 
        fill(qrp_coded, model_phase, .direction = "downup") %>% 
        ungroup()
```

```{r}
#| label: tbl-QRP-categories
#| tbl-cap: "Classes of Questionable Research Practices (QRPs) in ecological modelling."
#| echo: false

qrp_class_descriptions <- tribble(~ qrp_coded, ~ description,
        "S-hacking", "S- or *statistic* hacking: Manipulating model inputs, outputs or the model itself to obtain a favourable value of a performance measure, usually by systematically altering the modelling until a desired level of a statistic or metric is obtained [@Nakagawa2025; @Nagy2025]. This can include changing random seeds, outcome variable and/or performance thresholds, or performance metrics. Likely to occur when there are threshold dependent binary (or even ordinal) outcomes that are linked to whether the model will be publishable or not, in such instances S-hacking is analagous to *p*-hacking. Examples of threshold-dependent performance measures include partial ROC, true skill statistic (TSS), sensitivity and specificity [@feng2019]. <br>When there are multiple criteria informing model performance or multiple model performance tests, the relative weighting may be changed after observing results to make the model appear more credible [@Benett2013] (Or is this over-hyping).",
        "Overhyping", "Exaggerating or overstating the capabilities, generalisability, or reliability of a model beyond what is justified by the evidence. This can involve overstating a model's real-world applicability or predictive power.",
        "Sample Curation", "Based on Nagy et al. [-@Nagy2025]. Selectively including, excluding, or modifying data points in the sample used to develop or evaluate a model, often to improve apparent model performance. This can include practices like cherry-picking favourable data or arbitrarily removing outliers.",
        "Model Dredging", "Searching through a large number of possible model specifications or variable combinations without theoretical justification, in an attempt to find favourable results. This is sometimes referred to as 'fishing expeditions' in model space.",
        "Selective Debugging", "The practice of only checking for and correcting errors in a model when results are unexpected or unfavourable, rather than performing systematic error checking regardless of outcomes.",
        "Overfitting", "Developing a model that fits the training data too closely, including noise and peculiarities specific to that dataset, resulting in poor generalisability to new data. This often occurs when models are made overly complex relative to the amount of training data available.",
        "Executing Alternative Analyses", "Conducting multiple different analyses or model variations and selectively reporting only those that yield favourable results, without disclosing the full range of analyses performed.",
        "Selective Reporting", "Also termed *cherry-picking*. Choosing to report only certain results, metrics, or comparisons that support desired conclusions while omitting less favourable findings. This can create a misleading impression of model performance or study outcomes.")

waffle_plot_data %>% 
  distinct(qrp_coded) %>% 
  arrange(qrp_coded) %>% 
  left_join(qrp_class_descriptions, by = join_by(qrp_coded)) %>% 
  gt::gt() %>% 
  gt::cols_label(qrp_coded = "QRP Class") %>% 
  gtExtras::gt_theme_nytimes() %>% 
  gt::cols_width(
    qrp_coded ~ gt::pct(15),
    description ~ gt::pct(85)
  ) %>% 
  gt::fmt_markdown(columns = description) 

```

### Roadmap

We summarise a subset of QRPs, their location in the modelling process, and their target modelling object in @fig-waffle_plot, highlighting a mixture of common practices, or practices that have the potential for critically influencing the perceived credibility of a model and its publishability. The complete set of QRPs identified is presented in @tbl-QRP.

We grouped QRPs into broader classes described in @tbl-QRP-categories. We applied some umbrella terms identified by Nagy et al. [@Nagy2025] and Liu et al. [-@Liu2020], including: "Sample Curation", "Selective Reporting" or "Cherry-picking," "Overhyping" and "Executing Alternative Analyses." We used additional terms for groups of practices without an existing umbrella term, e.g. 'Overfitting' and 'Model Dredging', 'Selective Debugging'. In the same vein as Nagy et al.'s [-@Nagy2025] expansion of the concept of *p*-hacking to encompass other analysis metrics, we expanded the concept of *p*-hacking and renamed it "S-hacking", or statistic-hacking, encapsulating analogous practices that occur in modelling.

HARKing (*hypothesising after results are known*) is a well-known type of QRP, but was not explicitly included in our final classification of QRPs. However, some practices, like Overhyping, could be considered a form of HARKing.

Although overfitting is a common outcome of many QRPs, especially S-hacking and Sample Curation (see also @tbl-QRP for descriptions of the effects or impacts of each practice), we included Overfitting as a distinct class of QRPs because overfitting may also be the primary goal of the practice. Overfitting is like S-hacking but perhaps is less constrained in practice, because the 'model space' is less determined by a specific set of performance measures / statistics, whereas practices that specifically aim to 'overfit' the model to the calibration data operate on a much larger decision-space, so the risk of a spurious finding is greater.

Similarly, Sample Curation includes practices that directly affect or target input data, but the intent of the practice might be to S-hack. For instance, Optional Stopping rules directly affect the input data, but indirectly target some performance measure by continuing to collect and update the data until some desirable threshold in the performance measure is met.

We categorised QRPs according to the model object they directly target, i.e. the model inputs, the model itself, or the model outputs, corresponding to the the model objects in Figure X. 

We also grouped QRPs according to the phase of the modelling process in which they occur, i.e. Data, Model Construction, Model Evaluation and Model Application. Some classes of QRPs can occur in multiple phases of the modelling process, e.g. Sample Curation can occur in both Data and Model Construction phases, and Overfitting can occur in Model Construction and Model Evaluation phases while S-hacking can occur during Model Construction, Evaluation and Application. More narrowly defined classes of QRPs are restricted to one phase, e.g. Selective Debugging occurs only in Model Evaluation, or one type of target, e.g. Overfitting and Model Dredging target the model itself. Sometimes the QRP might occur in a second iteration of the phase, e.g. after model evaluation, but during model specification, i.e. the model specification is revised based on observing model evaluation results. 

```{r}
#| label: fig-waffle_plot
#| fig-cap: "Summary of Questionable Research Practices (QRPs) in ecological modelling. QRPs may target model inputs (yellow squares), the model itself (red squares), and/or model outputs (green squares), and may occur at different phases in the modelling cycle. QRPs are grouped according to broader classes defined in @tbl-QRP-categories. See @tbl-QRP for the full list of QRPs we identified."
#| fig-width: 14
#| fig-height: 18

# #| dev: "cairo_pdf" # when output is pdf
# options(hrbrthemes.loadfonts = TRUE)
waffle_plot <- waffle_plot_data %>% 
        distinct() %>% #TODO identify duplicated rows - suspect non-unique target coding is cause
        mutate(model_phase = 
                       forcats::as_factor(model_phase) %>% 
                       forcats::fct_relevel( c("Data", 
                                               "Model Construction", 
                                               "Model Evaluation",
                                               "Model Application"))) %>% 
        arrange(model_phase, 
                qrp_coded) %>% 
        ggplot(aes(fill = target, 
                   values = values)) +
        geom_waffle(size = 2, 
                    color = "white", 
                    make_proportional = FALSE, 
                    flip = TRUE) +
        scale_x_discrete(expand = c(0,0)) +
        scale_y_discrete(expand = c(0,0)) +
        ggsci::scale_fill_futurama(name = "QRP Target") +
        coord_equal() +
        facet_nested(qrp_coded + practice_coded ~ model_phase, 
                     switch = "y", 
                     labeller = 
                             labeller(
                                     practice_coded =
                                             label_wrap_gen(width = 80,
                                                            multi_line = TRUE),
                                     qrp_coded =
                                             label_wrap_gen(width = 10,
                                                            multi_line = TRUE),
                                     model_phase = 
                                             label_wrap_gen(width = 10, 
                                                            multi_line = TRUE)),
                     nest_line = element_line(linetype = 1),
                     solo_line = TRUE,
                     space = "free",
                     strip = strip_nested(size = "variable",
                                          clip = "off")
        ) +
        # facet_grid(practice_coded ~ model_phase, 
        #            switch = "y", 
        #            labeller = labeller(practice_coded = 
        #                                        label_wrap_gen(width = 100))) +
        # theme_no_axes() +
        hrbrthemes::theme_ipsum_rc() +
        waffle::theme_enhance_waffle() +
        theme(strip.text.y.left = element_text(angle = 0, 
                                               size = 12),
              strip.text.x = element_text(size = 14,vjust = 0),
              strip.background.x = element_part_rect(side = "b"),
              strip.background.y = element_blank()
        ) +
        ggh4x::force_panelsizes(rows = unit(1, "cm"), 
                                cols = unit(3, "cm"))

print(waffle_plot)



#DONE reorder the model process levels
#TODO exclude less important QRPs
#DONE Make strips pretty
#DONE recolour model object colours
#DONE group the QRPs into their umbrella terms -- nested facets?

```

## Takeaway messages

Although the QRPs we identified are not exhaustive, they do represent a range of practices that can occur in model-based research. Just as QRPs threaten the credibility and reliability of NHST research, so too do they threaten the credibility of model-based research. Similarly to [@Liu2020], we found that QRPs can occur at any point in the modelling process and may affect different model objects, i.e. the model inputs, the model itself, or the model outputs. Except for Overfitting & Model Dredging, all classes of QRPs have analogous practices under NHST, but the practices themselves are not directly comparable. We expanded the concept of *p*-hacking to include practices that target model performance measures, and renamed it S-hacking, or statistic-hacking. In essence, any performance metric, with a threshold dependent outcome (e.g. TSS, partial ROC, sensitivity, specificity) will be subject to the same types of practices as p-hacking.

# Discussion

## References {.unnumbered}

::: {#refs}
:::

::: landscape
`#show figure: set block(breakable: true)`{=typst}

```{r}
#| label: tbl-QRP
#| tbl-cap: "Full list of Questionable Research Practices identified from the literature. We categorised QRPs into broader classes"
#| column: body-outset
#| warning: false
#| message: false

gt_qrp <- tidy_QRP_database(here::here("data", "QRP_database.csv")) %>% 
        make_qrp_table_data() %>% 
        select(-notes) %>% 
        group_by(qrp_coded) %>% 
        arrange(qrp_coded) %>% 
        make_QRP_table() %>% 
        gt::fmt_markdown(columns = c(qrp_coded, practice_coded, source)) %>% 
        gt::cols_hide(columns = c(-ends_with("coded"), -source, -target) ) %>% 
        gt::cols_label(qrp_coded = "QRP Class") %>% 
        gtExtras::gt_theme_nytimes()

# gt_qrp


tidy_QRP_database(here::here("data", 
                             "QRP_database.csv")) %>%
  mutate(across(ends_with("_source"), 
                ~ str_c(.x, source))) %>% #annotate `*_source` with `source`
  group_by(qrp_coded, practice_coded) %>%
  mutate(across(c(starts_with("model_"), 
                  "target",
                  "reason_coded", 
                  "notes"), 
                ~ str_split(.x, ", ")),
         source = str_split(source, "; ")
         ) %>% 
  summarise(across(c(starts_with("model_"), 
                     "target",
                     "reason_coded",
                     "notes",
                     ends_with("source")), 
                   list)) %>% 
  rowwise() %>% 
  mutate(across(c(starts_with("model_"), 
                  "target",  
                  "reason_coded", 
                  "notes"), 
                ~ flatten_chr(unique(.x)) %>% 
                  unique() %>% 
                  str_flatten_comma(na.rm = TRUE)),
         source = list(source %>% 
                         str_remove_all(pattern = c("\\[|\\]")) %>% 
                         str_squish() %>% 
                         unique() %>% 
                         str_flatten("; ") %>% 
                         str_c("[", ., "]")),
         across(ends_with("_source"), 
                ~ str_squish(.x) %>% 
                         str_flatten(collapse = "<br>"))) %>% 
  gt::gt(groupname_col = c("qrp_coded", "practice_coded")) %>% 
  gt::cols_hide(source) %>% 
  # gt::cols_width(ends_with("_source") ~ gt::pct(20),
                 # reason_coded ~ gt::pct(20)) %>%
  gtExtras::gt_theme_pff() %>% 
  gt::tab_options(table.font.size = 8) %>% 
  gt::fmt_markdown(columns = source)


```
:::

::: appendices

`#set heading(numbering: "1.")`{=typst}

## Appendix 1: Modelling Process Literature Review {#sec-appendix-1}

## Appendix 2: QRP Roadmap Literature Review {#sec-appendix-2}

### Step 1 - Identify and collect QRPs

We unsystematically screened the published literature to generate an initial list of terms for QRPs in NHST research to guide search term selection in ecological modelling and related modelling fields. We used the following search terms to identify potential QRPs in different areas of ecological modelling: 
- "`modelling_area` AND type I error" 
- "`modelling_area` AND false positive" 
- "`modelling_area` AND modelling choice" 
- "`modelling_area` AND subjective judgment" 
- "`modelling_area` AND prediction error" 
- "`modelling_area` AND confirmation bias" - "`modelling_area` AND publication bias" 
- "`modelling_area` AND questionable research practice" - "`modelling_area` AND researcher degrees of freedom" 
- "`modelling_area` AND cherry picking" 
- "`modelling_area` AND p-hacking" - "`modelling_area` AND HARKING" 
- "`modelling_area` AND bias"
- "`modelling_area` AND good modelling practice" - "`modelling_area` AND best modelling practice" 
- "`modelling_area` AND bad modelling practice" 
- Where `modelling_area` included 'predictive modelling', 'habitat modelling', 'Species Distribution Modelling (SDM)', 'Ecological Niche Modelling', 'Ecological Modelling', 'Environmental Modelling'.

We inferred QRPs from practices described by authors with value judgements, such as "good" or "best practice," "bad" or "poor practice." For positively ascribed practices, we took the logical inverse of these practices as the QRP. We ignored perceived 'inconsequential' practices, and instead included practices that were commonly or routinely conducted and where authors argued strongly for the culture of research practice to change. We excluded QRPs that pertained to fraud, misconduct, or nefarious intent.

**Collating & Coding**

For each QRP identified, we collected a description of the research practice `practice_description`, the reason or justification for why the practice is 'questionable' `practice_reason`, including any quantitative and/or empirical evidence for: a) the negative consequences on research outcomes such as credibility, reliability, accuracy, precision, transparency, reproducibility and/or b) evidence for the use or occurrence of this practice; `practice_evidence`. We assigned each QRP to phases and sub-phases of the modelling process (section X) where the practice occurs. For each description, reason and evidence, we coded each into short descriptions of the practice `QRP_description`, reason for the practice's 'questionable' nature `QRP_reason`. Using the model phases and sub-phases identified in (Section X), we classified the location of the QRP in the modelling process, ascribing the `model_phase` and `model_subphase` in which the practice occurs. We then coded the `target` of the practice, i.e. the model object (Figure X) directly affected by the practice. Where mitigation measures or solutions to the practice were suggested alongside the description of the practice, we also coded the `practice_solution`.

**Categorising QRPs into Classes**

We adopt Nagy et al.'s [-@Nagy2025] approach and grouped QRPs consisting of the same family of research behaviours into broad classes of QRPs `QRP_coded`. Some umbrella terms were common QRPs in hypothesis testing research, some were hypothesis-testing analogues, while others were modelling-specific. Where possible we used existing umbrella terms used by Nagy et al. [-@Nagy2025] and others [e.g. @Liu2020], but created our own terms if no existing terms were applicable.

### *Step 3 - Refine QRP and QRP Class descriptions, aggregate QRPs*

We refined our coded descriptions of QRPs `QRP_description` into unifying or general descriptions effectively aggregating similar practices identified from different published sources. After categorising QRPs into broader classes of QRPs, we revised their description to be more modelling-specific, better reflecting the surveyed practices (Table X).

### QRP Code-Book / Metadata

-   [ ] TBD

## Session Info {.unnumbered}

```{r}
#| label: session-info

devtools::session_info()
```
:::

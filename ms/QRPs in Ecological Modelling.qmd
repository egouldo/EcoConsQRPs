---
title: A Roadmap of Questionable Research Practices in Ecological Modelling
running-head: Questionable Research Practices in Ecological Modelling
author:
  - name: Elliot Gould
    email: elliot.gould@unimelb.edu.au
    orcid: 0000-0002-6585-538X
    url: https://github.com/egouldo
    corresponding: true
    affiliation:
      - ref: 1
  - name: Hannah S. Fraser
    orcid: 0000-0003-2443-4463
    affiliation:
      - ref: 1
  - name: Libby Rumpff
    orcid: 0000-0001-9400-8086
    affiliation:
     -  ref: 1
  - name: Fiona Fidler
    orcid: 0000-0002-2700-2562
    affiliation:
      - ref: 2
affiliations:
  - id: 1
    name: The University of Melbourne
    department: School of Agriculture, Food and Ecosystem Sciences
  - id: 2
    name: The University of Melbourne
    department: School of Historical and Philosophical Studies
format:
  preprint-typst:
    keep-md: true
    keep-typ: true
    css-property-processing: translate
    wordcount: true
    execute:
      echo: false
      message: false
      include-before-body:
      - text: |
          #show figure: set block(breakable: true)
    # Common options - uncomment (and remove 'default') to use
    # theme-jou: true              # Journal theme (2-column)
    # line-number: true            # Line numbering
    # fontsize: 11pt               # Font size
    # leading: 0.6em               # Line spacing
    # first-line-indent: 1.8em     # Paragraph indent
  html:
    page-layout: full
    title-block-banner: true
    code-fold: true
pdf-engine: xelatex
abstract: |
  1. Questionable research practices (QRPs), result in low rates of replicability and contribute to biased accounts of studied phenomena in the published literature, producing a literature full of apparently strong and conclusive results. Recent metaresearch has revealed that ecology is at risk of a ‘replicability crisis’ due to the presence of QRPs and a lack of transparency in published research. However, the work to date focusses on QRPs as they occur in hypothesis testing research which is not a good fit for use in ecology and related fields that conduct exploratory or predictive research using complex models. 
  2. In order to protect against bias caused by QRPs in ecology, it is essential to identify QRPs relevant to model-based research and understand why they occur, how frequent they are and how severe the consequences might be. In this paper we propose a conceptual framework for describing QRPs in ecological model-based research. 
  3. We describe a generalised workflow for model development, evaluation and analysis as it is typically undertaken in ecology and related fields. We develop a typology of QRPs that describes potential mechanisms for emergence of QRPs throughout the modelling process. From this conceptual framework, we derive a definition of QRPs that captures the specificities and features of the modelling process particular to ecology. Finally, we create a diagrammatic roadmap that map QRPs onto common decision-points in the modelling process, highlighting points of caution for modelers. 
  4. Our analysis and framework highlight that QRPs in the modelling process are a threat to the credibility of model-based research. QRPs may target the model itself, or the outputs of model analyses, including model checking, or testing and evaluation performed on the model.  Finally, our proposed definition of QRPs de-emphasises the idea of finding false positive results and instead points to other measures of model credibility that affect publishability, such as the accuracy and precision of the model and or its outputs. 
  5. In this paper we aim to raise awareness among modelers about the potential types of QRPs and their mechanisms for emergence in the modelling process. The roadmaps in this paper will help modelers to understand the risks of QRPs in their research so they are empowered to implement procedures and practices that can reduce the occurrence and impact of QRPs.
keywords: 
  - questionable research practices
  - ecological modelling
  - metaresearch
  - transparency
  - reproducibility
  - model credibility
  - researcher degrees of freedom
date: last-modified
bibliography: references.bib
---

```{r}
#| label: setup
#| warning: false
#| message: false
library(tidyverse)
library(here)
library(janitor)

library(gt)
library(gtExtras)

library(ggh4x)
library(ggforce)
library(waffle)
library(ggsci)
library(hrbrthemes)

library(extrafont)
library(firasans)
extrafont::loadfonts()

source(here::here("R/tidy_QRP_database.R"))
source(here::here("R/make_table_data.R"))
source(here::here("R/make_QRP_table.R"))
```

## Introduction

Self-report surveys of researchers’ statistical practices suggest high rates of Questionable Research Practices (QRPs e.g., p-hacking, cherry picking, see @tbl-QRP-examples-typst @QRP_tbl_html) in several different disciplines: psychology (ref), education (ref) and ecology [@Fraser:2018cl]. Widespread QRPs, accompanied by a lack of transparency and openness in research reporting [@culina2020] can leave disciplines at risk of a ‘reproducibility crisis’ [@Fidler:2017he] . To understand the implications of this for ecology, we need a more comprehensive account of what constitutes Questionable Research Practice. QRPs are a set of methodological and statistical practices that fall into an ‘ethical grey zone’ between ideal responsible research conduct and unacceptable research behaviours like fabrication and plagiarism [@Steneck2006] . When researchers are faced with ambiguous decisions where there is no single acceptable decision or strategy according to scientific standards and consensus, they are likely to select the decision that results in the most ‘favourable’ finding with “convincing self-justification” [@Hoffmann2021; @Simmons:2011iw]. QRPs may not be the product of conscious ‘fishing expeditions’ [@Gelman2013] but rather occur because researchers are motivated actors prone to cognitive biases incentivised by a ‘publish or perish’ research culture in which publication bias selects for particular types of findings [@Agnoli:2017kl]. These findings are generally significant results or ‘unexpected’ /surprising findings [@Nissen2016; @Nakagawa2025]. In NHST these are generally statistically significant findings, correlations rather than absence of correlations ‘unexpected’ findings (ref). Findings that are not deemed ‘publishable’ are either sent to the ‘file drawer’ or else subject to practices, such as ‘p-hacking,’ that make the results more likely to be published. Publication bias and the file drawer effect facilitate the dissemination of false knowledge, consequently, QRPs bias the literature and ultimately undermine the credibility and reproducibility of research findings [@Cockburn2020; @Agnoli:2017kl]. Current definitions of QRPs are focused on hypothesis testing research (specifically null-hypothesis significance testing), with QRPs typically defined as practices that inflate the chance of finding a false positive result (type I error) (ref? Simons maybe?). Common examples of QRPs are centred around practices that manipulate p-values. Whilst Null Hypothesis Significance Testing has been and still is in popular use in ecology [@Fidler:2017he; @Stephens2007], a large amount of applied problem-solving work in the field is model-based. Ecological modellers may be inclined to think that the concerns associated with QRPs and reproducibility do not apply to them. In practice, this work is quite different to null hypothesis significance testing: Instead of relying on a single quantitative measure, studies describing ecological models usually generate a suite of measures and analyses that pertain to different desirable attributes of the model and collectively inform claims about the model being sufficient to fulfil its intended purpose. This begs the question of whether QRPs are relevant to model-based research? If specific QRPs related to p-values, like p-hacking, don’t apply, are there parallel or counterpart practices that do? What form might they take? And where in the research process would we be likely to find them? This paper addresses each of these questions. Our primary aim in this paper is to highlight the range of particular decision-points and practices that could be questionable so as to identify potential QRPs in model-based research. We encourage modellers to pause at such junctions and assess for themselves what the relative risk of QRPs is.

```{r}
#| label: create-QRP-table
#| message: false
#| warning: false
#| echo: false

image_out <- function(x, height = 50){
  out <- map_chr(x, gt::local_image, height = height)
  out <- map(out, gt::html)
  out
}

QRP_tbl <- readr::read_csv(here::here("data", "tbl-QRP-frequency.csv"),
                show_col_types = FALSE) %>% 
  mutate(Category = here::here("data", "icons", 
                               glue::glue("{Category}.png"))) %>%
  gt::gt(groupname_col = "Category",
         row_group_as_column = TRUE,
         caption = 'Examples and self-reported frequency of questionable research practices in hypothesis-testing research. QRPs are categorised as "cherry-picking", "p-hacking", and "methodologically flawed", indicated by the cherry, saw, and cross icons respectively.') %>% 
  gt::tab_footnote(footnote = "Makel et al. (2019)", 
                   locations = cells_column_labels(columns = Abbreviation)) %>% 
  gt::tab_footnote(footnote = gt::md("Fraser et al. (2018), $n=494$"), 
                   locations = cells_column_labels(columns = 
                                                     c("Ecology", 
                                                       "Evolution"))) %>% 
  gt::text_transform(locations = gt::cells_row_groups(everything()),
                     fn = image_out
  ) %>% 
  gtExtras::gt_theme_nytimes() %>% 
  gt::cols_hide("Evolution")


QRP_tbl_html <- QRP_tbl %>% 
  gtExtras::gt_plt_bar_pct(
    fill = "purple",
    column = c("Ecology"),
    scaled = TRUE,
    width = 150,
    labels = TRUE,
    background = "lightgrey"
  ) %>% 
  gt::cols_width(
    Category ~ gt::pct(10),
    `Questionable Research Practice` ~ gt::pct(40),
    Abbreviation ~ gt::pct(30),
    Ecology ~ gt::pct(20)
  ) %>% 
  gt::tab_options(quarto.disable_processing = FALSE)

```

::::: landscape
::: {.content-visible when-format="html"}
```{r}
#| label: tbl-QRP-examples-html
#| column: body
QRP_tbl_html
```
:::

::: {.content-visible when-format="typst"}
```{r}
#| label: tbl-QRP-examples-typst
#| column: body
QRP_tbl %>% 
  gt::fmt_percent(
    columns = Ecology,
    scale_values = FALSE,
    decimals = 0
  )
```
:::
:::::

### Why QRPs in Hypothesis Testing don’t directly translate to modelling

One principle underlying the classification of these practices as ‘questionable’ relates to data driven decision making. In contrast to hypothesis testers, modellers makes a series of analytic decisions on the basis of both objectively identified model performance criteria and subjective judgements [@Babel2019; @Bennett2013], wherein which they shift between subjecting the model to analysis, validation and testing, and refining the model in response to those results [@Getz2017]. This process is non-linear, iterative, and generates many interim versions of the model preceding publication [@Augusiak2014], unfolding much like the ‘survival of the fittest’ model(s) [@Prosperi2019] given a specified purpose and the available resources. In modelling, analytic decision-making is adaptive and data-dependent. The emphasis of existing QRP definitions on type I errors is unhelpful for model-based research. Firstly, multiple sources and types of error may arise in the modelling process - there is model structural uncertainty, uncertainty in parameter estimates, and uncertainty in predictions. What constitutes an ‘error’, what the source of that error is, as well as the relative weighting of those different errors, depend on some combination of the model’s purpose (prediction, explanation, description), the type of model used (phenomenological or mechanistic) and the context for the model. Rather than relying on p-value as a metric, model performance metrics consist of both qualitative and quantitative measures that incrementally build a subjective picture of model credibility rather than providing dichotomous responses assessing model credibility [@Augusiak2014; @Hamilton2019]. Since the publishability of a model-based study hinges on the collection of these model outputs and their contribution to overall model credibility, each of these model outputs may be susceptible to questionable research practices in order to strategically alter the perceived credibility of the model. What can be ‘tweaked’, ‘hacked’ or ‘exploited’ in hypothesis testing are *p*-values and whilst there may not be a direct counterpart in model-based research, we do expect similar a similar motivation from publishability to apply.

## A Conceptual Framework of Questionable Research Practices in Ecological Modelling

## A Roadmap of QRPs in the Ecological Modelling Process

```{r}
#| label: waffle_data_prep
#| message: false
#| warning: false
waffle_plot_data <- 
        tidy_QRP_database(here::here("data", "QRP_database.csv")) %>% 
        filter(include) %>% 
        select(-notes, -model_subphase, -source, -reason_coded, -include) %>% 
        distinct() %>% #rm duplicate practice_coded #TODO next merge duplicates while keeping source
        drop_na() %>% #interim approach until datachecks in place
        mutate(model_phase = str_split(model_phase, ", "),
               target = str_split(target, ", "),
               values = 1) %>% 
        unnest(model_phase) %>% 
        unnest(target) %>% 
        complete(practice_coded, 
                 model_phase, 
                 target, 
                 fill = list(values = 0.0001) # see https://github.com/hrbrmstr/waffle/issues/66
                 ) %>%  
        group_by(practice_coded) %>% 
        fill(qrp_coded, model_phase, .direction = "downup") %>% 
        ungroup()
```

```{r}
#| label: fig-waffle_plot
#| fig-cap: "Summary of Questionable Research Practices (QRPs) in ecological modelling. QRPs may target model inputs (yellow squares), the model itself (red squares), and/or model outputs (green squares), and may occur at different phases in the modelling cycle. QRPs are grouped according to broader classes defined in @tbl-QRP-categories. See @tbl-QRP for the full list of QRPs we identified."
#| fig-width: 15
#| fig-height: 24

# options(hrbrthemes.loadfonts = TRUE)
waffle_plot_data %>% 
        distinct() %>% #TODO identify duplicated rows - suspect non-unique target coding is cause
        mutate(model_phase = 
                       forcats::as_factor(model_phase) %>% 
                       forcats::fct_relevel( c("Data", 
                                               "Model Construction", 
                                               "Model Evaluation",
                                               "Model Application"))) %>% 
        arrange(model_phase, 
                qrp_coded) %>% 
        ggplot(aes(fill = target, 
                   values = values)) +
        geom_waffle(size = 2, 
                    color = "white", 
                    make_proportional = FALSE, 
                    flip = TRUE) +
        scale_x_discrete(expand = c(0,0)) +
        scale_y_discrete(expand = c(0,0)) +
        ggsci::scale_fill_futurama(name = "QRP Target") +
        coord_equal() +
        facet_nested(qrp_coded + practice_coded ~ model_phase, 
                     switch = "y", 
                     labeller = 
                             labeller(
                                     practice_coded =
                                             label_wrap_gen(width = 80,
                                                            multi_line = TRUE),
                                     qrp_coded =
                                             label_wrap_gen(width = 10,
                                                            multi_line = TRUE),
                                     model_phase = 
                                             label_wrap_gen(width = 10, 
                                                            multi_line = TRUE)),
                     nest_line = element_line(linetype = 1),
                     solo_line = TRUE,
                     space = "free",
                     strip = strip_nested(size = "variable",
                                          clip = "off")
        ) +
        # facet_grid(practice_coded ~ model_phase, 
        #            switch = "y", 
        #            labeller = labeller(practice_coded = 
        #                                        label_wrap_gen(width = 100))) +
        # theme_no_axes() +
        hrbrthemes::theme_ipsum_rc() +
        waffle::theme_enhance_waffle() +
        theme(strip.text.y.left = element_text(angle = 0, 
                                               size = 10),
              strip.text.x = element_text(size = 14,vjust = 0),
              strip.background.x = element_part_rect(side = "b"),
              strip.background.y = element_blank()
        ) +
        ggh4x::force_panelsizes(rows = unit(1, "cm"), 
                                cols = unit(3, "cm"))
#DONE reorder the model process levels
#TODO exclude less important QRPs
#DONE Make strips pretty
#DONE recolour model object colours
#DONE group the QRPs into their umbrella terms -- nested facets?

```

```{r}
#| label: tbl-QRP-categories
#| tbl-cap: "Classes of Questionable Research Practices (QRPs) in ecological modelling."
waffle_plot_data %>% 
        distinct(qrp_coded) %>% 
        arrange(qrp_coded) %>% 
        gt::gt() %>% 
        gt::cols_label(qrp_coded = "QRP Class") %>% 
        gtExtras::gt_theme_pff() 
```

```{r}
#| label: tbl-QRP
#| tbl-cap: "Full list of Questionable Research Practices identified from the literature. We categorised QRPs into broader classes"
#| column: body-outset
#| warning: false
#| message: false

gt_qrp <- tidy_QRP_database(here::here("data", "QRP_database.csv")) %>% 
        make_qrp_table_data() %>% 
        select(-notes) %>% 
        group_by(qrp_coded) %>% 
        arrange(qrp_coded) %>% 
        make_QRP_table() %>% 
        gt::fmt_markdown(columns = c(qrp_coded, practice_coded, source)) %>% 
        gt::cols_hide(columns = c(-ends_with("coded"), -source, -target) ) %>% 
        gt::cols_label(qrp_coded = "QRP Class") %>% 
        gtExtras::gt_theme_pff()

gt_qrp

#gt_qrp %>% gt::gtsave(path = here::here("tables"), filename = "qrp_table_draft.pdf")
```

# Notes

*Diagram*

-   QRPs on y-axis, model phase / subphase on x-axis (top facets), nested.
-   There's mark (donut) if the QRP occurs in that phase / subphase.
-   The donut is coloured according to the target of the QRP, from Figure 1: input/model/output.
-   Have simplified the phases / subphases into broader categories

*Questions*

-   [ ] What QRP grouping categories can we drop? ONLY USE ONE CATEGORY for each

-   [ ] What symbols to use?

    \- Dumping ground catgegory 'Poor Practice': for QRPs that don't fit into the other categories

    \- Examples of poor practice include: - Not validating the model on independent data: the location of the QRP is actually somewhere else... maybe with other users.

    \- engaging in commonly used techniques / practices, but that are known to be problematic, e.g. using AUC as a performance metric

    -   'incomplete' "application" or something similar. Where the full method is not applied.
    -   Misuse/Misapplication of an existing metric.

**Todo:**

-   [x] Can merge some phases / subphases
-   [x] Aggregate some QRPs, they are essentially the same

For the main figure:

-   [x] only have single groups (of QRP categories / codes `qrp_coded` ) & simplify the text (i.e. the description of each QRP `practice_coded` ).
-   [ ] For each broad category can have two example QRPs - either most important (worst outcome), and easiest to explain. Only include hard to explain QRP in if it's serious and common.

fuzzy area - poor practice vs. QRP. Potentially consider dropping some QRPs from the list.

'Not following best-modelling practice'.

QRPs invisible? Harder to spot. Incomplete reporting + ~*dishonest*~*/ambiguous/misleading* framing.

Other problems like (e.g. QRPs that are poor practice)

Incomplete reporting (QRP problem) stops you from fairly evaluating the research that was done. So if people reported their modeling proces fully then we would be able to discriminate when people are *not* doing best practice, and when they are.

-   [ ] just have table + text in supp mat.

Here's the problem. 20 years of people illustrating best-practice.

We're not going to address that.

We can't fairly evaluate the literature because of this poor transparency. Don't have to force modellers to change, but can encourage reviewers editors to push modellers' to report their modelling process more fully. reference point for reviewers to give to editor. Ammunition for reviewers to push back. Giving them language to start calling these things out. FAIRLY EVALUATING SOMEONE's WORK. We're so hard we are on people who deviate from prereg plans.

# 6 august

Have weaned out a third of them that fit squarely in "poor modelling practice" (out of 42)

But still not sure about some other ones.

# Session Info {.unnumbered}

```{r}
#| label: session-info

devtools::session_info()
```

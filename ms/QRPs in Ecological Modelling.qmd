---
title: A Roadmap of Questionable Research Practices in Ecological Modelling
running-head: Questionable Research Practices in Ecological Modelling
author:
  - name: Elliot Gould
    email: elliot.gould@unimelb.edu.au
    orcid: 0000-0002-6585-538X
    url: https://github.com/egouldo
    corresponding: true
    affiliation:
      - ref: 1
  - name: Hannah S. Fraser
    orcid: 0000-0003-2443-4463
    affiliation:
      - ref: 1
  - name: Bonnie C. Wintle
    orcid: 0000-0003-0236-6906
    affiliation:
      - ref: 1
  - name: Libby Rumpff
    orcid: 0000-0001-9400-8086
    affiliation:
     -  ref: 1
  - name: Fiona Fidler
    orcid: 0000-0002-2700-2562
    affiliation:
      - ref: 2
affiliations:
  - id: 1
    name: The University of Melbourne
    department: School of Agriculture, Food and Ecosystem Sciences
  - id: 2
    name: The University of Melbourne
    department: School of Historical and Philosophical Studies
format:
  html:
    page-layout: full
    title-block-banner: true
    code-fold: true
  pdf:
    keep-tex: true
    documentclass: scrartcl
    papersize: A4
    fontsize: 11pt
    toc: true
    toc-depth: 2
    number-sections: true
    linkcolor: blue
    include: false
    header-includes:
    - \usepackage{mathtools}
    - \usepackage{amssymb}
    - \usepackage{xcolor}
    - \usepackage{booktabs, caption, longtable, colortbl, array}
custom-numbered-blocks:
  classes: 
    Box:
      boxstyle: foldbox.simple
      collapse: false
pdf-engine: xelatex
abstract: |
  1. Questionable research practices (QRPs) bias the published literature towards apparently strong and conclusive results, resulting in low rates of replicability. Recent metaresearch reveals that ecology is not immune to the ‘reproducibility crisis’ seen in other disciplines, due to similar rates of QRPs and a lack of transparency in published research. However, metaresearch to date focusses on hypothesis-testing research and treats data-dependent analytic decisions as inherently questionable. This is not a good fit for ecology and related fields that conduct exploratory or predictive research using complex models, where data-dependent decisions are often necessary and legitimate aspects of the modelling process.
  2. To aid in understanding why and how frequently QRPs occur, and how severe the consequences might be, we develop a conceptual framework describing QRPs in ecological modelling, distinguishing questionable from legitimate data-dependent decisions. We present a typology of QRPs by decision-making mechanism and target and reframe QRPs in modelling as practices that inflate perceived model credibility, rather than producing false-positive statistical results. 
  3. We identified six QRP classes that may occur at various points in the modelling process: selective reporting, S-hacking, (manipulating performance metrics), model fishing, sample curation, HARKing and overhyping. These practices threaten the reliability and reproducibility of model-based research by artificially inflating the apparent credibility of models.
  4. We aim to raise awareness among modellers about different types of QRPs and how they might emerge in ecological modelling. We offer strategies to mitigate QRP risks, while preserving legitimate adaptive decision-making characteristic of ecological modelling.
keywords: 
  - questionable research practices
  - ecological modelling
  - metaresearch
  - transparency
  - reproducibility
  - model credibility
  - researcher degrees of freedom
date: last-modified
bibliography: references.bib
filters:
- ute/custom-numbered-blocks
- callout-box.lua
- addcalloutnumber.lua
---

```{r}
#| label: setup
#| warning: false
#| message: false
library(tidyverse)
library(here)
library(janitor)

library(gt)
library(gtExtras)

library(ggh4x)
library(ggforce)
library(waffle)
library(ggsci)
library(hrbrthemes)

library(extrafont)
library(firasans)
# extrafont::loadfonts()

source(here::here("R/tidy_QRP_database.R"))
source(here::here("R/make_table_data.R"))
source(here::here("R/make_QRP_table.R"))
```

# Introduction

Self-report surveys of researchers’ statistical practices suggest high rates of Questionable Research Practices (QRPs, e.g., p-hacking, cherry picking, @tbl-QRP-examples-html @tbl-QRP-examples) in several different disciplines: psychology [@John:2012eo], education [@Makel2023] and ecology [@Fraser:2018cl]. Widespread QRPs, accompanied by a lack of transparency and openness in research reporting [@culina2020], can leave disciplines at risk of a ‘reproducibility crisis’ [@Fidler:2017he]. QRPs are a set of methodological and statistical practices that fall into an “ethical grey zone” between ideal responsible research conduct and unacceptable research behaviours like fabrication and plagiarism [@Butler:2017ks p. 94]. Researchers are motivated actors prone to cognitive biases incentivised by a ‘publish or perish’ research culture in which publication bias selects for particular types of findings [@Agnoli:2017kl]. These findings are generally statistically significant results or ‘unexpected’ / ‘surprising’ findings [@Nissen2016; @Nakagawa2025]. Findings that are not deemed ‘publishable’ (in a Null-Hypothesis Significance Testing framework \[NHST\], these are often results with p values $\gt 0.05$) are either sent to the ‘file drawer’ or else subject to QRPs, such as ‘p-hacking,’ that make the results more likely to be published.

@Fraser:2018cl surveyed ecology researchers, asking how often they used the Questionable Research Practices (QRPs) that have been identified in other fields. Many of these QRPs relate to p-values. The study found these practices were used at similar rates to other fields, but with notably higher rates of analysis gaming and cherry-picking [@John:2012eo; @Fraser:2018cl; @Makel2023; @Chin2023]. These differences may reflect the greater prevalence of model-based research in ecology, raising questions about whether, and in what contexts, these practices are problematic for modelling. Other research shows that the same pressures and motivations associated with QRPs in other fields are present in ecology, including publication pressure, pressure to obtain funding, and career incentives [@Fidler:2017he; @ODea2021a]. p-curve analysis has revealed the effects of both reporting and publication bias in ecology studies featuring p-values [@kimmel2023]. While @Ottaviani2023 found evidence of publication bias favouring novel results, with increasing use of novelty terms in ecological abstracts over time, especially in high-impact journals. In addition to selective reporting of results, ecological research is plagued by underpowered studies, which exaggerate effect sizes in the published literature [@Parker2023; @Yang2023c]. These studies underscore the prevalence of reporting and publication bias in ecology, ultimately influencing conclusions drawn from the literature [@deressa2023].

Current definitions and lists of QRPs are focused on hypothesis testing research (specifically NHST, @tbl-QRP-examples-html @tbl-QRP-examples), characterising these practices as those that inflate the probability of false positive findings [see Table 3 in @Nagy2025]. Which makes sense, since this is the primary statistical estimand on which a finding is deemed ‘publishable’ in null-hypothesis significance testing. This NHST-centric focus creates particular challenges for ecology, because, although Null Hypothesis Significance Testing is still popular in ecology [@Fidler:2017he; @Stephens2007], model-based methods in ecology are increasingly common, especially within applied research contexts [@Connolly2017; @Garcia-Diaz2019; @DeAngelis2021]. The emphasis of existing QRP definitions on Type I errors is unhelpful for model-based research. Multiple sources and types of error may arise in the modelling process — there is model structural uncertainty, uncertainty in parameter estimates and predictions, and uncertainty in scenarios [@Rounsevell2021; @Simmonds2024]. What constitutes ‘error’, what the source of that error is, as well as the relative weighting of those different errors, depend on some combination of the model’s purpose (prediction, explanation, description), the type of model used (correlative or mechanistic) and the context for the model. Ecological modellers may be inclined to think that the concerns associated with QRPs and reproducibility are irrelevant, since many of the practices described as questionable within an NHST context do not directly relate to their work.

```{r}
#| label: create-QRP-table
#| message: false
#| warning: false
#| echo: false

QRP_tbl <- readr::read_csv(here::here("data", "tbl-QRP-frequency.csv"),
                           show_col_types = FALSE) %>% 
  mutate(Category = here::here("data", "icons", 
                               glue::glue("{Category}.png"))) %>%
  gt::gt(groupname_col = "Category",
         row_group_as_column = TRUE
  ) %>% 
  gt::tab_footnote(footnote = "Makel et al. (2019)", 
                   locations = cells_column_labels(columns = Abbreviation)) %>% 
  gt::tab_footnote(footnote = "Fraser et al. (2018), n=494", 
                   locations = cells_column_labels(columns = 
                                                     c("Ecology", 
                                                       "Evolution"))) %>% 
  gt::cols_align(align = "center", columns = "Category")

image_out <- function(x, height = 50){
  out <- map_chr(x, gt::local_image, height = height)
  out <- map(out, gt::html)
  out
}

QRP_tbl_html <- QRP_tbl %>% 
  gt::text_transform(locations = gt::cells_row_groups(everything()),
                     fn = image_out
  ) %>% 
  gtExtras::gt_theme_nytimes() %>%
  gtExtras::gt_plt_bar_pct(
    fill = "green",
    column = c("Ecology"),
    scaled = TRUE,
    width = 100,
    labels = TRUE,
    background = "lightgrey"
  ) %>% 
  gtExtras::gt_plt_bar_pct(
    fill = "blue",
    column = c("Evolution"),
    scaled = TRUE,
    width = 100,
    labels = TRUE,
    background = "lightgrey"
  ) %>% 
  gt::cols_width(
    Category ~ gt::pct(8),
    `Questionable Research Practice` ~ gt::pct(30),
    Abbreviation ~ gt::pct(20),
    Ecology ~ gt::pct(10),
    Evolution ~ gt::pct(10)
  ) %>% 
  tab_style(style = cell_text(color = "black", size = "medium"),
            locations = cells_column_labels())

QRP_tbl_html %>% 
  gt::gtsave(here::here("figures", "tbl-QRP-examples.png"), 
             vwidth = 800,
             vheight = 1000)
```

::: {.content-visible when-format="html"}
```{r}
#| label: tbl-QRP-examples-html
#| tbl-cap: 'Examples and self-reported frequency of questionable research practices in hypothesis-testing research in Ecology and Evolutionary Biology. QRPs are categorised as "cherry-picking", "p-hacking", and "methodologically flawed", indicated by the cherry, saw, and cross icons respectively.'
QRP_tbl_html
```
:::

::: {.content-visible when-format="pdf"}
```{r}
#| include: true
#| label: tbl-QRP-examples
#| tbl-cap: "A test caption"
#| echo: false
#| out-width: "100"
#| out-height: "100"
magick::image_read(here::here("figures/tbl-QRP-examples.png") )
```
:::

Building on these limitations, we argue that current QRP frameworks fail to address model-based research because the underlying research processes are fundamentally different. One principle underlying the classification of these practices as ‘questionable’ relates to data driven decision making. In contrast to hypothesis testers, modellers make a series of analytic decisions on the basis of both objectively identified model performance criteria and subjective judgements [@Babel2019; @Bennett2013], wherein which they shift between subjecting the model to analysis, validation and testing, and refining the model in response to those results [@Getz2017]. This process is non-linear, iterative, and generates many interim versions of the model preceding publication [@Augusiak2014], unfolding much like the ‘survival of the fittest’ models[@Prosperi2019], given a specified purpose and the available resources. In modelling, analytic decision-making is adaptive and data-dependent.

Rather than dichotomous inferences relying on almost exclusively on p-values, model performance metrics consist of both qualitative and quantitative measures that incrementally build a subjective picture of model credibility [@Augusiak2014; @Hamilton2019]. Since the publishability of a model-based study hinges on the collection of these model outputs and their contribution to overall model credibility, each of these model outputs may be susceptible to QRPs that aim to strategically alter the perceived credibility of the model.

Therefore, we argue that a conceptual framework of QRPs in model-based research must account for certain kinds of data-dependent decisions, which are appropriate and justifiable aspects of the modelling process, while highlighting the primacy of guarding against data-dependent decision-making that might be questionable. Moreover, the conceptual framework should de-emphasise the risk of type I errors and false positive findings to account for other biases more relevant to how complex models are evaluated and used. This begs the question of whether QRPs are relevant to model-based research? If specific QRPs related to p-values do not apply, such as p-hacking, are there parallel or counterpart practices that do? What form might they take? And where in the research process would we be likely to locate them? This paper addresses each of these questions. Our primary aim is to highlight the range of specific practices that are problematic in some circumstances so as to identify QRPs and their decision-points in model-based research. We encourage modellers to pause at such junctions and assess for themselves what the risk of QRPs is, in their context. We aim to raise awareness among ecologists (and modellers within other disciplines) about the potential for QRPs throughout the modelling process and hope to facilitate future attempts to both estimate the severity and extent of QRPs and mitigate the risk of QRPs in in ecology by providing a more comprehensive account of what constitutes questionable practice in model-based research.

# Conditions for Questionable Research Practices in Ecological Modelling

Below we outline the conditions under which QRPs may arise in ecological modelling and give an overview of the modelling process and the 'objects' it produces (inputs, outputs, the model itself). This sets the scene for exploring how QRPs could unfold in model-based research.

## What makes a model publishable? Identifying motivations for QRPs

Understanding what makes ecological models "publishable" is crucial for identifying where QRPs might emerge, since publication bias provides a primary motivation for engaging in questionable practices [@Ware2015]. Unlike hypothesis testing research where p-values serve as the main target for manipulation, model-based research involves multiple attributes that collectively determine publishability. These attributes become potential targets for the QRP classes we later identify in our typology (@tbl-QRP-typology).

### Model Advantage and Novelty

Novelty is an important factor influencing the publishability of modelling research. Publication bias and funding stipulations reward and require advantage over existing approaches; either through development of new methodological approaches, enhanced performance of existing algorithms and modelling methods, or application of existing models to new contexts, such as new environmental conditions or ecological contexts [@Alexandrov:2011iv]. Conversely, publication bias disincentivises the evaluation and testing of existing published models [@Babel2019; @Houlahan:2016fl]. This requirement for novelty incentivises model fishing and selective reporting, where researchers may opportunistically explore new modelling approaches until they achieve apparent superiority over baseline approaches.

### Model Credibility

Model Credibility is based on the subjective degree of confidence that both the model and model-derived inferences about the real system can be used as claimed [@Risbey2005; @Augusiak2014; @Hamilton2019]. That is, can the model adequately answer the research question (Christin et al 2020), and can it be used reliably to inform management decisions [@Alexandrov:2011iv]? Credibility emerges gradually throughout the modelling process, by demonstrated adequacy [conceptual validity and predictive accuracy, @RykielJr1996] and reliability [consistent performance and transferability, @Schmolke:2010fd; @Yates2018]. Unlike the binary nature of statistical significance, model credibility builds incrementally through multiple performance metrics and evaluation approaches (Figure 1, model outputs). This multi-faceted assessment creates numerous opportunities for statistic hacking, or “S-hacking” (Table 3 and 4), where researchers can manipulate:

-   Performance metric selection and thresholds,
-   Validation approaches and data partitioning strategies, and
-   Evaluation timeframes and spatial scales.

The subjective nature of credibility assessment also fosters *overhyping* (Table 3), where model capabilities are overstated beyond what evaluation results justify.  

### Purpose-dependent Vulnerabilities

Models that serve different purposes are vulnerable to different QRPs and depending on the combination of modelling approach, model type (e.g. agnostic, correlative, or mechanistic) and purpose of the model, QRPs will target different *estimands* of interest [@Hoffmann2021] . For instance, explanatory models are more susceptible to practices targeting parameter estimates and goodness-of-fit measures, whilst predictive models face greater risks around forecast accuracy metrics and transferability claims, because in each case a different *estimand*, or "target quantity to be estimated in an analysis" [@Borger2025, p. 2], is sought after by the researchers. Different questionable practices are concentrated at different locations across modelling phases (Figure 2).

## Which modelling objects (inputs, outputs, the model itself) might be affected by QRPs?

To help conceptualise where in the modelling process QRPs might emerge, and which 'model objects' QRPs may affect, we first  give an overview of the modelling process, articulating various inputs and outputs, including the model itself, model fit statistics, summary measures and other evaluation results (Figure 1), to which we ascribe mathematical notation[^1]. This framework will also provide the foundations for extending Gelman and Loken’s [-@Gelman2013] mathematical formalism to modelling (which we do in section 4).

[^1]: Boldface notation represents a vector or a set, indicating where multiples of those objects could be generated, e.g. there may be multiple ways to operationalise a conceptual model.

We acknowledge the plurality and lack of consensus in how the modelling process is described [@Lahtinen2017], including the terminology used for different modelling phases, steps and tasks [@Schmolke:2010fd; @Augusiak2014]. Rather than adopting a comprehensive taxonomy that captures all distinct processes and categories of modelling, we instead describe the modelling process at a high-level that can be generalised across different model purposes, contexts, types and methods. There will, of course, be exceptions. Some aspects may not apply in every modelling problem, and the specific collection of model objects, their relative weighting in informing study conclusions, and the relative weighting of publishable attributes, will differ depending on the model purpose, context and methodology applied to the problem at hand. We also recognise that analysis decisions are *procedurally* dependent [@Liu2020], for instance, the way we specify and parameterise models depends on the model type [e.g. whether you're using a correlative, mechanistic, or agnostic model, *sensu* @Hoffmann2021] and modelling purpose [exploration, inference, prediction, see @Tredennick2021].

We divide the modelling process into three phases, 1) model construction, 2) model evaluation, and 3) model application. These distinctions align with the phases underpinning the preregistration template in Chapter 6 (Appendix X).

```{r}
#| label: fig-modelling-process
#| fig-cap: |
#|   Three phases of model development: model construction, where the conceptual model $M_c$ is specified into the formal model (${M_c\rightarrow M}_s$) then parameterised ($M_s{\rightarrow M}_p$); model evaluation, where the calibration and validation fits are evaluated, possibly leading to re-specification and re-parameterisation (dashed arrow); and model application, where the model is analysed to answer the research question. <br><br>
#|   Modelling generates objects, including the conceptual, specified and parameterised model, ($\boldsymbol{M}=\left\{M_c,M_s,M_p\right\}$, purple circles); model inputs (blue squares), including hyper parameters $\lambda$ and calibration settings, data $\boldsymbol{X}$  for model parameterisation, evaluation and application. Model outputs ($M_j$ green hexagons) include model predictions $\hat{y}$, which are used to characterise model performance during model evaluation or subject to further aggregation, transformation, analysis and visualisation during model application. <br><br>
#|   Note: Data analyses may also inform model specification during construction. New or alternative input data may be used during scenario analysis to make predictions or projections about how the system will respond to intervention $\hat{y}$.

magick::image_read("images/Figure_1.png")
```

### Conceptual model, $M_c$

To begin the model development process, a conceptual model, $M_c$ or *candidate set* of models, $\boldsymbol{M_c}$ is specified by the modeller, synthesising their understanding of the ecological system. Conceptual models may be represented by a set of qualitative statements, mathematical formulas, or else visually as plots or directed acyclic graphs [@Shmueli2010]. A candidate set of multiple models at this stage may represent competing hypotheses, or critical uncertainties in ecological processes, principles, variable importance or framing.

### Specified Model, $M_s$

Next, the modeller formalises each conceptual model mathematically or statistically, $M_s$ (Figure 1). The modeller chooses which variables should be included in the model, how to operationalise or represent them in the chosen framework, what the appropriate dependencies are between variables and the model type, and the functional form of the model (if relevant). Because the variables in the conceptual model are not directly observable, they are operationalised into measurable outcomes $Y$ and input variables $X$ in a data matrix $\boldsymbol{X} = {X_1, X_2,\ldots,X_p}$, where $p$ is the number of input variables, and *f* represents a function relating $Y$to$X$such that $E\left(Y\right) = f\left(\boldsymbol{X}\right)$ [@Hoffmann2021; @Shmueli2010]. Note that, for some predictive modelling contexts, such as data-driven modelling employing black-box algorithmic approaches, like machine-learning, $f$ may not be specified and is instead represented by $\mathscr{I}_{\lambda}$ where $\mathscr{I}$ represents some learning algorithm and $\lambda$ denotes its hyperparameters [following @Bischl2023]. Exploratory analyses are often conducted at this stage to inform variable selection, for example by analysing variable importance and examining collinearity among variables [@Kass2025].

### Parameterised Model, $M_p$

Next, each specified model $M_s$is parameterised yielding $M_p$ (Figure 1). Model parameters refer to any component of a model that can be quantified or estimated, such as slopes or intercepts in a linear regression or growth rate in a population model [@Garcia-Diaz2019, p. 2]. Regardless of the overarching model purpose [e.g. explanation or prediction, @Shmueli2010], for correlative or agnostic (e.g. machine-learning) models, parameterisation typically occurs by *estimation*, or *calibration,* whereby the modeller applies techniques, like maximum-likelihood estimation or Bayesian inference, to the data matrix $\boldsymbol{X}$ (Figure 1) to estimate the parameters specified by $f$, with uncertainty [@Garcia-Diaz2019; @Hoffmann2021], yielding $\hat{y}=\hat{f}\left(\boldsymbol{X}\right)$. In the case of agnostic models, the algorithm $\mathscr{I }_{\lambda}$ returns the fitted model and its parameters when applied to $\boldsymbol{X}$, $\hat{y}=\hat{f}_{\hat{\lambda}}\left(\boldsymbol{X}\right)$. Parameters of mechanistic models are typically provided as inputs to the specified model $f$*,* gleaned from expert knowledge, published literature or else via calibration [@Hoffmann2021].

When conducting inference or explanatory modelling, the estimand(s) of interest are the parameters $\boldsymbol{\hat{\theta}}$, like standardised mean differences, correlation coefficients or response ratios [@Williams2025],whereas for predictive modelling, predicted values $\hat{y}$ constitute the estimand(s) of interest [@Tredennick2021; @Hoffmann2021; @Shmueli2010]. This is true regardless of whether the model is correlative [e.g. a correlative SDM, @Briscoe2019, or mixed-effects linear regression model, @silk2020], mechanistic (e.g. a population viability model), or agnostic [e.g. a machine learning or deep learning models, @Pichler2023]. However, different types of models are more likely to be used for inference or prediction in practice, for example, agnostic models are more likely to be used for prediction, but inferences about parameters are certainly possible[@Lucas2020]. Note that agnostic modelling approaches require the modeller to supply hyperparameters $\boldsymbol{\lambda}$ (figure 1), which may be decided by the modeller, or else estimated by some tuning or optimisation method $\boldsymbol{\hat{\lambda}}$. Hyperparameters may influence the model learning process, such that with each set of hyperparameters the model will provide a different set of results [@Ahmed2025].

### Model Evaluation: Characterising Model Performance and Fitness for Purpose

### A Provisional Typology of QRPs

```{r}
#| label: tbl-QRP-typology
#| html-table-processing: none
#| tbl-cap: |
#|   A model-based typology of Questionable Research Practices (QRPs), based on @Gelman2013. QRPs may affect different components of a model (table rows) indirectly altering the publishable attributes of a model by selectively reporting or directly altering objects generated throughout the modelling proces, targeting the model itself ($M$, purple cells) or model outputs ($M_j$, green cells). The final row represents the unambiguous ideal where no QRP occurs, that is, analytical decisions $\\phi$ are pre-specified rather than contingent on the model and/or data at that decision-point $\\phi(\\mathbf{X}, M)$.
tibble::tibble(QRP_outcome = c("The Model or Model Outputs<br>$(M, M_j)$", "Publishable Attributes of a model, via selective reporting or 'fishing'", "No QRP, e.g. pre-specified analytic decisions"),
               QRP_target_M = c("$$C (\\mathbf{X}, M;\\phi (\\mathbf{X},M))$$", 
                                "$$C (\\mathbf{X}, M;{\\phi}^{best} (\\mathbf{X},M)) $$", 
                                "$$C (\\mathbf{X}, M;\\phi ) $$"),
               QRP_target_Mj = c("$$ C (\\mathbf{X}, M;{\\phi}_{{M}_{j}} (\\mathbf{X},M))$$",
                                 "$$C (\\mathbf{X}, M;{{\\phi}_{{M}_{j}}}^{best} (\\mathbf{X},M))$$",
                                 "$$ C (\\mathbf{X}, M;{\\phi}_{{M}_{j}})$$"),
               row_group = c("QRP", "QRP", "OK")) %>% 
  gt::gt() %>% 
  gt::tab_spanner(label = "The practice may target:",
                  columns = c(QRP_target_M, QRP_target_Mj),) %>% 
  # gt::tab_row_group(label = "Questionable Research Practice", rows = row_group == "QRP" ) %>% 
  #   gt::tab_row_group(label = "Acceptable Research Practice", rows = row_group == "OK" ) %>% 
  gt::cols_label(QRP_outcome = "It may alter:",
                 QRP_target_M = gt::md("The model $$M$$"),
                 QRP_target_Mj = gt::md("Any model output $$M_j$$")) %>% 
  gtExtras::gt_theme_nytimes() %>% 
  gt::tab_style(style = gt::cell_fill(color = "#A02B93", alpha = 0.8),
                locations = gt::cells_body(columns = QRP_target_M,  rows = row_group == "QRP")) %>% 
  gt::tab_style(style = gt::cell_fill(color = "#4EA72E", alpha = 0.8),
                locations = gt::cells_body(columns = QRP_target_Mj, rows = row_group == "QRP")) %>% 
  gt::cols_hide(row_group) %>% 
  gt::fmt_markdown(columns = c(QRP_outcome)) %>% 
  gt::cols_align(columns = starts_with("QRP_target"),
                 align = "center")
```

## A Roadmap of QRPs in the Ecological Modelling Process

### Method

We surveyed the modelling literature to identify potential QRPs in ecological modelling and their location in the modelling process. We categorised QRPs into broader classes corresponding to families of similar practices using well-known published classifications (e.g. @tbl-QRP-examples-html @tbl-QRP-examples) and adopted new classes when there was no NHST-analogue. We coded the phase and sub-phases of modelling in which the practice occurs, as well as the model object (input, model, output) directly affected by the QRP, i.e. the *target* of the practice. After initial coding of the QRPs we refined the practice descriptions to encapsulate similar practices, as well as the broader QRP classes into which practices were grouped. Our coding is described in further detail in @sec-appendix-2.

```{r}
#| label: waffle_data_prep
#| message: false
#| warning: false
waffle_plot_data <- 
        tidy_QRP_database(here::here("data/QRP_database_2025-10-20.csv")) %>% 
        filter(include) %>% 
        select(-practice_notes, -model_subphase, -source, -qrp_reason, -include, -starts_with("practice_"), -file_name, practice_target) %>% 
        distinct() %>% #rm duplicate qrp_coded #TODO next merge duplicates while keeping source
        drop_na() %>% #interim approach until datachecks in place
        mutate(model_phase = str_split(model_phase, ", "),
               practice_target = str_split(practice_target, ", "),
               values = 1) %>% 
        unnest(model_phase) %>% 
        unnest(practice_target) %>% 
        complete(qrp_description, 
                 model_phase, 
                 practice_target, 
                 fill = list(values = 0.0001) # see https://github.com/hrbrmstr/waffle/issues/66
                 ) %>%  
        group_by(qrp_description) %>% 
        fill(qrp_coded, model_phase, .direction = "downup") %>% 
        ungroup()
```

```{r}
#| label: tbl-QRP-categories
#| tbl-cap: "Classes of Questionable Research Practices (QRPs) in ecological modelling."
#| echo: false

qrp_class_descriptions <- tribble(~ qrp_coded, ~ description,
        "Selective Debugging", "The practice of only checking for and correcting errors in a model when results are unexpected or unfavourable, rather than performing systematic error checking regardless of outcomes.",
        "Overfitting", "Developing a model that fits the training data too closely, including noise and peculiarities specific to that dataset, resulting in poor generalisability to new data. This often occurs when models are made overly complex relative to the amount of training data available.",
        "Executing Alternative Analyses", "Conducting multiple different analyses or model variations and selectively reporting only those that yield favourable results, without disclosing the full range of analyses performed.",
        "S-hacking", 
        "Manipulating model inputs, outputs or the model itself to obtain a favourable value of a performance measure, usually by systematically altering the modelling until a desired level of a statistic or metric is obtained [@Nakagawa2025; @Nagy2025]. This can include changing random seeds, outcome variable and/or performance thresholds or metrics and re-executing the analysis, fitting, validation or evaluation without disclosing [@feng2019]. ",
        "Overhyping", 
        "Typically, overyhyping features in the discussion section of a paper and involves exaggerating or overstating the capabilities, generalisability, or reliability of a model beyond what is justified by the evidence.",
        "Data Curation", 
        "Selectively including, excluding, or modifying data points in the sample used to develop or evaluate a model, often to improve apparent model performance (a specific instance of S-hacking).",
        "Model Fishing", 
        "Searching through a large number of possible model specifications or variable combinations without theoretical justification, in an attempt to find favourable results, and not reporting all models and/or the dredging process. Alternatively, conducting alternative analyses or fitting new model variations, and selectively reporting only those that yield favourable results, without disclosing the full range of analyses performed.",
        "Selective Reporting", 
        "Choosing to report only certain analyses, models, metrics, model results or comparisons that yield favourable results or desired conclusions without disclosing the full range of analyses performed, or their results.  This can create a misleading impression of model performance or study outcomes. In contrast to ‘fishing expeditions’, models have theoretical justification.",
        "HARKing", "May accompany fishing expeditions, or model dredging analyses and model selection procedures that have been selectively reported. The selected model(s) is presented as if it was pre-specified and theoretically justified before parameterisation, and/or, the model dredging process is not disclosed.")

waffle_plot_data %>% 
  distinct(qrp_coded) %>% 
  filter(qrp_coded != "Poor Practice") %>% 
  arrange(match(qrp_coded, c("Selective Reporting", "S-hacking", "Model Fishing", "Data Curation", "Overhyping"))) %>% 
  left_join(qrp_class_descriptions, by = join_by(qrp_coded)) %>% 
  bind_rows(qrp_class_descriptions %>% filter(qrp_coded == "HARKing")) %>% 
  gt::gt() %>% 
  gt::cols_label(qrp_coded = "QRP Class") %>% 
  gtExtras::gt_theme_nytimes() %>% 
  gt::cols_width(
    qrp_coded ~ gt::pct(15),
    description ~ gt::pct(85)
  ) %>% 
  gt::fmt_markdown(columns = description) 

```

### Roadmap

We summarise a subset of QRPs, their location in the modelling process, and their target modelling object in @fig-waffle_plot, highlighting a mixture of common practices, or practices that have the potential for critically influencing the perceived credibility of a model and its publishability. The complete set of QRPs identified is presented in @tbl-QRP.

We grouped QRPs into broader classes described in @tbl-QRP-categories. We applied some umbrella terms identified by Nagy et al. [@Nagy2025] and Liu et al. [-@Liu2020], including: "Sample Curation", "Selective Reporting" or "Cherry-picking," "Overhyping" and "Executing Alternative Analyses." We used additional terms for groups of practices without an existing umbrella term, e.g. 'Overfitting' and 'Model Dredging', 'Selective Debugging'. In the same vein as Nagy et al.'s [-@Nagy2025] expansion of the concept of *p*-hacking to encompass other analysis metrics, we expanded the concept of *p*-hacking and renamed it "S-hacking", or statistic-hacking, encapsulating analogous practices that occur in modelling.

HARKing (*hypothesising after results are known*) is a well-known type of QRP, but was not explicitly included in our final classification of QRPs. However, some practices, like Overhyping, could be considered a form of HARKing.

Although overfitting is a common outcome of many QRPs, especially S-hacking and Sample Curation (see also @tbl-QRP for descriptions of the effects or impacts of each practice), we included Overfitting as a distinct class of QRPs because overfitting may also be the primary goal of the practice. Overfitting is like S-hacking but perhaps is less constrained in practice, because the 'model space' is less determined by a specific set of performance measures / statistics, whereas practices that specifically aim to 'overfit' the model to the calibration data operate on a much larger decision-space, so the risk of a spurious finding is greater.

Similarly, Sample Curation includes practices that directly affect or target input data, but the intent of the practice might be to S-hack. For instance, Optional Stopping rules directly affect the input data, but indirectly target some performance measure by continuing to collect and update the data until some desirable threshold in the performance measure is met.

We categorised QRPs according to the model object they directly target, i.e. the model inputs, the model itself, or the model outputs, corresponding to the the model objects in Figure X.

We also grouped QRPs according to the phase of the modelling process in which they occur, i.e. Data, Model Construction, Model Evaluation and Model Application. Some classes of QRPs can occur in multiple phases of the modelling process, e.g. Sample Curation can occur in both Data and Model Construction phases, and Overfitting can occur in Model Construction and Model Evaluation phases while S-hacking can occur during Model Construction, Evaluation and Application. More narrowly defined classes of QRPs are restricted to one phase, e.g. Selective Debugging occurs only in Model Evaluation, or one type of target, e.g. Overfitting and Model Dredging target the model itself. Sometimes the QRP might occur in a second iteration of the phase, e.g. after model evaluation, but during model specification, i.e. the model specification is revised based on observing model evaluation results.

```{r}
#| label: fig-waffle_plot
#| fig-cap: "Summary of Questionable Research Practices (QRPs) in ecological modelling. QRPs may target model inputs (yellow squares), the model itself (red squares), and/or model outputs (green squares), and may occur at different phases in the modelling cycle. QRPs are grouped according to broader classes defined in @tbl-QRP-categories. See @tbl-QRP for the full list of QRPs we identified."
#| fig-width: 14
#| fig-height: 16

# #| dev: "cairo_pdf" # when output is pdf
# options(hrbrthemes.loadfonts = TRUE)
waffle_plot <- waffle_plot_data %>% 
        distinct() %>% #TODO identify duplicated rows - suspect non-unique practice_target coding is cause
        mutate(model_phase = 
                       forcats::as_factor(model_phase) %>% 
                       forcats::fct_relevel( c("Data", 
                                               "Model Construction", 
                                               "Model Evaluation",
                                               "Model Application"))) %>% 
        arrange(model_phase, 
                qrp_coded) %>% 
        ggplot(aes(fill = practice_target, 
                   values = values)) +
        geom_waffle(size = 2, 
                    color = "white", 
                    make_proportional = FALSE, 
                    flip = TRUE) +
        scale_x_discrete(expand = c(0,0)) +
        scale_y_discrete(expand = c(0,0)) +
  scale_fill_manual(
          name = "Target Model Object",
          values = c("input" = "#0F9ED5", # blue
                     "model" = "#A02B93", # purple
                     "output" = "#4EA72E"), # green
          labels = c("input" = "Model Input",
                     "model" = "Model",
                     "output" = "Model Output")
        ) +
        # ggsci::scale_fill_futurama(name = "QRP Target") +
        coord_equal() +
        facet_nested(qrp_coded + qrp_description ~ model_phase, 
                     switch = "y", 
                     labeller = 
                             labeller(
                                     qrp_description =
                                             label_wrap_gen(width = 80,
                                                            multi_line = TRUE),
                                     qrp_coded =
                                             label_wrap_gen(width = 10,
                                                            multi_line = TRUE),
                                     model_phase = 
                                             label_wrap_gen(width = 10, 
                                                            multi_line = TRUE)),
                     nest_line = element_line(linetype = 1),
                     solo_line = TRUE,
                     space = "free",
                     strip = strip_nested(size = "variable",
                                          clip = "off")
        ) +
        # facet_grid(qrp_description ~ model_phase, 
        #            switch = "y", 
        #            labeller = labeller(qrp_description = 
        #                                        label_wrap_gen(width = 100))) +
        # theme_no_axes() +
        hrbrthemes::theme_ipsum_rc() +
        waffle::theme_enhance_waffle() +
        theme(strip.text.y.left = element_text(angle = 0, 
                                               size = 14),
              strip.text.x = element_text(size = 16,vjust = 0),
              strip.background.x = element_part_rect(side = "b"),
              strip.background.y = element_blank()
        ) +
        ggh4x::force_panelsizes(rows = unit(1, "cm"), 
                                cols = unit(3, "cm"))

print(waffle_plot)
loadfonts(device = "all")
ggsave(waffle_plot, 
       filename = here::here("figures/fig-waffle-QRPs-ecological-modelling.png"),
       device = grDevices::png,
       width = 14,
       height = 16,
       dpi = 600)

#TODO update based on revised definitions
#DONE reorder the model process levels
#TODO exclude less important QRPs
#DONE Make strips pretty
#DONE recolour model object colours
#DONE group the QRPs into their umbrella terms -- nested facets?

```

## Takeaway messages

Our analysis identified *55* (but 28 included) distinct (QRPs) in ecological modelling, spanning the entire modelling workflow from problem formulation through to model application. We categorised these practices into seven primary types: model dredging, overfitting, S-hacking, sample curation, selective reporting, overhyping, and poor practice. The distribution of QRPs across modelling phases highlights systematic vulnerabilities in how ecological models are constructed, evaluated, and applied.

### Data Handling

Data-related QRPs $(n=10)$ included *sample curation* and *missing data hacking* and *employing optional stopping rules*. All of these practices involve making post-hoc decisions about model inputs without justification and prior planning, i.e. after fitting model and observing model results. Sample curation may include removing observations that should have been included in order to make a correlation of interest become significant, and generating a post-hoc exclusion criteria for those observations being excluded [@Nagy2025]. Opportunistic handling of missing data could occur when a researcher tries multiple ways of handling missing data, for instance, list-wise deletion, multiple imputation and inverse probability weighting. But the expected results only appear with inverse probability weighting and so the researcher only reports this strategy in the paper, omitting the results when the other data handling methods were used [@Nagy2025]. *Opportunistic stopping* occurs when new data is collected and is used to re-parameterise the model after previously observing model validation and model evaluation results, without reporting results of earlier iterations (Table X).

### Model Construction

The Model Construction phase exhibited the highest number and diversity of QRPs $(n=15)$, with practices occurring across multiple sub-phases including Model Selection, Model Specification, Model Tuning, and Method Selection with particularly concerning practices around model specification and selection.

*Model dredging* emerged as a prominant concern, characterised by unconstrained exploration of model space without theoretical guidance or consideration of biological plausibiliy [@Shmueli2010]. As MacNally [-@MacNally2000] observed:

> "Ecologists and conservation biologists too often measure almost everything they can (especially in these days of automated probes and data loggers, remote sensing and GIS) and hope that something important will by 'fished out' of the resultant murky data."

25 years later the "kitchen sink" or "causal salad" approach is still a common approach to ecological modelling [@Franks2025]. @Fourcade2018's simulation analysis provides a pedagogical example of the consequences of this practice, with paintings of the Mona Lisa successfully predicting species' distributions and being selected over models with plausible predictors when coupled with methodologically flawed use of model performance metrics.

*Overfitting* was identified as another critical issue, manifesting in multiple forms including the inclusion of excessive predictors relative to dataset size and complexity. Nakagawa et al. [-@Nakagawa2017] highlighted the prevalence of this practice, and the consequences are severe when the modelling purpose is to generate anticipatory predictions to inform decisions because overfitted models lose predictive power and generalisability to new datasets and generate spurious results [@lewis2023]. Overfitting is of particular concern in ecology where datasets are small and noisy (ref), heightening the risk of spurious results.

*S-hacking* practices in this phase included changing model specifications after observing test set performance and manipulating random seeds to improve results. @Liu2020 noted that manipulating random seeds during model tuning could drastically alter model results, underscoring that seemingly minor technical decisions can be exploited to inflate performance. S-hacking artificially inflate model performance, resulting in spuriously selected models that that may not reflect genuine predictive relationships.

Several QRPs revealed systematic biases towards confirming expectations rather than rigorous testing. *Selective debugging* exemplifies the problem of confirmation bias, where error checks during computational implementation of the modelling tend to occur only or mostly when results deviate from expectation, creating systematic bias because any silent errors when model results align with expectations are cursory, such that errors will be uncorrected [@Risbey2005].

### Model Evaluation

The model evaluation phase contained 14 distinct QRPs, with *selective reporting* and *executing alternative analyses* being particularly prevalent. *S-hacking* occurs when these practices are combined, for example a modeller may trial multiple evaluation metrics, selectively reporting those that present the model in the most favourable light [@Hildebrandt2018]. For models of systems that are difficult to interpret, such as ecological systems, high accuracy may not be meaningful, with the model being inaccurate despite a high measure accuracy [@Hildebrandt2018]. This problem is compounded further in ecological modelling, for example, because ecological models are rarely tested on independent data due to data constraints (ref). Evaluating models only on training data or 'resubstitution' is problematic because it leads to biased estimates of performance, insofar as the model is fitted to regular and irregular features of the sampled data, but is unable to distinguish between them [@Pu2018].

The potential for data leakage during this phase emerged as a second critical concern, particularly the practice of continuing to optimise models after already evaluating it on the test set ('*overhyping*'). Overhyping occurs in tandem with other post-hoc modifications described previously, for example, trialling alternative hyperparameter settings in cross-validation or systematically trialling different feature combinations on performance until test performance was improved [@hofman2023]. Alarmingly, interviews with modellers reveals that these practices are routine in the machine learning community [@hofman2023] (check this ref, P1 vs P2).

### Model Application

The Model Application phase showed fewer but distinct QRP types focused on the misrepresentation of model capabilities and findings. Overhyping involves claims about the models' performance that are not substantiated by model evaluation results, such as claiming the model has greater generalisability than it does. A specific form of overhyping involves misreporting correlative claims using causal language, which is particularly common in studies of conservation intervention evaluations using observational study designs [@Josefsson2020]. The practice of implying causation from correlation can cause false confidence in the intervention's effectiveness while ignoring the real reasons for the observed effect.

# Discussion

Far from being irrelevant to model-based research, researcher degrees of freedom are abound in the modelling process, providing ample opportunity for questionable research practices to operate. While some QRPs in modelling are analogous to those in hypothesis-testing research, such as S-hacking, some are specific to modelling. Regardless, all QRPs are motivated by external pressures related to the apparent publishability of the analysis. The iterative modelling process and the need to make data-based decisions during modelling has resulted in significant resistance to the idea of QRPs in model-based research, we explore this tension further here to clarify the distinction between legitimate and opportunistic data-dependent decisions, highlighting where the concept of QRPs apply and where they do not.

## A mathematical formalism of the garden of forking paths for model-based research

QRPs identified in our roadmap illustrate the two-axes of our typology of QRPs. During model construction, many QRPs directly alter the model itself, $C (\mathbf{X}, M;\phi (\mathbf{X},M))$, for example model dredging in model construction, where model selections are data-dependent, overfitting practices that modify the model specification $M_s$ based on observed performance, and S-hacking, involving changes to the model structure after seeing results. QRPs also targeted model outputs, without necessarily changing the underlying model $C(\mathbf{X}, M;{\phi}_{{M}_{j}})$, for example, selective reporting of performance metrics, trialling different model evaluation metrics, and changing model output thresholds or transformations during model application.

The prevalence of QRPs in Model Construction (affecting the transition from $M_s$ to $M_p$) and Model Evaluation (affecting the assessment of $Mp$ fitness for purpose) suggests these are the critical junctures where $\phi (\mathbf{X},M)$ problematic decision-making occurs most frequently. Practices like model dredging and overfitting during Model Construction exemplify how researchers make data-dependent choices about model structure and complexity after observing initial results, directly contradicting the idealised practice of pre-specified analytic decisions represented by $C(\mathbf{X}, M;\phi )$. Similarly, the abundance of QRPs in Model Evaluation, where researchers assess $M_p$ fitness for purpose, demonstrates how the iterative nature of model validation creates multiple opportunities for post-hoc optimisation of apparent model performance.

Practices targeting the model itself $C (\mathbf{X}, M;\phi (\mathbf{X},M))$ were prevalent throughout Model Construction, including model dredging, overfitting, and specification changes made after observing results. These practices directly alter the realised model, changing what would have been observed if the questionable practice had not occurred. Conversely, practices targeting model outputs $C (\mathbf{X}, M;{\phi}_{{M}_{j}} (\mathbf{X},M))$ were concentrated in Model Evaluation and Application phases, including selective reporting of performance metrics, manipulation of thresholds in model performance metrics. These practices leave the underlying model unchanged while altering perceptions of its performance or credibility.

The distinction between direct manipulation and selective reporting—central to our typology—was also reflected in our QRP Roadmaps. Model Evaluation phases showed both types: direct manipulation of model performance through threshold changes and output transformations $C(\mathbf{X}, M;{\phi}_{{M}_{j}} (\mathbf{X},M))$ , and selective reporting through cherry-picking of favourable performance metrics $C(\mathbf{X}, M;{{\phi}_{{M}_{j}}}^{best} (\mathbf{X},M))$. This exemplifies the contention in our formalism that QRPs can affect both the actual models and model outputs as well as the attributes that determine perceived model credibility.

As described in our framework, analytic uncertainty propagates combinatorially along each decision-point in the modelling process. The identification of QRPs that span multiple phases of the modelling workflow exemplifies our mathematical formalism $C (\mathbf{X}, M;\phi (\mathbf{X},M))$, capturing not just isolated questionable decisions, but interconnected sequences of data-dependent choices that collectively bias the final model and its perceived credibility. Sample curation, S-hacking, and overfitting practices demonstrate how questionable practices conducted early in the process can cascade through subsequent phases, with bias propagating along the realised modelling path, and the effects of questionable decisions compounding across the modelling process. For instance, post-hoc data exclusions during the data curation and preparation phase can inflate apparent model performance leading to unwarranted claims about the model performance and generating unreliable findings during model application. This pattern highlights that the proliferation of questionable practices follows the same combinatorial logic that characterises analytical flexibility in traditional hypothesis testing.

## Implications for Model-Based Research Integrity

The empirical patterns validate our emphasis on model credibility rather than statistical significance as the target of QRPs in modelling contexts. Many identified practices—particularly overhyping claims and misreporting correlative findings with causal language—specifically target perceptions of model reliability, accuracy, and generalisability rather than traditional statistical thresholds. This supports our model-centric definition of QRPs as practices that "artificially inflate the apparent accuracy or precision of a model, its predictions, and/or evaluation tests." The database demonstrates that the mathematical formalism successfully captures the diverse ways researchers can manipulate both technical model properties and broader perceptions of model fitness for purpose, providing a comprehensive framework for understanding questionable practices in model-based research.

QRPs collectively undermine the reliability and reproducibility of ecological modelling research in several ways:

1.  **Inflated performance estimates** that do not reflect true performance.
2.  **Spurious model selection** that identifies models based on chance rather than plausible biological mechanisms or predictive relationships.
3.  **Reduced reproducibility** due to undisclosed researcher degrees of freedom.
4.  **Compromised generalisability** from overfitted models that fail to transfer to new contexts or make accurate forecasts.
5.  **False confidence** in ecological understanding and management recommendations.

The prevalence of these practices suggests systematic issues in training, incentives, and quality control within the ecological modelling community. The concentration of QRPs in the model construction and evaluation phases indicates particular vulnerabilities in how models are specified, fitted, and evaluated. The large number of decision-points where researchers can exercise degrees of freedom suggest that interventions and safeguards should target these critical phases of the modelling process.

## Distinguishing "poor" from "questionable" practices

Our analysis revealed the context-dependent nature of questionable research practices, between clear QRPs and "poor practices" that may not always constitute questionable behaviour. For example, 'resubstitution' or assessing model performance on the training set only, was classified as poor practice rather than as questionable when done due to data limitations rather than intentional manipulation. However, as our analysis highlighted, flawed practices have the potential to compound the effects of QRPs. In this example, failing to evaluate the model on independent data can provide the opportunity for QRPs like S-hacking to remain hidden. Similarly, the continued misuse of AUC as a measure of predictive performance can result in biologically implausible models being selected when combined with model dredging behaviours.

Clearly, context matters when it comes to delineating questionable from poor practice. We must consider constraints on feasibility, like data availability, and the stated model purpose (e.g. assessing generalisability on new data is important for predictive modelling, but less so for explanatory modelling). However, a shared feature of *all* QRPs is that the practices are undisclosed so that the potential for these practices to bias the model and its results are hidden. Transparency in reporting is essential for being able to properly evaluate the credibility of the model.

### Data-dependent modelling choices: when are they ‘questionable’ ?

Our formalism emphasised the data-dependency in analytic decision-making $\phi (\mathbf{X},M)$ as a hallmark of QRPs. For instance, practices like "changing the model specification after observing model evaluation results" or "modifying exclusion criteria after fitting the model and observing results" exemplify the formulation $\phi (\mathbf{X},M)$ where choices depend on observed model states.

However, we hesitate to designate all data-dependent analytic decisions as questionable research practices, as is implied in Gelman and Loken's (2013) framework. That is because there are situations in modelling where decisions are necessarily dependent on previous analytic decisions within the modelling workflow [@Liu2020]. We describe and illustrate these further in Chapter 6, but as an illustrative example, the choice about an appropriate model-fitting method to derive $M_p$ might depend on the specified model structure $M_s$. Thus, not all data-dependent analytic decisions are automatically ‘questionable’ within the context of modelling.

@Liu2020 helpfully distinguish defensible from questionable motivations for engaging in data-dependent analytic decision-making through the dichotomy of *systematicity* and *opportunism*, or the desire to find a particular result. In the case of model evaluation, systematicity can be distinguished from opportunism when an objective metric is used to evaluate outcomes, and the choice of metric does not hinge on anticipated conclusions. While during model construction, formal model selection procedures, wherein reasonable alternative models are systematically enumerated and implemented, can be characterised as ‘defensible’ data-analytic decision-making or ‘exploration’, on the grounds that alternative models are chosen based on objective metrics and values, such as goodness of fit, which are selected a priori (Liu, 2020). Or, if the outcome determining the modelling choice is a qualitative measure, a rule of thumb for choosing one alternative over another is articulated *a priori*.

## References {.unnumbered}

::: {#refs}
:::

::: landscape
```{r}
#| label: tbl-QRP
#| tbl-cap: "Full list of Questionable Research Practices identified from the literature. We categorised QRPs into broader classes"
#| column: body-outset
#| warning: false
#| message: false

gt_qrp <- tidy_QRP_database(here::here("data/QRP_database_2025-10-20.csv")) %>%
        make_qrp_table_data() %>% 
        select(-practice_notes) %>% 
        group_by(qrp_coded) %>% 
        arrange(qrp_coded) %>% 
        make_QRP_table() %>% 
        gt::fmt_markdown(columns = c(qrp_coded, qrp_description, source)) %>% 
        gt::cols_hide(columns = c(-starts_with("practice_"), -source, -practice_target) ) %>% 
        gt::cols_label(qrp_coded = "QRP Class") %>% 
        gtExtras::gt_theme_nytimes()

# gt_qrp


tidy_QRP_database(here::here("data", 
                             "QRP_database_2025-10-20.csv")) %>%
  select(-file_name, -description) %>% 
  mutate(across(c(starts_with("practice_"), -practice_target), 
                ~ str_c(.x, source))) %>% #annotate `practice_*` with `source`
  group_by(qrp_coded, qrp_description) %>%
  mutate(across(c(starts_with("model_"), 
                  "practice_target",
                  "qrp_reason", 
                  "practice_notes"), 
                ~ str_split(.x, ", ")),
         source = str_split(source, "; ")
         ) %>% 
  summarise(across(c(starts_with("model_"), 
                     "qrp_reason",
                     "practice_notes",
                     starts_with("practice_")), 
                   list)) %>% 
  rowwise() %>% 
  mutate(across(c(starts_with("model_"), 
                  "practice_target",  
                  "qrp_reason", 
                  "practice_notes"), 
                ~ flatten_chr(unique(.x)) %>% 
                  unique() %>% 
                  str_flatten_comma(na.rm = TRUE)),
         source = list(
                         str_remove_all(string = "source", pattern = c("\\[|\\]")) %>% 
                         str_squish() %>% 
                         unique() %>% 
                         str_flatten("; ") %>% 
                         str_c("[", ., "]")),
         across(c(starts_with("practice_"), -"practice_target"), 
                ~ str_squish(.x) %>% 
                         str_flatten(collapse = "<br>"))) %>% 
  gt::gt(groupname_col = c("qrp_coded", "qrp_description")) %>% 
  gt::cols_hide(source) %>% 
  # gt::cols_width(ends_with("_source") ~ gt::pct(20),
                 # qrp_reason ~ gt::pct(20)) %>%
  gtExtras::gt_theme_pff() %>% 
  gt::tab_options(table.font.size = 8) %>% 
  gt::fmt_markdown(columns = source)


```
:::

::: appendices
## Appendix 1: Modelling Process Literature Review {#sec-appendix-1}

## Appendix 2: QRP Roadmap Literature Review {#sec-appendix-2}

### Step 1 - Identify and collect QRPs

We unsystematically screened the published literature to generate an initial list of terms for QRPs in NHST research to guide search term selection in ecological modelling and related modelling fields. We used the following search terms to identify potential QRPs in different areas of ecological modelling:

-   "`modelling_area` AND type I error"
-   "`modelling_area` AND false positive"
-   "`modelling_area` AND modelling choice"
-   "`modelling_area` AND subjective judgment"
-   "`modelling_area` AND prediction error"
-   "`modelling_area` AND confirmation bias"
-   "`modelling_area` AND publication bias"
-   "`modelling_area` AND questionable research practice"
-   "`modelling_area` AND researcher degrees of freedom"
-   "`modelling_area` AND cherry picking"
-   "`modelling_area` AND p-hacking"
-   "`modelling_area` AND HARKING"
-   "`modelling_area` AND bias"
-   "`modelling_area` AND good modelling practice"
-   "`modelling_area` AND best modelling practice"
-   "`modelling_area` AND bad modelling practice"
-   Where `modelling_area` included 'predictive modelling', 'habitat modelling', 'Species Distribution Modelling (SDM)', 'Ecological Niche Modelling', 'Ecological Modelling', 'Environmental Modelling'.

We inferred QRPs from practices described by authors with value judgements, such as "good" or "best practice," "bad" or "poor practice." For positively ascribed practices, we took the logical inverse of these practices as the QRP. We ignored perceived 'inconsequential' practices, and instead included practices that were commonly or routinely conducted and where authors argued strongly for the culture of research practice to change. We excluded QRPs that pertained to fraud, misconduct, or nefarious intent.

**Collating & Coding**

For each QRP identified, we collected a description of the research practice `practice_description`, the reason or justification for why the practice is 'questionable' `practice_reason`, including any quantitative and/or empirical evidence for: a) the negative consequences on research outcomes such as credibility, reliability, accuracy, precision, transparency, reproducibility and/or b) evidence for the use or occurrence of this practice; `practice_evidence`. We assigned each QRP to phases and sub-phases of the modelling process (section X) where the practice occurs. For each description, reason and evidence, we coded each into short descriptions of the practice `QRP_description`, reason for the practice's 'questionable' nature `QRP_reason`. Using the model phases and sub-phases identified in (Section X), we classified the location of the QRP in the modelling process, ascribing the `model_phase` and `model_subphase` in which the practice occurs. We then coded the `target` of the practice, i.e. the model object (Figure X) directly affected by the practice. Where mitigation measures or solutions to the practice were suggested alongside the description of the practice, we also coded the `practice_solution`.

**Categorising QRPs into Classes**

We adopt Nagy et al.'s [-@Nagy2025] approach and grouped QRPs consisting of the same family of research behaviours into broad classes of QRPs `QRP_coded`. Some umbrella terms were common QRPs in hypothesis testing research, some were hypothesis-testing analogues, while others were modelling-specific. Where possible we used existing umbrella terms used by Nagy et al. [-@Nagy2025] and others [e.g. @Liu2020], but created our own terms if no existing terms were applicable.

### *Step 3 - Refine QRP and QRP Class descriptions, aggregate QRPs*

We refined our coded descriptions of QRPs `QRP_description` into unifying or general descriptions effectively aggregating similar practices identified from different published sources. After categorising QRPs into broader classes of QRPs, we revised their description to be more modelling-specific, better reflecting the surveyed practices (Table X).

### QRP Code-Book / Metadata

-   [ ] TBD

## Session Info {.unnumbered}

```{r}
#| label: session-info

devtools::session_info()
```
:::

---
title: A Roadmap of Questionable Research Practices in Ecological Modelling
running-head: Questionable Research Practices in Ecological Modelling
author:
  - name: Elliot Gould
    email: elliot.gould@unimelb.edu.au
    orcid: 0000-0002-6585-538X
    url: https://github.com/egouldo
    corresponding: true
    affiliation:
      - ref: 1
  - name: Hannah S. Fraser
    orcid: 0000-0003-2443-4463
    affiliation:
      - ref: 1
  - name: Libby Rumpff
    orcid: 0000-0001-9400-8086
    affiliation:
     -  ref: 1
  - name: Fiona Fidler
    orcid: 0000-0002-2700-2562
    affiliation:
      - ref: 2
affiliations:
  - id: 1
    name: The University of Melbourne
    department: School of Agriculture, Food and Ecosystem Sciences
  - id: 2
    name: The University of Melbourne
    department: School of Historical and Philosophical Studies
format:
  preprint-typst:
    keep-md: true
    keep-typ: true
    css-property-processing: translate
    wordcount: true
    execute:
      echo: false
      message: false
      include-before-body:
      - text: |
          #show figure: set block(breakable: true)
    # Common options - uncomment (and remove 'default') to use
    # theme-jou: true              # Journal theme (2-column)
    # line-number: true            # Line numbering
    # fontsize: 11pt               # Font size
    # leading: 0.6em               # Line spacing
    # first-line-indent: 1.8em     # Paragraph indent
  html:
    page-layout: full
    title-block-banner: true
    code-fold: true
pdf-engine: xelatex
abstract: |
  1. Questionable research practices (QRPs), result in low rates of replicability and contribute to biased accounts of studied phenomena in the published literature, producing a literature full of apparently strong and conclusive results. Recent metaresearch has revealed that ecology is at risk of a ‘replicability crisis’ due to the presence of QRPs and a lack of transparency in published research. However, the work to date focusses on QRPs as they occur in hypothesis testing research which is not a good fit for use in ecology and related fields that conduct exploratory or predictive research using complex models. 
  2. In order to protect against bias caused by QRPs in ecology, it is essential to identify QRPs relevant to model-based research and understand why they occur, how frequent they are and how severe the consequences might be. In this paper we propose a conceptual framework for describing QRPs in ecological model-based research. 
  3. We describe a generalised workflow for model development, evaluation and analysis as it is typically undertaken in ecology and related fields. We develop a typology of QRPs that describes potential mechanisms for emergence of QRPs throughout the modelling process. From this conceptual framework, we derive a definition of QRPs that captures the specificities and features of the modelling process particular to ecology. Finally, we create a diagrammatic roadmap that map QRPs onto common decision-points in the modelling process, highlighting points of caution for modelers. 
  4. Our analysis and framework highlight that QRPs in the modelling process are a threat to the credibility of model-based research. QRPs may target the model itself, or the outputs of model analyses, including model checking, or testing and evaluation performed on the model.  Finally, our proposed definition of QRPs de-emphasises the idea of finding false positive results and instead points to other measures of model credibility that affect publishability, such as the accuracy and precision of the model and or its outputs. 
  5. In this paper we aim to raise awareness among modelers about the potential types of QRPs and their mechanisms for emergence in the modelling process. The roadmaps in this paper will help modelers to understand the risks of QRPs in their research so they are empowered to implement procedures and practices that can reduce the occurrence and impact of QRPs.
keywords: 
  - questionable research practices
  - ecological modelling
  - metaresearch
  - transparency
  - reproducibility
  - model credibility
  - researcher degrees of freedom
date: last-modified
bibliography: references.bib
---

```{r}
#| label: setup
#| warning: false
#| message: false
library(tidyverse)
library(here)
library(janitor)

library(gt)
library(gtExtras)

library(ggh4x)
library(ggforce)
library(waffle)
library(ggsci)
library(hrbrthemes)

library(extrafont)
library(firasans)
extrafont::loadfonts()

source(here::here("R/tidy_QRP_database.R"))
source(here::here("R/make_table_data.R"))
source(here::here("R/make_QRP_table.R"))
```

## Introduction

Self-report surveys of researchers’ statistical practices suggest high rates of Questionable Research Practices (QRPs e.g., p-hacking, cherry picking, see @tbl-QRP-examples-typst @QRP_tbl_html) in several different disciplines: psychology (ref), education (ref) and ecology [@Fraser:2018cl]. Widespread QRPs, accompanied by a lack of transparency and openness in research reporting [@culina2020] can leave disciplines at risk of a ‘reproducibility crisis’ [@Fidler:2017he] . To understand the implications of this for ecology, we need a more comprehensive account of what constitutes Questionable Research Practice. QRPs are a set of methodological and statistical practices that fall into an ‘ethical grey zone’ between ideal responsible research conduct and unacceptable research behaviours like fabrication and plagiarism [@Steneck2006] . When researchers are faced with ambiguous decisions where there is no single acceptable decision or strategy according to scientific standards and consensus, they are likely to select the decision that results in the most ‘favourable’ finding with “convincing self-justification” [@Hoffmann2021; @Simmons:2011iw]. QRPs may not be the product of conscious ‘fishing expeditions’ [@Gelman2013] but rather occur because researchers are motivated actors prone to cognitive biases incentivised by a ‘publish or perish’ research culture in which publication bias selects for particular types of findings [@Agnoli:2017kl]. These findings are generally significant results or ‘unexpected’ /surprising findings [@Nissen2016; @Nakagawa2025]. In NHST these are generally statistically significant findings, correlations rather than absence of correlations ‘unexpected’ findings (ref). Findings that are not deemed ‘publishable’ are either sent to the ‘file drawer’ or else subject to practices, such as ‘p-hacking,’ that make the results more likely to be published. Publication bias and the file drawer effect facilitate the dissemination of false knowledge, consequently, QRPs bias the literature and ultimately undermine the credibility and reproducibility of research findings [@Cockburn2020; @Agnoli:2017kl]. Current definitions of QRPs are focused on hypothesis testing research (specifically null-hypothesis significance testing), with QRPs typically defined as practices that inflate the chance of finding a false positive result (type I error) (ref? Simons maybe?). Common examples of QRPs are centred around practices that manipulate p-values. Whilst Null Hypothesis Significance Testing has been and still is in popular use in ecology [@Fidler:2017he; @Stephens2007], a large amount of applied problem-solving work in the field is model-based. Ecological modellers may be inclined to think that the concerns associated with QRPs and reproducibility do not apply to them. In practice, this work is quite different to null hypothesis significance testing: Instead of relying on a single quantitative measure, studies describing ecological models usually generate a suite of measures and analyses that pertain to different desirable attributes of the model and collectively inform claims about the model being sufficient to fulfil its intended purpose. This begs the question of whether QRPs are relevant to model-based research? If specific QRPs related to p-values, like p-hacking, don’t apply, are there parallel or counterpart practices that do? What form might they take? And where in the research process would we be likely to find them? This paper addresses each of these questions. Our primary aim in this paper is to highlight the range of particular decision-points and practices that could be questionable so as to identify potential QRPs in model-based research. We encourage modellers to pause at such junctions and assess for themselves what the relative risk of QRPs is.

```{r}
#| label: create-QRP-table
#| message: false
#| warning: false
#| echo: false

image_out <- function(x, height = 50){
  out <- map_chr(x, gt::local_image, height = height)
  out <- map(out, gt::html)
  out
}

QRP_tbl <- readr::read_csv(here::here("data", "tbl-QRP-frequency.csv"),
                show_col_types = FALSE) %>% 
  mutate(Category = here::here("data", "icons", 
                               glue::glue("{Category}.png"))) %>%
  gt::gt(groupname_col = "Category",
         row_group_as_column = TRUE,
         caption = 'Examples and self-reported frequency of questionable research practices in hypothesis-testing research. QRPs are categorised as "cherry-picking", "p-hacking", and "methodologically flawed", indicated by the cherry, saw, and cross icons respectively.') %>% 
  gt::tab_footnote(footnote = "Makel et al. (2019)", 
                   locations = cells_column_labels(columns = Abbreviation)) %>% 
  gt::tab_footnote(footnote = gt::md("Fraser et al. (2018), $n=494$"), 
                   locations = cells_column_labels(columns = 
                                                     c("Ecology", 
                                                       "Evolution"))) %>% 
  gt::text_transform(locations = gt::cells_row_groups(everything()),
                     fn = image_out
  ) %>% 
  gtExtras::gt_theme_nytimes() %>% 
  gt::cols_hide("Evolution")


QRP_tbl_html <- QRP_tbl %>% 
  gtExtras::gt_plt_bar_pct(
    fill = "purple",
    column = c("Ecology"),
    scaled = TRUE,
    width = 150,
    labels = TRUE,
    background = "lightgrey"
  ) %>% 
  gt::cols_width(
    Category ~ gt::pct(10),
    `Questionable Research Practice` ~ gt::pct(40),
    Abbreviation ~ gt::pct(30),
    Ecology ~ gt::pct(20)
  ) %>% 
  gt::tab_options(quarto.disable_processing = FALSE)

```

::::: landscape
::: {.content-visible when-format="html"}
```{r}
#| label: tbl-QRP-examples-html
#| column: body
QRP_tbl_html
```
:::

::: {.content-visible when-format="typst"}
```{r}
#| label: tbl-QRP-examples-typst
#| column: body
QRP_tbl %>% 
  gt::fmt_percent(
    columns = Ecology,
    scale_values = FALSE,
    decimals = 0
  )
```
:::
:::::

### Why QRPs in Hypothesis Testing don’t directly translate to modelling

One principle underlying the classification of these practices as ‘questionable’ relates to data driven decision making. In contrast to hypothesis testers, modellers makes a series of analytic decisions on the basis of both objectively identified model performance criteria and subjective judgements [@Babel2019; @Bennett2013], wherein which they shift between subjecting the model to analysis, validation and testing, and refining the model in response to those results [@Getz2017]. This process is non-linear, iterative, and generates many interim versions of the model preceding publication [@Augusiak2014], unfolding much like the ‘survival of the fittest’ model(s) [@Prosperi2019] given a specified purpose and the available resources. In modelling, analytic decision-making is adaptive and data-dependent. The emphasis of existing QRP definitions on type I errors is unhelpful for model-based research. Firstly, multiple sources and types of error may arise in the modelling process - there is model structural uncertainty, uncertainty in parameter estimates, and uncertainty in predictions. What constitutes an ‘error’, what the source of that error is, as well as the relative weighting of those different errors, depend on some combination of the model’s purpose (prediction, explanation, description), the type of model used (phenomenological or mechanistic) and the context for the model. Rather than relying on p-value as a metric, model performance metrics consist of both qualitative and quantitative measures that incrementally build a subjective picture of model credibility rather than providing dichotomous responses assessing model credibility [@Augusiak2014; @Hamilton2019]. Since the publishability of a model-based study hinges on the collection of these model outputs and their contribution to overall model credibility, each of these model outputs may be susceptible to questionable research practices in order to strategically alter the perceived credibility of the model. What can be ‘tweaked’, ‘hacked’ or ‘exploited’ in hypothesis testing are *p*-values and whilst there may not be a direct counterpart in model-based research, we do expect similar a similar motivation from publishability to apply.

## A Conceptual Framework of Questionable Research Practices in Ecological Modelling

## A Roadmap of QRPs in the Ecological Modelling Process

### Classes of QRPs

```{r}
#| label: waffle_data_prep
#| message: false
#| warning: false
waffle_plot_data <- 
        tidy_QRP_database(here::here("data", "QRP_database.csv")) %>% 
        filter(include) %>% 
        select(-notes, -model_subphase, -source, -reason_coded, -include) %>% 
        distinct() %>% #rm duplicate practice_coded #TODO next merge duplicates while keeping source
        drop_na() %>% #interim approach until datachecks in place
        mutate(model_phase = str_split(model_phase, ", "),
               target = str_split(target, ", "),
               values = 1) %>% 
        unnest(model_phase) %>% 
        unnest(target) %>% 
        complete(practice_coded, 
                 model_phase, 
                 target, 
                 fill = list(values = 0.0001) # see https://github.com/hrbrmstr/waffle/issues/66
                 ) %>%  
        group_by(practice_coded) %>% 
        fill(qrp_coded, model_phase, .direction = "downup") %>% 
        ungroup()
```

```{r}
#| label: tbl-QRP-categories
#| tbl-cap: "Classes of Questionable Research Practices (QRPs) in ecological modelling."
#| echo: false

qrp_class_descriptions <- tribble(~ qrp_coded, ~ description,
        "S-hacking", "S- or *statistic* hacking: Manipulating model inputs, outputs or the model itself to obtain a favourable value of a performance measure, usually by systematically altering the modelling until a desired level of a statistic or metric is obtained [@Nakagawa2025; @Nagy2025]. This can include changing random seeds, outcome variable and/or performance thresholds, or performance metrics. Likely to occur when there are threshold dependent binary (or even ordinal) outcomes that are linked to whether the model will be publishable or not, in such instances S-hacking is analagous to *p*-hacking. Examples of threshold-dependent performance measures include partial ROC, true skill statistic (TSS), sensitivity and specificity [@feng2019]. <br>When there are multiple criteria informing model performance or multiple model performance tests, the relative weighting may be changed after observing results to make the model appear more credible [@Benett2013] (Or is this over-hyping).",
        "Overhyping", "Exaggerating or overstating the capabilities, generalisability, or reliability of a model beyond what is justified by the evidence. This can involve overstating a model's real-world applicability or predictive power.",
        "Sample Curation", "Based on Nagy et al. [-@Nagy2025]. Selectively including, excluding, or modifying data points in the sample used to develop or evaluate a model, often to improve apparent model performance. This can include practices like cherry-picking favourable data or arbitrarily removing outliers.",
        "Model Dredging", "Searching through a large number of possible model specifications or variable combinations without theoretical justification, in an attempt to find statistically significant or favourable results. This is sometimes referred to as 'fishing expeditions' in model space.",
        "Selective Debugging", "The practice of only checking for and correcting errors in a model when results are unexpected or unfavourable, rather than performing systematic error checking regardless of outcomes.",
        "Overfitting", "Developing a model that fits the training data too closely, including noise and peculiarities specific to that dataset, resulting in poor generalisability to new data. This often occurs when models are made overly complex relative to the amount of training data available.",
        "Executing Alternative Analyses", "Conducting multiple different analyses or model variations and selectively reporting only those that yield favourable or statistically significant results, without disclosing the full range of analyses performed.",
        "Selective Reporting", "Also termed *cherry-picking*. Choosing to report only certain results, metrics, or comparisons that support desired conclusions while omitting less favourable findings. This can create a misleading impression of model performance or study outcomes.")

waffle_plot_data %>% 
  distinct(qrp_coded) %>% 
  arrange(qrp_coded) %>% 
  left_join(qrp_class_descriptions, by = join_by(qrp_coded)) %>% 
  gt::gt() %>% 
  gt::cols_label(qrp_coded = "QRP Class") %>% 
  gtExtras::gt_theme_nytimes() %>% 
  gt::cols_width(
    qrp_coded ~ gt::pct(15),
    description ~ gt::pct(85)
  ) %>% 
  gt::fmt_markdown(columns = description) 

```

### QRPs in the Modelling Process



```{r}
#| label: fig-waffle_plot
#| fig-cap: "Summary of Questionable Research Practices (QRPs) in ecological modelling. QRPs may target model inputs (yellow squares), the model itself (red squares), and/or model outputs (green squares), and may occur at different phases in the modelling cycle. QRPs are grouped according to broader classes defined in @tbl-QRP-categories. See @tbl-QRP for the full list of QRPs we identified."
#| fig-width: 14
#| fig-height: 18

# #| dev: "cairo_pdf" # when output is pdf
# options(hrbrthemes.loadfonts = TRUE)
waffle_plot <- waffle_plot_data %>% 
        distinct() %>% #TODO identify duplicated rows - suspect non-unique target coding is cause
        mutate(model_phase = 
                       forcats::as_factor(model_phase) %>% 
                       forcats::fct_relevel( c("Data", 
                                               "Model Construction", 
                                               "Model Evaluation",
                                               "Model Application"))) %>% 
        arrange(model_phase, 
                qrp_coded) %>% 
        ggplot(aes(fill = target, 
                   values = values)) +
        geom_waffle(size = 2, 
                    color = "white", 
                    make_proportional = FALSE, 
                    flip = TRUE) +
        scale_x_discrete(expand = c(0,0)) +
        scale_y_discrete(expand = c(0,0)) +
        ggsci::scale_fill_futurama(name = "QRP Target") +
        coord_equal() +
        facet_nested(qrp_coded + practice_coded ~ model_phase, 
                     switch = "y", 
                     labeller = 
                             labeller(
                                     practice_coded =
                                             label_wrap_gen(width = 80,
                                                            multi_line = TRUE),
                                     qrp_coded =
                                             label_wrap_gen(width = 10,
                                                            multi_line = TRUE),
                                     model_phase = 
                                             label_wrap_gen(width = 10, 
                                                            multi_line = TRUE)),
                     nest_line = element_line(linetype = 1),
                     solo_line = TRUE,
                     space = "free",
                     strip = strip_nested(size = "variable",
                                          clip = "off")
        ) +
        # facet_grid(practice_coded ~ model_phase, 
        #            switch = "y", 
        #            labeller = labeller(practice_coded = 
        #                                        label_wrap_gen(width = 100))) +
        # theme_no_axes() +
        hrbrthemes::theme_ipsum_rc() +
        waffle::theme_enhance_waffle() +
        theme(strip.text.y.left = element_text(angle = 0, 
                                               size = 12),
              strip.text.x = element_text(size = 14,vjust = 0),
              strip.background.x = element_part_rect(side = "b"),
              strip.background.y = element_blank()
        ) +
        ggh4x::force_panelsizes(rows = unit(1, "cm"), 
                                cols = unit(3, "cm"))

print(waffle_plot)



#DONE reorder the model process levels
#TODO exclude less important QRPs
#DONE Make strips pretty
#DONE recolour model object colours
#DONE group the QRPs into their umbrella terms -- nested facets?

```





::: {.landscape}

`#show figure: set block(breakable: true)`{=typst}

```{r}
#| label: tbl-QRP
#| tbl-cap: "Full list of Questionable Research Practices identified from the literature. We categorised QRPs into broader classes"
#| column: body-outset
#| warning: false
#| message: false

gt_qrp <- tidy_QRP_database(here::here("data", "QRP_database.csv")) %>% 
        make_qrp_table_data() %>% 
        select(-notes) %>% 
        group_by(qrp_coded) %>% 
        arrange(qrp_coded) %>% 
        make_QRP_table() %>% 
        gt::fmt_markdown(columns = c(qrp_coded, practice_coded, source)) %>% 
        gt::cols_hide(columns = c(-ends_with("coded"), -source, -target) ) %>% 
        gt::cols_label(qrp_coded = "QRP Class") %>% 
        gtExtras::gt_theme_nytimes()

gt_qrp
```
:::

## References {.unnumbered}

::: {#refs}
:::




# Session Info {.unnumbered}
```{r}
#| label: session-info

devtools::session_info()
```

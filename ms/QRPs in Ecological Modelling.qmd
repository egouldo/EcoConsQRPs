---
title: A Roadmap of Questionable Research Practices in Ecological Modelling
running-head: Questionable Research Practices in Ecological Modelling
author:
  - name: Elliot Gould
    email: elliot.gould@unimelb.edu.au
    orcid: 0000-0002-6585-538X
    url: https://github.com/egouldo
    corresponding: true
    affiliation:
      - ref: 1
  - name: Hannah S. Fraser
    orcid: 0000-0003-2443-4463
    affiliation:
      - ref: 1
  - name: Bonnie C. Wintle
    orcid: 0000-0003-0236-6906
    affiliation:
      - ref: 1
  - name: Libby Rumpff
    orcid: 0000-0001-9400-8086
    affiliation:
     -  ref: 1
  - name: Fiona Fidler
    orcid: 0000-0002-2700-2562
    affiliation:
      - ref: 2
affiliations:
  - id: 1
    name: The University of Melbourne
    department: School of Agriculture, Food and Ecosystem Sciences
  - id: 2
    name: The University of Melbourne
    department: School of Historical and Philosophical Studies
format:
  html:
    page-layout: full
    title-block-banner: true
    code-fold: true
    number-sections: true
    number-depth: 3
  pdf:
    knitr:
      opts_chunk:
        dev: "cairo_pdf"
    keep-tex: true
    documentclass: scrartcl
    papersize: A4
    fontsize: 11pt
    toc: false
    number-sections: true
    linkcolor: blue
    include: false
    echo: false
    header-includes:
    - \usepackage{mathtools}
    - \usepackage{amssymb}
    - \usepackage{xcolor}
    - \usepackage{booktabs, caption, longtable, colortbl, array}
    - \usepackage[font=small,labelfont={bf,small}]{caption}
custom-numbered-blocks:
  classes: 
    Box:
      boxstyle: foldbox.simple
      collapse: false
pdf-engine: lualatex
abstract: |
  1. Questionable research practices (QRPs) bias the published literature towards apparently strong and conclusive results, resulting in low rates of replicability. Recent metaresearch reveals that ecology is not immune to the ‘reproducibility crisis’ seen in other disciplines, due to similar rates of QRPs and a lack of transparency in published research. However, metaresearch to date focusses on hypothesis-testing research and treats data-dependent analytic decisions as inherently questionable. This is not a good fit for ecology and related fields that conduct exploratory or predictive research using complex models, where data-dependent decisions are often necessary and legitimate aspects of the modelling process.
  2. To aid in understanding why and how frequently QRPs occur, and how severe the consequences might be, we develop a conceptual framework describing QRPs in ecological modelling, distinguishing questionable from legitimate data-dependent decisions. We present a typology of QRPs by decision-making mechanism and target and reframe QRPs in modelling as practices that inflate perceived model credibility, rather than producing false-positive statistical results. 
  3. We identified six QRP classes that may occur at various points in the modelling process: selective reporting, S-hacking, (manipulating performance metrics), model fishing, sample curation, HARKing and overhyping. These practices threaten the reliability and reproducibility of model-based research by artificially inflating the apparent credibility of models.
  4. We aim to raise awareness among modellers about different types of QRPs and how they might emerge in ecological modelling. We offer strategies to mitigate QRP risks, while preserving legitimate adaptive decision-making characteristic of ecological modelling.
keywords: 
  - questionable research practices
  - ecological modelling
  - metaresearch
  - transparency
  - reproducibility
  - model credibility
  - researcher degrees of freedom
date: last-modified
bibliography: references.bib
filters:
- ute/custom-numbered-blocks
- callout-box.lua
- addcalloutnumber.lua
---

```{r}
#| label: setup
#| warning: false
#| message: false
library(tidyverse)
library(easystats)
library(here)
library(janitor)
library(glue)

library(gt)
library(gtExtras)

library(ggh4x)
library(ggforce)
library(waffle)
library(ggsci)
library(hrbrthemes)
library(patchwork)

library(extrafont)
library(firasans)
# extrafont::loadfonts()

library(latex2exp)
library(equatiomatic)

source(here::here("R/tidy_QRP_database.R"))
source(here::here("R/make_table_data.R"))
source(here::here("R/make_QRP_table.R"))
# source(here::here("R/synthetic_example_QRPs.R")) # quarto processing is applied to gt table when sourced here! GRRR leaving here as reminder to run in final render pipeline
```

# Introduction

Self-report surveys of researchers’ statistical practices suggest high rates of Questionable Research Practices (QRPs, e.g., p-hacking, cherry picking, @tbl-QRP-examples-html @tbl-QRP-examples) in several different disciplines: psychology [@John:2012eo], education [@Makel2023] and ecology [@Fraser:2018cl]. Widespread QRPs, accompanied by a lack of transparency and openness in research reporting [@culina2020], can leave disciplines at risk of a ‘reproducibility crisis’ [@Fidler:2017he]. QRPs are a set of methodological and statistical practices that fall into an “ethical grey zone” between ideal responsible research conduct and unacceptable research behaviours like fabrication and plagiarism [@Butler:2017ks p. 94]. Researchers are motivated actors prone to cognitive biases incentivised by a ‘publish or perish’ research culture in which publication bias selects for particular types of findings [@Agnoli:2017kl]. These findings are generally statistically significant results or ‘unexpected’ / ‘surprising’ findings [@Nissen2016; @Nakagawa2025]. Findings that are not deemed ‘publishable’ (in a Null-Hypothesis Significance Testing framework \[NHST\], these are often results with p values $> 0.05$) are either sent to the ‘file drawer’ or else subject to QRPs, such as ‘p-hacking,’ that make the results more likely to be published.

@Fraser:2018cl surveyed ecology researchers, asking how often they used the Questionable Research Practices (QRPs) that have been identified in other fields. Many of these QRPs relate to p-values. The study found these practices were used at similar rates to other fields, but with notably higher rates of analysis gaming and cherry-picking [@John:2012eo; @Fraser:2018cl; @Makel2023; @Chin2023]. These differences may reflect the greater prevalence of model-based research in ecology, raising questions about whether, and in what contexts, these practices are problematic for modelling. Other research shows that the same pressures and motivations associated with QRPs in other fields are present in ecology, including publication pressure, pressure to obtain funding, and career incentives [@Fidler:2017he; @ODea2021a]. p-curve analysis has revealed the effects of both reporting and publication bias in ecology studies featuring p-values [@kimmel2023]. While @Ottaviani2023 found evidence of publication bias favouring novel results, with increasing use of novelty terms in ecological abstracts over time, especially in high-impact journals. In addition to selective reporting of results, ecological research is plagued by underpowered studies, which exaggerate effect sizes in the published literature [@Parker2023; @Yang2023c]. These studies underscore the prevalence of reporting and publication bias in ecology, ultimately influencing conclusions drawn from the literature [@deressa2023].

Current definitions and lists of QRPs are focused on hypothesis testing research (specifically NHST, @tbl-QRP-examples-html @tbl-QRP-examples), characterising these practices as those that inflate the probability of false positive findings [see Table 3 in @Nagy2025]. Which makes sense, since this is the primary statistical estimand on which a finding is deemed ‘publishable’ in null-hypothesis significance testing. This NHST-centric focus creates particular challenges for ecology, because, although Null Hypothesis Significance Testing is still popular in ecology [@Fidler:2017he; @Stephens2007], model-based methods in ecology are increasingly common, especially within applied research contexts [@Connolly2017; @Garcia-Diaz2019; @DeAngelis2021]. The emphasis of existing QRP definitions on Type I errors is unhelpful for model-based research. Multiple sources and types of error may arise in the modelling process — there is model structural uncertainty, uncertainty in parameter estimates and predictions, and uncertainty in scenarios [@Rounsevell2021; @Simmonds2024]. What constitutes ‘error’, what the source of that error is, as well as the relative weighting of those different errors, depend on some combination of the model’s purpose (prediction, explanation, description), the type of model used (correlative or mechanistic) and the context for the model. Ecological modellers may be inclined to think that the concerns associated with QRPs and reproducibility are irrelevant, since many of the practices described as questionable within an NHST context do not directly relate to their work.

```{r}
#| label: create-QRP-table
#| message: false
#| warning: false
#| include: false

QRP_tbl <- readr::read_csv(here::here("data", "tbl-QRP-frequency.csv"),
                           show_col_types = FALSE) %>% 
  mutate(Category = here::here("data", "icons", 
                               glue::glue("{Category}.png"))) %>%
  gt::gt(groupname_col = "Category",
         row_group_as_column = TRUE
  ) %>% 
  gt::tab_footnote(footnote = "Makel et al. (2019)", 
                   locations = cells_column_labels(columns = Abbreviation)) %>% 
  gt::tab_footnote(footnote = "Fraser et al. (2018), n=494", 
                   locations = cells_column_labels(columns = 
                                                     c("Ecology", 
                                                       "Evolution"))) %>% 
  gt::cols_align(align = "center", columns = "Category")

image_out <- function(x, height = 50){
  out <- map_chr(x, gt::local_image, height = height)
  out <- map(out, gt::html)
  out
}

QRP_tbl_html <- QRP_tbl %>% 
  gt::text_transform(locations = gt::cells_row_groups(everything()),
                     fn = image_out
  ) %>% 
  gtExtras::gt_theme_538() %>%
  gtExtras::gt_plt_bar_pct(
    fill = "green",
    column = c("Ecology"),
    scaled = TRUE,
    width = 100,
    labels = TRUE,
    background = "lightgrey"
  ) %>% 
  gtExtras::gt_plt_bar_pct(
    fill = "blue",
    column = c("Evolution"),
    scaled = TRUE,
    width = 100,
    labels = TRUE,
    background = "lightgrey"
  ) %>% 
  gt::cols_width(
    Category ~ gt::pct(8),
    `Questionable Research Practice` ~ gt::pct(30),
    Abbreviation ~ gt::pct(20),
    Ecology ~ gt::pct(10),
    Evolution ~ gt::pct(10)
  ) %>% 
  tab_style(style = cell_text(color = "black", size = "medium"),
            locations = cells_column_labels())

QRP_tbl_html %>% 
  gt::gtsave(here::here("figures", "tbl-QRP-examples.png"), 
             vwidth = 800,
             vheight = 1000)
```

::: {.content-visible when-format="html"}
```{r}
#| label: tbl-QRP-examples-html
#| tbl-cap: 'Examples and self-reported frequency of questionable research practices in hypothesis-testing research in Ecology and Evolutionary Biology. QRPs are categorised as "cherry-picking", "p-hacking", and "methodologically flawed", indicated by the cherry, saw, and cross icons respectively.'
QRP_tbl_html
```
:::

::: {.content-visible when-format="pdf"}
```{r}
#| include: true
#| label: tbl-QRP-examples
#| tbl-cap: "A test caption"

magick::image_read(here::here("figures/tbl-QRP-examples.png") )
```
:::

Building on these limitations, we argue that current QRP frameworks fail to address model-based research because the underlying research processes are fundamentally different. One principle underlying the classification of these practices as ‘questionable’ relates to data driven decision making. In contrast to hypothesis testers, modellers make a series of analytic decisions on the basis of both objectively identified model performance criteria and subjective judgements [@Babel2019; @Bennett2013], wherein which they shift between subjecting the model to analysis, validation and testing, and refining the model in response to those results [@Getz2017]. This process is non-linear, iterative, and generates many interim versions of the model preceding publication [@Augusiak2014], unfolding much like the ‘survival of the fittest’ models[@Prosperi2019], given a specified purpose and the available resources. In modelling, analytic decision-making is adaptive and data-dependent.

Rather than dichotomous inferences relying on almost exclusively on p-values, model performance metrics consist of both qualitative and quantitative measures that incrementally build a subjective picture of model credibility [@Augusiak2014; @Hamilton2019]. Since the publishability of a model-based study hinges on the collection of these model outputs and their contribution to overall model credibility, each of these model outputs may be susceptible to QRPs that aim to strategically alter the perceived credibility of the model.

Therefore, we argue that a conceptual framework of QRPs in model-based research must account for certain kinds of data-dependent decisions, which are appropriate and justifiable aspects of the modelling process, while highlighting the primacy of guarding against data-dependent decision-making that might be questionable. Moreover, the conceptual framework should de-emphasise the risk of type I errors and false positive findings to account for other biases more relevant to how complex models are evaluated and used. This begs the question of whether QRPs are relevant to model-based research? If specific QRPs related to p-values do not apply, such as p-hacking, are there parallel or counterpart practices that do? What form might they take? And where in the research process would we be likely to locate them? This paper addresses each of these questions. Our primary aim is to highlight the range of specific practices that are problematic in some circumstances so as to identify QRPs and their decision-points in model-based research. We encourage modellers to pause at such junctions and assess for themselves what the risk of QRPs is, in their context. We aim to raise awareness among ecologists (and modellers within other disciplines) about the potential for QRPs throughout the modelling process and hope to facilitate future attempts to both estimate the severity and extent of QRPs and mitigate the risk of QRPs in in ecology by providing a more comprehensive account of what constitutes questionable practice in model-based research.

# Conditions for Questionable Research Practices in Ecological Modelling

Below we outline the conditions under which QRPs may arise in ecological modelling and give an overview of the modelling process and the 'objects' it produces (inputs, outputs, the model itself). This sets the scene for exploring how QRPs could unfold in model-based research.

## What makes a model publishable? Identifying motivations for QRPs

Understanding what makes ecological models "publishable" is crucial for identifying where QRPs might emerge, since publication bias provides a primary motivation for engaging in questionable practices [@Ware2015]. Unlike hypothesis testing research where p-values serve as the main target for manipulation, model-based research involves multiple attributes that collectively determine publishability. These attributes become potential targets for the QRP classes we later identify in our typology (@tbl-QRP-typology).

### Model Advantage and Novelty

Novelty is an important factor influencing the publishability of modelling research. Publication bias and funding stipulations reward and require advantage over existing approaches; either through development of new methodological approaches, enhanced performance of existing algorithms and modelling methods, or application of existing models to new contexts, such as new environmental conditions or ecological contexts [@Alexandrov:2011iv]. Conversely, publication bias disincentivises the evaluation and testing of existing published models [@Babel2019; @Houlahan:2016fl]. This requirement for novelty incentivises model fishing and selective reporting, where researchers may opportunistically explore new modelling approaches until they achieve apparent superiority over baseline approaches.

### Model Credibility

Model Credibility is based on the subjective degree of confidence that both the model and model-derived inferences about the real system can be used as claimed [@Risbey2005; @Augusiak2014; @Hamilton2019]. That is, can the model adequately answer the research question (Christin et al 2020), and can it be used reliably to inform management decisions [@Alexandrov:2011iv]? Credibility emerges gradually throughout the modelling process, by demonstrated adequacy [conceptual validity and predictive accuracy, @RykielJr1996] and reliability [consistent performance and transferability, @Schmolke:2010fd; @Yates2018]. Unlike the binary nature of statistical significance, model credibility builds incrementally through multiple performance metrics and evaluation approaches (Figure 1, model outputs). This multi-faceted assessment creates numerous opportunities for statistic hacking, or “S-hacking” (Table 3 and 4), where researchers can manipulate:

-   Performance metric selection and thresholds,
-   Validation approaches and data partitioning strategies, and
-   Evaluation timeframes and spatial scales.

The subjective nature of credibility assessment also fosters *overhyping* (Table 3), where model capabilities are overstated beyond what evaluation results justify.  

### Purpose-dependent Vulnerabilities

Models that serve different purposes are vulnerable to different QRPs and depending on the combination of modelling approach, model type (e.g. agnostic, correlative, or mechanistic) and purpose of the model, QRPs will target different *estimands* of interest [@Hoffmann2021] . For instance, explanatory models are more susceptible to practices targeting parameter estimates and goodness-of-fit measures, whilst predictive models face greater risks around forecast accuracy metrics and transferability claims, because in each case a different *estimand*, or "target quantity to be estimated in an analysis" [@Borger2025, p. 2], is sought after by the researchers. Different questionable practices are concentrated at different locations across modelling phases (Figure 2).

## Which modelling objects (inputs, outputs, the model itself) might be affected by QRPs?

To help conceptualise where in the modelling process QRPs might emerge, and which 'model objects' QRPs may affect, we first  give an overview of the modelling process, articulating various inputs and outputs, including the model itself, model fit statistics, summary measures and other evaluation results (Figure 1), to which we ascribe mathematical notation[^1]. This framework will also provide the foundations for extending Gelman and Loken’s [-@Gelman2013] mathematical formalism to modelling (which we do in section 4).

[^1]: Boldface notation represents a vector or a set, indicating where multiples of those objects could be generated, e.g. there may be multiple ways to operationalise a conceptual model.

We acknowledge the plurality and lack of consensus in how the modelling process is described [@Lahtinen2017], including the terminology used for different modelling phases, steps and tasks [@Schmolke:2010fd; @Augusiak2014]. Rather than adopting a comprehensive taxonomy that captures all distinct processes and categories of modelling, we instead describe the modelling process at a high-level that can be generalised across different model purposes, contexts, types and methods. There will, of course, be exceptions. Some aspects may not apply in every modelling problem, and the specific collection of model objects, their relative weighting in informing study conclusions, and the relative weighting of publishable attributes, will differ depending on the model purpose, context and methodology applied to the problem at hand. We also recognise that analysis decisions are *procedurally* dependent [@Liu2020], for instance, the way we specify and parameterise models depends on the model type [e.g. whether you're using a correlative, mechanistic, or agnostic model, *sensu* @Hoffmann2021] and modelling purpose [exploration, inference, prediction, see @Tredennick2021].

We divide the modelling process into three phases, 1) model construction, 2) model evaluation, and 3) model application. These distinctions align with the phases underpinning the preregistration template in Chapter 6 (Appendix X).

```{r}
#| label: fig-modelling-process
#| fig-cap: |
#|   Three phases of model development: model construction, where the conceptual model $M_c$ is specified into the formal model (${M_c\rightarrow M}_s$) then parameterised ($M_s{\rightarrow M}_p$); model evaluation, where the calibration and validation fits are evaluated, possibly leading to re-specification and re-parameterisation (dashed arrow); and model application, where the model is analysed to answer the research question. <br><br>
#|   Modelling generates objects, including the conceptual, specified and parameterised model, ($\boldsymbol{M}=\left\{M_c,M_s,M_p\right\}$, purple circles); model inputs (blue squares), including hyper parameters $\lambda$ and calibration settings, data $\boldsymbol{X}$  for model parameterisation, evaluation and application. Model outputs ($M_j$ green hexagons) include model predictions $\hat{y}$, which are used to characterise model performance during model evaluation or subject to further aggregation, transformation, analysis and visualisation during model application. <br><br>
#|   Note: Data analyses may also inform model specification during construction. New or alternative input data may be used during scenario analysis to make predictions or projections about how the system will respond to intervention $\hat{y}$.
#| include: true
#| echo: false

magick::image_read("images/Figure_1.png")
```

### Conceptual model, $M_c$

To begin the model development process, a conceptual model, $M_c$ or *candidate set* of models, $\boldsymbol{M_c}$ is specified by the modeller, synthesising their understanding of the ecological system. Conceptual models may be represented by a set of qualitative statements, mathematical formulas, or else visually as plots or directed acyclic graphs [@Shmueli2010]. A candidate set of multiple models at this stage may represent competing hypotheses, or critical uncertainties in ecological processes, principles, variable importance or framing.

### Specified Model, $M_s$

Next, the modeller formalises each conceptual model mathematically or statistically, $M_s$ (Figure 1). The modeller chooses which variables should be included in the model, how to operationalise or represent them in the chosen framework, what the appropriate dependencies are between variables and the model type, and the functional form of the model (if relevant). Because the variables in the conceptual model are not directly observable, they are operationalised into measurable outcomes $Y$ and input variables $X$ in a data matrix $\boldsymbol{X} = {X_1, X_2,\ldots,X_p}$, where $p$ is the number of input variables, and *f* represents a function relating $Y$to$X$such that $E\left(Y\right) = f\left(\boldsymbol{X}\right)$ [@Hoffmann2021; @Shmueli2010]. Note that, for some predictive modelling contexts, such as data-driven modelling employing black-box algorithmic approaches, like machine-learning, $f$ may not be specified and is instead represented by $\mathscr{I}_{\lambda}$ where $\mathscr{I}$ represents some learning algorithm and $\lambda$ denotes its hyperparameters [following @Bischl2023]. Exploratory analyses are often conducted at this stage to inform variable selection, for example by analysing variable importance and examining collinearity among variables [@Kass2025].

### Parameterised Model, $M_p$

Next, each specified model $M_s$is parameterised yielding $M_p$ (Figure 1). Model parameters refer to any component of a model that can be quantified or estimated, such as slopes or intercepts in a linear regression or growth rate in a population model [@Garcia-Diaz2019, p. 2]. Regardless of the overarching model purpose [e.g. explanation or prediction, @Shmueli2010], for correlative or agnostic (e.g. machine-learning) models, parameterisation typically occurs by *estimation*, or *calibration,* whereby the modeller applies techniques, like maximum-likelihood estimation or Bayesian inference, to the data matrix $\boldsymbol{X}$ (Figure 1) to estimate the parameters specified by $f$, with uncertainty [@Garcia-Diaz2019; @Hoffmann2021], yielding $\hat{y}=\hat{f}\left(\boldsymbol{X}\right)$. In the case of agnostic models, the algorithm $\mathscr{I }_{\lambda}$ returns the fitted model and its parameters when applied to $\boldsymbol{X}$, $\hat{y}=\hat{f}_{\hat{\lambda}}\left(\boldsymbol{X}\right)$. Parameters of mechanistic models are typically provided as inputs to the specified model $f$*,* gleaned from expert knowledge, published literature or else via calibration [@Hoffmann2021].

When conducting inference or explanatory modelling, the estimand(s) of interest are the parameters $\boldsymbol{\hat{\theta}}$, like standardised mean differences, correlation coefficients or response ratios [@Williams2025],whereas for predictive modelling, predicted values $\hat{y}$ constitute the estimand(s) of interest [@Tredennick2021; @Hoffmann2021; @Shmueli2010]. This is true regardless of whether the model is correlative [e.g. a correlative SDM, @Briscoe2019, or mixed-effects linear regression model, @silk2020], mechanistic (e.g. a population viability model), or agnostic [e.g. a machine learning or deep learning models, @Pichler2023]. However, different types of models are more likely to be used for inference or prediction in practice, for example, agnostic models are more likely to be used for prediction, but inferences about parameters are certainly possible[@Lucas2020]. Note that agnostic modelling approaches require the modeller to supply hyperparameters $\boldsymbol{\lambda}$ (figure 1), which may be decided by the modeller, or else estimated by some tuning or optimisation method $\boldsymbol{\hat{\lambda}}$. Hyperparameters may influence the model learning process, such that with each set of hyperparameters the model will provide a different set of results [@Ahmed2025].

### Model Evaluation: Characterising Model Performance and Fitness for Purpose

Arriving at the optimal final model or collection of final models is typically iterative, determined by the outcomes of model validation and evaluation [@Shmueli2010] whereby the model is subjected to a series of analyses that generate performance measures that are used to establish its validity, reliability and credibility and ensuring that the model is fit for intended use [@Bennett2013; @eker2018; @Garcia-Diaz2019; @RykielJr1996].

We distinguish between model validation and model evaluation. Model validation checks that the fitted model $M_p$ suitably approximates the data $\boldsymbol{X}$, and is evaluated using goodness-of-fit tests, and model diagnoses like residual analyses [@Shmueli2010]. Model selection whittles down the candidate set of models into a single ‘best’ model or a smaller subset of ‘best models’ (e.g. AIC within $\Delta2$), after which the modeller might choose to consider multiple models or conduct model averaging. Outside of formal model selection approaches, the procedure for determining the best model(s) may involve a degree of trial and error of different model structures that is not always pre-planned (i.e. new $M_s$ are specified after validation).

Models used for inference are assessed on how well $M_p$ fits or explain the data $\boldsymbol{X}$, informed by measures like ${R}^2$, statistical significance and F statistics, whereas predictive models are additionally assessed on their predictive power or accuracy on new observations of $\boldsymbol{X}$ [@Shmueli2010]. Depending on the type of prediction (e.g. classification, predicted probabilities of different states or classes, ranking of probabilities or point predictions) different performance metrics will be appropriate [@Shmueli2010].

Following validation, model evaluation of the best model(s) is undertaken, assessing the fitness of $M_p$ for purpose by calculating additional performance measures to fully understand the model’s capabilities, like constructing confusion matrices or calculating omission and commission rates. Ultimately, model evaluation is case-dependent and context-specific insofar as the overall evaluation process, types of analyses, metrics, estimand of interest, and desirable properties of the model differing depending on the overarching modelling purpose and type of model and modelling approach [@Tredennick2021; @Bokulich2013].

### Model Application and Analysis

Once $M_p$ is considered plausible and fit for purpose, the modeller shifts to model application [@Engelschalt2023] , querying the model and using the model to undertake analyses that inform the stated research questions (Figure 1). Prior to analysis, model output may be subject to further processing, for example, continuous predictions may be aggregated or transformed into binary predictions for visualisation and communication purposes [@feng2019] . Explanatory model output may be visualised with coefficient plots, or effect size plots to inform the relevance of observed effects [@Ludecke2020]. In applied settings, forecasts or anticipatory predictions into the future or across space are generated from the model based on plausible scenarios or to simulate outcomes under different management actions or policies [@Paniw2023], which may be subject to a range of visualisations [e.g. @Barros2023, Figure 2].

To summarise, a collection of model outputs are generated in modelling, which may variously be the target of QRPs, including: point-estimates; such as means, medians and effect-sizes; uncertainty measures, like confidence intervals, prediction intervals, standard errors; model performance metrics, like $R^2$/AIC/BIC; inference results, like *p*-values, credible intervals, and significance determinations; predictions and forecasts, like future values or classification outcomes; or, the model itself.

# Mapping QRPs onto the Modelling Process

Here we present a taxonomy and map of QRPs derived from the modelling literature, which aims to illustrate the different types of QRPs that might occur throughout different points in the modelling process. We follow with a synthetic example that reveals how these different types of QRPs might look in practice (Box \ref{boxsyntheg}).

## Methods

We surveyed the modelling literature to identify potential QRPs in ecological modelling and their location in the modelling process. We categorised QRPs into broader classes corresponding to families of similar practices using well-known published classifications (e.g. @tbl-QRP-examples-html @tbl-QRP-examples, adopting new classes when there was no analogue in the existing QRP literature. We coded the phase and sub-phase of modelling in which the practice occurs, as well as the *target* of the practice (input, model, output). After initial coding of the QRPs we generalised the descriptions of individual practices and categorised them according to our QRP class schema. Our literature review and coding are described in further detail in @sec-appendix-2.

## Results

We identified 6 classes of QRPs: sample curation, selective reporting, model fishing, S-hacking, overhyping, HARKing (Table 3). All classes of QRPs have analogous practices under NHST, but the practices themselves are not directly comparable. The list of QRPs we identified is not exhaustive and instead illustrates a range of practices that can occur in model-based research (See Appendix X, Table X for the complete list).

We present paradigmatic examples of QRPs for each class in our roadmap, illustrating that QRPs occur throughout all phases of the modelling process, and some may occur at multiple stages (Figure 3). Sample curation, S-hacking and fishing were the classes of QRPs most likely to occur during model construction. The iterative nature of model validation and evaluation creates multiple opportunities for opportunistic optimisation of apparent model performance during model evaluation, with selective reporting, S-hacking and model fishing primarily affecting this phase of modelling. Fewer, but distinct, QRP types were identified for the model application phase, primarily concerning the misrepresentation of model capabilities and findings.

The target objects affected by QRPs across modelling phases reflected the focus of modelling activities and tasks, with QRPs affecting the model itself occurring primarily during model construction and evaluation, and QRPs affecting the model outputs being concentrated in model evaluation and model application phases. Below we briefly describe the different classes of QRPs, providing illustrative examples and explaining how they can bias results.

### Selective Reporting

Selective reporting involves failure to disclose methods and/or results. Selective reporting can be distinguished from other practices, such as S-hacking and model fishing, in that it lends unwarranted credibility to the model, but the model and model outputs, remain unaffected.  Instead of analytic decisions being data-dependent, *the communication of those results* is data-dependent. The ‘garden of forking paths’ is not altered by selective reporting but rather is not fully transparent.

### S-hacking

We expanded the concept of p-hacking and termed it ‘S-hacking’, or ‘statistic hacking’, which encompasses analogous practices in modelling that target metrics that contribute to the publishability of a model. S-hacking involves an element of selective reporting; however, a critical point of difference is that S-hacking involves execution of alternative analyses or manipulation of data, models, outputs and analyses to obtain a favourable result. For example, a modeller may systematically trial multiple different evaluation metrics, selectively reporting only those that present the model in a favourable light [@Hildebrandt2018] . In this instance the model remains unaffected by S-hacking. Alternatively, random seeds in model tuning can be changed after observing test set performance which can drastically alter model results [@Liu2020] . If S-hacking is performed during model construction or validation, or alternative model specifications are trialled after observing model performance results, the model itself is altered, and overfitted to the training data. If S-hacking is performed during model construction or validation, the model is overfitted to the training data and poorly generalises to new data. S-hacking artificially inflates model performance, resulting in spuriously selected models that that may not reflect genuine ecological or predictive relationships. Any performance metric with a threshold dependent outcome [e.g. AUC, TSS, partial ROC, sensitivity, specificity, @feng2019] will be subject to the same types of practices as *p*-hacking.

### Model Fishing

We distinguish ‘model *fishing’* from the methodological technique of ‘model *dredging’* for the purposes of exploration or model selection. In the case of formal model selection procedures employing dredging, there is some a priori chosen objective criteria on which the model is selected, and the model space (usually, though not always) is constrained by a priori specification of candidate models that are theoretically or ecologically motivated. In contrast, when model dredging for the purposes of exploration in pursuit of generating new hypotheses, the initial model space may not be as constrained, but the dredging procedure is transparently reported, and the exploratory nature of the modelling exercise is disclosed and not presented post-hoc as otherwise. Model fishing occurs when the dredging procedure is not disclosed, and/or there is no formal criterion for model selection, and the overarching purpose is not exploration. Alternatively, model fishing can occur without dredging through a large model space, but by conducting alternative analyses or new model variations and selectively reporting only those with favourable results. Model fishing is problematic because of the risk of cognitive biases, such as hindsight bias, where post-hoc rationalisation combined with haphazard model selection leads to spuriously selected models. Model fishing therefore involves an element of systematic exploration of researcher degrees of freedom that is not necessarily planned, nor transparent.

### Sample Curation

Sample curation [*sensu* @Nagy2025] includes a range of data-dependent decisions about model inputs without justification or prior planning, i.e. after model fitting or observing model evaluation or application results. Sample curation may include removing observations that should have been included in order to make a correlation of interest become significant and generating a data-dependent exclusion criteria for those observations being excluded [@Nagy2025]. Opportunistic handling of missing data could occur when a researcher tries multiple ways of handling missing data, for instance, list-wise deletion, multiple imputation and inverse probability weighting. But the expected results only appear with inverse probability weighting and so the researcher only reports this strategy in the paper, omitting the results when the other data handling methods were used [@Nagy2025]. *Opportunistic stopping* occurs when new data is collected and is used to re-parameterise the model after previously observing model validation and model evaluation results, without reporting results of earlier iterations (Table X, Appendix X).

### Hypothesising After Results Are Known (HARKing)

Although the overarching purpose of ecological modelling in applied contexts is not hypothesis-testing, it is important to acknowledge that ecological models implicitly encapsulate hypotheses in the form of assumptions about which patterns, relationships, or predictors are most relevant to the system being modelled [@Bodner2020; @Prosperi2019; @Schuwirth2019]. For example, the choice of which variables to include or exclude from a model are based on implicit hypotheses about which processes are relevant to the system. In the case of modelling, HARKing can occur when a researcher presents a post-hoc explanation and justification for the variables or model structure that performed best, while failing to disclose the initial exploration of other variables or model structures. As such, HARKing in ecological modelling for purposes other than hypothesis testing is likely to occur as an effect of other related QRPs (Table 3) rather than as the motivating practice.

### Overhyping

Overhyping involves claims about the models’ performance that are not substantiated by model evaluation results, such as claiming the model has greater generalisability than it does. A specific form of overhyping involves misreporting correlative claims using causal language, which is particularly common in studies of conservation intervention evaluations using observational study designs [@Josefsson2020]. The practice of implying causation from correlation can cause false confidence in the intervention’s effectiveness while ignoring the real mechanisms for the observed effect. See @Corneille2023 for a summary of communication devices that may be used to overhype the results of a study.

```{r}
#| label: waffle_data_prep
#| message: false
#| warning: false
#| cache: true
waffle_plot_data <- 
        tidy_QRP_database(here::here("data/QRP_database_2025-10-20.csv")) %>% 
        filter(include) %>% 
        select(-practice_notes, -model_subphase, -source, -qrp_reason, -include, -starts_with("practice_"), -file_name, practice_target) %>% 
        distinct() %>% #rm duplicate qrp_coded #TODO next merge duplicates while keeping source
        drop_na() %>% #interim approach until datachecks in place
        mutate(model_phase = str_split(model_phase, ", "),
               practice_target = str_split(practice_target, ", "),
               values = 1) %>% 
        unnest(model_phase) %>% 
        unnest(practice_target) %>% 
        complete(qrp_description, 
                 model_phase, 
                 practice_target, 
                 fill = list(values = 0.0001) # see https://github.com/hrbrmstr/waffle/issues/66
                 ) %>%  
        group_by(qrp_description) %>% 
        fill(qrp_coded, model_phase, .direction = "downup") %>% 
        ungroup()
```

```{r}
#| label: QRP-categories
#| tbl-cap: "Classes of Questionable Research Practices (QRPs) in ecological modelling."
#| echo: false

qrp_class_descriptions <- tribble(~ qrp_coded, ~ description,
        "Selective Debugging", "The practice of only checking for and correcting errors in a model when results are unexpected or unfavourable, rather than performing systematic error checking regardless of outcomes.",
        "Overfitting", "Developing a model that fits the training data too closely, including noise and peculiarities specific to that dataset, resulting in poor generalisability to new data. This often occurs when models are made overly complex relative to the amount of training data available.",
        "Executing Alternative Analyses", "Conducting multiple different analyses or model variations and selectively reporting only those that yield favourable results, without disclosing the full range of analyses performed.",
        "S-hacking", 
        "Manipulating model inputs, outputs or the model itself to obtain a favourable value of a performance measure, usually by systematically altering the modelling until a desired level of a statistic or metric is obtained [@Nakagawa2025; @Nagy2025]. This can include changing random seeds, outcome variable and/or performance thresholds or metrics and re-executing the analysis, fitting, validation or evaluation without disclosing [@feng2019]. ",
        "Overhyping", 
        "Typically, overyhyping features in the discussion section of a paper and involves exaggerating or overstating the capabilities, generalisability, or reliability of a model beyond what is justified by the evidence.",
        "Data Curation", 
        "Selectively including, excluding, or modifying data points in the sample used to develop or evaluate a model, often to improve apparent model performance (a specific instance of S-hacking).",
        "Model Fishing", 
        "Searching through a large number of possible model specifications or variable combinations without theoretical justification, in an attempt to find favourable results, and not reporting all models and/or the dredging process. Alternatively, conducting alternative analyses or fitting new model variations, and selectively reporting only those that yield favourable results, without disclosing the full range of analyses performed.",
        "Selective Reporting", 
        "Choosing to report only certain analyses, models, metrics, model results or comparisons that yield favourable results or desired conclusions without disclosing the full range of analyses performed, or their results.  This can create a misleading impression of model performance or study outcomes. In contrast to ‘fishing expeditions’, models have theoretical justification.",
        "HARKing", "May accompany fishing expeditions, or model dredging analyses and model selection procedures that have been selectively reported. The selected model(s) is presented as if it was pre-specified and theoretically justified before parameterisation, and/or, the model dredging process is not disclosed.")

# waffle_plot_data %>% 
#   distinct(qrp_coded) %>% 
#   filter(qrp_coded != "Poor Practice") %>% 
#   arrange(match(qrp_coded, c("Selective Reporting", "S-hacking", "Model Fishing", "Data Curation", "Overhyping"))) %>% 
#   left_join(qrp_class_descriptions, by = join_by(qrp_coded)) %>% 
#   bind_rows(qrp_class_descriptions %>% filter(qrp_coded == "HARKing")) %>% 
#   gt::gt() %>% 
#   gt::cols_label(qrp_coded = "QRP Class") %>% 
#   gtExtras::gt_theme_nytimes() %>% 
#   gt::cols_width(
#     qrp_coded ~ gt::pct(15),
#     description ~ gt::pct(85)
#   ) %>% 
#   gt::fmt_markdown(columns = description) 

```

\newpage

```{r}
#| label: fig-waffle_plot
#| fig-cap: "Summary of Questionable Research Practices (QRPs) in ecological modelling. QRPs may target model inputs (blue squares), the model itself (purple squares), and/or model outputs (green squares), and may occur at different phases in the modelling cycle. QRPs are grouped according to broader classes defined in @tbl-QRP-categories. See @tbl-QRP for the full list of QRPs we identified."
#| fig-width: 14
#| fig-height: 16
#| cache: true
#| include: true

# #| dev: "cairo_pdf" # when output is pdf
# options(hrbrthemes.loadfonts = TRUE)
waffle_plot <- waffle_plot_data %>% 
        distinct() %>% #TODO identify duplicated rows - suspect non-unique practice_target coding is cause
        mutate(model_phase = 
                       forcats::as_factor(model_phase) %>% 
                       forcats::fct_relevel( c("Data", 
                                               "Model Construction", 
                                               "Model Evaluation",
                                               "Model Application"))) %>% 
        arrange(model_phase, 
                qrp_coded) %>% 
        ggplot(aes(fill = practice_target, 
                   values = values)) +
        geom_waffle(size = 2, 
                    color = "white", 
                    make_proportional = FALSE, 
                    flip = TRUE) +
        scale_x_discrete(expand = c(0,0)) +
        scale_y_discrete(expand = c(0,0)) +
  scale_fill_manual(
          name = "Target Model Object",
          values = c("input" = "#0F9ED5", # blue
                     "model" = "#A02B93", # purple
                     "output" = "#4EA72E"), # green
          labels = c("input" = "Model Input",
                     "model" = "Model",
                     "output" = "Model Output")
        ) +
        # ggsci::scale_fill_futurama(name = "QRP Target") +
        coord_equal() +
        facet_nested(qrp_coded + qrp_description ~ model_phase, 
                     switch = "y", 
                     labeller = 
                             labeller(
                                     qrp_description =
                                             label_wrap_gen(width = 80,
                                                            multi_line = TRUE),
                                     qrp_coded =
                                             label_wrap_gen(width = 10,
                                                            multi_line = TRUE),
                                     model_phase = 
                                             label_wrap_gen(width = 10, 
                                                            multi_line = TRUE)),
                     nest_line = element_line(linetype = 1),
                     solo_line = TRUE,
                     space = "free",
                     strip = strip_nested(size = "variable",
                                          clip = "off")
        ) +
        # facet_grid(qrp_description ~ model_phase, 
        #            switch = "y", 
        #            labeller = labeller(qrp_description = 
        #                                        label_wrap_gen(width = 100))) +
        # theme_no_axes() +
        hrbrthemes::theme_ipsum_rc() +
        waffle::theme_enhance_waffle() +
        theme(strip.text.y.left = element_text(angle = 0, 
                                               size = 14),
              strip.text.x = element_text(size = 16,vjust = 0),
              strip.background.x = element_part_rect(side = "b"),
              strip.background.y = element_blank()
        ) +
        ggh4x::force_panelsizes(rows = unit(1, "cm"), 
                                cols = unit(3, "cm"))

# print(waffle_plot)
# loadfonts(device = "all")
waffle_plot
# ggsave(waffle_plot, 
#        filename = here::here("figures/fig-waffle-QRPs-ecological-modelling.pdf"),
#        device = grDevices::cairo_pdf,
#        width = 14,
#        height = 16,
#        dpi = 600)

# magick::image_read_pdf(here::here("figures/fig-waffle-QRPs-ecological-modelling.pdf"))

#TODO update based on revised definitions
#DONE reorder the model process levels
#TODO exclude less important QRPs
#DONE Make strips pretty
#DONE recolour model object colours
#DONE group the QRPs into their umbrella terms -- nested facets?
```

::: {#boxsyntheg .Box title="Synthetic Example of QRPs in Applied Ecological Modelling"}
A modeller seeks to predict species abundance on the basis of habitat quality to help inform conservation management (@fig-synthetic-example). When the modeller evaluates how two different management actions affecting habitat quality influence species abundance (Panel A, Stage 1), the Initial Model does not provide sufficient certainty for choosing between one action and another. The modeller goes back and changes the model without theoretical justification, instead opportunistically trialling different models and selecting the one with the best Performance Score (Panel B). On checking the predicted species abundance for the two actions on the overfitted model, the modeller finds that the actions are still not clearly distinguishable in terms of their predicted outcomes (Panel A, Stage 2), so the modeller adjusts the input values for the two actions, and plots the predicted outcomes (Panel B, Stage 3). They are sufficiently happy that the model clearly supports their preferred management action B and proceed to publish the overfitted model, its predictions and management recommendations without ever disclosing their model fishing and scenario hacking, effectively a form of HARKing ('hypothesising after results are known'). The impacts of the modeller’s actions are summarised in Panel C.
:::

```{r}
#| fig-cap: | 
#|   Illustrating model fishing & scenario hacking (HARKing). A. Violin boxplots of predicted species abundance for two management actions from two models constructed at different stages of the modelling process: a priori model (stage 1), a model generated from a model fishing exercise (stage 2), and the same model, but illustrating scenario hacking (stage 3). Dots are predicted values. Violin outlines illustrate kernel density probability distributions, where the width of the shaded area represents the corresponding proportion of data. The model-estimated median and quartiles are displayed for each action. The colours of each action correspond to those shown in Panel C. B. Performance Scores calculated from multiple model performance measures for the Initial Model and a new, superior Overfitted Model (greater Performance Score). See @Ludecke2020 for metric calculation details. C. Predicted species abundance as a function of habitat quality from the Initial Model (yellow line) and the Overfitted Model (dashed orange line) generated from the model fishing exercise. The management action scenarios used in the first two stages of modelling are shown as solid blue and green lines. Scenario hacking occurs when the modeller selects two new management scenarios with a greater mean difference in predicted species abundance under the stage 2 model.
#| label: fig-synthetic-example
#| message: false
#| cache: true
#| include: true

magick::image_read_pdf(here::here("figures/synthetic_example_QRPs.pdf")) %>% 
  magick::image_scale("70%")
```

## Formalising the 'Garden of Forking Paths' in Model-Based Research

In this section, we extend Gelman and Loken’s [-@Gelman2013] mathematical formalism explaining the emergence of QRPs, or the ‘garden of forking paths’ (introduced in Chapter 1) to model-based research. It helps us to formally differentiate defensible and questionable data-dependent decisions.

As we emphasise above, we hesitate to designate all data-dependent analytic decisions as *questionable* , as is implied in the prevailing literature on preregistration and QRPs. That is because there are situations in modelling where decisions are necessarily dependent on the outcome of previous analytic decisions within the modelling workflow [@Liu2020] – not all data-dependent analytic decisions are automatically ‘questionable’ within the context of modelling. For example, many modelling decisions are data-driven, like the choice to remove correlated variables or checking for distributional assumptions in order to decide the most appropriate model functional form (See Chapter 6, Fig XX, for an example from our case study). @Liu2020 helpfully distinguish defensible from questionable motivations for engaging in data-dependent analytic decision-making, by classifying them as either systematic or opportunistic.

Above, we have established a foundation for extending Gelman and Loken’s [-@Gelman2013] mathematical formalism to modelling, which describes the process of analytical decision making when making inferences within a hypothesis testing framework. Now, we can adapt this formalism for model-based research (Box 2), helping us to *formally* distinguish between defensible and opportunistic data-dependent decisions – both analytic and reporting decisions. Once we've set out this formalism, we can identify and distinguish between different types of QRPs.

::: Box
The modeller makes a series of analytic decisions to derive $M_p$ from $M_s$, from $M_c$, referred to hereafter as $M$ for simplicity . We term the sequence of modelling choices throughout the modelling process the realised "modelling path".  Analytic uncertainty, or analogously researcher degrees of freedom, propagates combinatorially along each decision-point to inform a multiplicity of plausible analysis strategies [@Hoffmann2021] constituting the "garden of forking paths" [@Gelman2013]. Consider that a modeller faces some decision $C$ along that path about a modelling task concerning model $M$ and some observed data $\boldsymbol{X}$, with a choice or decision rule $\phi$ . Decisions made before observing data or model outputs reflect idealised practice where choices are predetermined and independent of results $C(\mathbf{X}, M; \phi)$*.* When the modelling choice is "data-contingent" $\phi(\boldsymbol{X},M,M_j )$ insofar as it hinges on the observed state of the model(s) $M$and/or any associated outputs $M_j$and data $\boldsymbol{X}$ at that point along the modelling path, it satisfies a broad definition of 'questionable'.

We define defensible data-dependent decisions $\phi_D$ as following a systematic process $\Psi_{\text{systematic}}$:

$$
\begin{aligned}
\phi_D(\mathbf{X}, M; \Omega, \Psi_{\text{systematic}}) = g(\mathbf{X}, M, \Omega)
\end{aligned}
$$ {#eq-defensible-data-dependent}

Where $g(x)$ is a deterministic function of the data, model state, and pre-defined decision-rule $\Omega$, based on systematic objectives such as, model adequacy, predictive accuracy or theoretical consistency.

In contrast, questionable practices $\phi_Q$ we define as:

$$
\begin{aligned}
\phi_Q(\mathbf{X}, M; \Psi{\text{opportunistic}}) = \phi^* \\
& \text{ such that } h(\phi^* | \mathbf{X}, M, R^*) \geq h(\phi | \mathbf{X}, M, R^*) \\
&\text{ for all } \phi \in \Phi
\end{aligned}
$$ {#eq-questionable-practices}

Where the decision-making is opportunistic and result-seeking $\Psi{\text{opportunistic}}$, and $h(\phi | \mathbf{X}, M, R^*)$ represents how well decision $\phi$ serves the desired outcome $R^*$. Data-contingent decisions are therefore *questionable* when a researcher’s drive to make their research publishable influences the direction that the realised modelling path takes.

A defining aspect of QRPs is that they remain undisclosed. Just as decisions about the modelling process can be questionable, so too can reporting practices. We apply the same logic describing questionable modelling practices to reporting practices:

$$
\begin{aligned}C_{\text{Reported}} = S({C(\mathbf{X}, M, \phi)})\end{aligned}
$$ {#eq-generic-reporting-function}

Where $S$ is a selecting function that determines what to report from a set of conducted analyses.

Journal method or article length convention restricts complete transparency, and not all results can be reported. The decision about what to report from a set of conducted analyses $S$ is made following predetermined plans $\Omega$, not contingent on observed results $S_{\text{pre}}$:

$$
\begin{aligned}S_{\text{pre}}({C(\mathbf{X}, M, \phi)}) = s({C(\boldsymbol{X}, M, \phi)},\Omega)\end{aligned}
$$ {#eq-apriori-reporting}

Questionable reporting $S_Q$, or as it is more commonly known, selective reporting, occurs when the reporting is opportunistic and contingent on the observed results, and optimised for desired outcomes $R^*$:

$$
\begin{aligned}S_Q({C(\mathbf{X}, M, \phi)}; \Psi_{\text{opportunistic}}) = C^* \\ & \text{ such that } h(C^*|R^*) \geq h(C|R^*) \\ 
& \text{ for all } C \in {C(\mathbf{X}, M, \phi)}\end{aligned}
$$ {#eq-questionable-reporting}

We avoid defining $\Psi\text{systematic}$ reporting decisions, as we did for modelling decisions, and instead advocate modellers pre-specify what results will be reported.
:::

## A Typology of QRPs

# References {.unnumbered}

::: {#refs}
:::

::: landscape
```{r}
#| label: tbl-QRP
#| tbl-cap: "Full list of Questionable Research Practices identified from the literature. We categorised QRPs into broader classes"
#| column: body-outset
#| warning: false
#| message: false

gt_qrp <- tidy_QRP_database(here::here("data/QRP_database_2025-10-20.csv")) %>%
        make_qrp_table_data() %>% 
        select(-practice_notes) %>% 
        group_by(qrp_coded) %>% 
        arrange(qrp_coded) %>% 
        make_QRP_table() %>% 
        gt::fmt_markdown(columns = c(qrp_coded, qrp_description, source)) %>% 
        gt::cols_hide(columns = c(-starts_with("practice_"), -source, -practice_target) ) %>% 
        gt::cols_label(qrp_coded = "QRP Class") %>% 
        gtExtras::gt_theme_nytimes()

# gt_qrp


tidy_QRP_database(here::here("data", 
                             "QRP_database_2025-10-20.csv")) %>%
  select(-file_name, -description) %>% 
  mutate(across(c(starts_with("practice_"), -practice_target), 
                ~ str_c(.x, source))) %>% #annotate `practice_*` with `source`
  group_by(qrp_coded, qrp_description) %>%
  mutate(across(c(starts_with("model_"), 
                  "practice_target",
                  "qrp_reason", 
                  "practice_notes"), 
                ~ str_split(.x, ", ")),
         source = str_split(source, "; ")
         ) %>% 
  summarise(across(c(starts_with("model_"), 
                     "qrp_reason",
                     "practice_notes",
                     starts_with("practice_")), 
                   list)) %>% 
  rowwise() %>% 
  mutate(across(c(starts_with("model_"), 
                  "practice_target",  
                  "qrp_reason", 
                  "practice_notes"), 
                ~ flatten_chr(unique(.x)) %>% 
                  unique() %>% 
                  str_flatten_comma(na.rm = TRUE)),
         source = list(
                         str_remove_all(string = "source", pattern = c("\\[|\\]")) %>% 
                         str_squish() %>% 
                         unique() %>% 
                         str_flatten("; ") %>% 
                         str_c("[", ., "]")),
         across(c(starts_with("practice_"), -"practice_target"), 
                ~ str_squish(.x) %>% 
                         str_flatten(collapse = "<br>"))) %>% 
  gt::gt(groupname_col = c("qrp_coded", "qrp_description")) %>% 
  gt::cols_hide(source) %>% 
  # gt::cols_width(ends_with("_source") ~ gt::pct(20),
                 # qrp_reason ~ gt::pct(20)) %>%
  gtExtras::gt_theme_pff() %>% 
  gt::tab_options(table.font.size = 8) %>% 
  gt::fmt_markdown(columns = source)


```
:::

::: appendices
## Appendix 1: Modelling Process Literature Review {#sec-appendix-1}

## Appendix 2: QRP Roadmap Literature Review {#sec-appendix-2}

### Step 1 - Identify and collect QRPs

We unsystematically screened the published literature to generate an initial list of terms for QRPs in NHST research to guide search term selection in ecological modelling and related modelling fields. We used the following search terms to identify potential QRPs in different areas of ecological modelling:

-   "`modelling_area` AND type I error"
-   "`modelling_area` AND false positive"
-   "`modelling_area` AND modelling choice"
-   "`modelling_area` AND subjective judgment"
-   "`modelling_area` AND prediction error"
-   "`modelling_area` AND confirmation bias"
-   "`modelling_area` AND publication bias"
-   "`modelling_area` AND questionable research practice"
-   "`modelling_area` AND researcher degrees of freedom"
-   "`modelling_area` AND cherry picking"
-   "`modelling_area` AND p-hacking"
-   "`modelling_area` AND HARKING"
-   "`modelling_area` AND bias"
-   "`modelling_area` AND good modelling practice"
-   "`modelling_area` AND best modelling practice"
-   "`modelling_area` AND bad modelling practice"
-   Where `modelling_area` included 'predictive modelling', 'habitat modelling', 'Species Distribution Modelling (SDM)', 'Ecological Niche Modelling', 'Ecological Modelling', 'Environmental Modelling'.

We inferred QRPs from practices described by authors with value judgements, such as "good" or "best practice," "bad" or "poor practice." For positively ascribed practices, we took the logical inverse of these practices as the QRP. We ignored perceived 'inconsequential' practices, and instead included practices that were commonly or routinely conducted and where authors argued strongly for the culture of research practice to change. We excluded QRPs that pertained to fraud, misconduct, or nefarious intent.

**Collating & Coding**

For each QRP identified, we collected a description of the research practice `practice_description`, the reason or justification for why the practice is 'questionable' `practice_reason`, including any quantitative and/or empirical evidence for: a) the negative consequences on research outcomes such as credibility, reliability, accuracy, precision, transparency, reproducibility and/or b) evidence for the use or occurrence of this practice; `practice_evidence`. We assigned each QRP to phases and sub-phases of the modelling process (section X) where the practice occurs. For each description, reason and evidence, we coded each into short descriptions of the practice `QRP_description`, reason for the practice's 'questionable' nature `QRP_reason`. Using the model phases and sub-phases identified in (Section X), we classified the location of the QRP in the modelling process, ascribing the `model_phase` and `model_subphase` in which the practice occurs. We then coded the `target` of the practice, i.e. the model object (Figure X) directly affected by the practice. Where mitigation measures or solutions to the practice were suggested alongside the description of the practice, we also coded the `practice_solution`.

**Categorising QRPs into Classes**

We adopt Nagy et al.'s [-@Nagy2025] approach and grouped QRPs consisting of the same family of research behaviours into broad classes of QRPs `QRP_coded`. Some umbrella terms were common QRPs in hypothesis testing research, some were hypothesis-testing analogues, while others were modelling-specific. Where possible we used existing umbrella terms used by Nagy et al. [-@Nagy2025] and others [e.g. @Liu2020], but created our own terms if no existing terms were applicable.

### *Step 3 - Refine QRP and QRP Class descriptions, aggregate QRPs*

We refined our coded descriptions of QRPs `QRP_description` into unifying or general descriptions effectively aggregating similar practices identified from different published sources. After categorising QRPs into broader classes of QRPs, we revised their description to be more modelling-specific, better reflecting the surveyed practices (Table X).

### QRP Code-Book / Metadata

-   [ ] TBD

## Session Info {.unnumbered}

```{r}
#| label: session-info

devtools::session_info()
```
:::
